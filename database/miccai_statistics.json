[{"title": "3D CVT-GAN: A 3D Convolutional Vision Transformer-GAN for PET Reconstruction", "abstract": "To obtain high-quality positron emission tomography (PET) scans while reducing potential radiation hazards brought to patients, various generative adversarial network (GAN)-based methods have been developed to reconstruct high-quality standard-dose PET (SPET) images from low-dose PET (LPET) images. However, due to the intrinsic locality of convolution operator, these methods have failed to explore global contexts of the entire 3D PET image. In this paper, we propose a novel 3D convolutional vision transformer GAN framework, named 3D CVT-GAN, for SPET reconstruction using LPET images. Specifically, we innovatively design a generator with a hierarchical structure that uses multiple 3D CVT blocks as the encoder for feature extraction and multiple 3D transposed CVT (TCVT) blocks as the decoder for SPET restoration, capturing both local spatial features and global contexts from different network layers. Different from the vanilla 2D vision transformer that uses linear embedding and projection, our 3D CVT and TCVT blocks employ 3D convolutional embedding and projection instead, allowing the model to overcome semantic ambiguity problem caused by the attention mechanism and further preserve spatial details. In addition, residual learning and a patch-based discriminator embedded with 3D CVT blocks are added inside and after the generator, facilitating the training process while mining more discriminative feature representations. Validation on the clinical PET datasets shows that our proposed 3D CVT-GAN outperforms the state-of-the-art methods qualitatively and quantitatively with minimal parameters.\n"}, {"title": "3D Global Fourier Network for Alzheimer\u00e2\u0080\u0099s Disease Diagnosis using Structural MRI", "abstract": "Deep learning models, such as convolutional neural networks and self-attention mechanisms, have been shown to be effective in computer-aided diagnosis (CAD) of Alzheimer\u00e2\u0080\u0099s disease (AD) using structural magnetic resonance imaging (sMRI). Most of them use spatial convolutional filters to learn local information from the images, ignoring frequency domain information. In this paper, we propose a 3D Global Fourier Network (GF-Net) to utilize global frequency information that also captures long-range dependency in the spatial domain. The GF-Net contains three primary components: a 3D discrete Fourier transform, an element-wise multiplication between frequency domain features and learnable global filters, and a 3D inverse Fourier transform. The GF-Net is trained by a multi-instance learning strategy to identify discriminative features. Extensive experiment on two independent datasets (ADNI and AIBL) have demonstrated that our proposed GF-Net outperforms several state-of-the-art methods in terms of accuracy and other metrics, and can also identify pathological regions of AD. The code is released at https://github.com/qbmizsj/GFNet.\n"}, {"title": "4D-OR: Semantic Scene Graphs for OR Domain Modeling", "abstract": "Surgical procedures are conducted in highly complex operating rooms (OR), comprising different actors, devices, and interactions. To date, only medically trained human experts are capable of understanding all the links and interactions in such a demanding environment. This paper aims to bring the community one step closer to automated, holistic and semantic understanding and modeling of OR domain.  Towards this goal, for the first time, we propose using semantic scene graphs (SSG) to describe and summarize the surgical scene. The nodes of the scene graphs represent different actors and objects in the room, such as medical staff, patients, and medical equipment, whereas edges are the relationships between them. To validate the possibilities of the proposed representation, we create the first publicly available 4D surgical SSG dataset, 4D-OR, containing ten simulated total knee replacement surgeries recorded with six RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734 frames and is richly annotated with SSGs, human and object poses, and clinical roles. We propose an end-to-end neural network-based SSG generation pipeline, with a rate of success of 0.75 macro F1, indeed being able to infer semantic reasoning in the OR. We further demonstrate the representation power of our scene graphs by using it for the problem of clinical role prediction, where we achieve 0.85 macro F1. The code and dataset will be made available upon acceptance.\n"}, {"title": "A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000", "abstract": "Computer aided diagnosis (CAD) has gained an increased amount of attention in the general research community over the last years as an example of a typical limited data application - with experiments on labeled 100k-200k datasets.\nAlthough these datasets are still small compared to natural image datasets like ImageNet1k, ImageNet21k and JFT, they are large for annotated medical datasets, where 1k-10k labeled samples are much more common. There is no baseline on which methods to build on in the low data regime. In this work we bridge this gap by providing an extensive study on medical image classification with limited annotations (5k). We present a study of modern architectures applied to a fixed low data regime of 5000 images on the CheXpert dataset. Conclusively we find that models pretrained on ImageNet21k achieve a higher AUC and larger models require less training steps. All models are quite well calibrated even though we only fine-tuned on 5000 training samples. All \u00e2\u0080\u0098modern\u00e2\u0080\u0099 architectures have higher AUC than ResNet50. Regularization of Big Transfer Models with MixUp or Mean Teacher improves calibration, MixUp also improves accuracy. Vision Transformer achieve comparable or on par results to Big Transfer Models.\n"}, {"title": "A Deep-Discrete Learning Framework for Spherical Surface Registration", "abstract": "Cortical surface registration is a fundamental tool for neuroimaging analysis that has been shown to improve the alignment of functional regions relative to volumetric approaches. Classically, image registration is performed by optimizing a complex objective similarity function, leading to long run times. This contributes to a convention for aligning all data to a global average reference frame that poorly reflects the underlying cortical heterogeneity. In this paper, we propose a novel unsupervised learning-based framework that converts registration to a multi-label classification problem, where each point in a low-resolution control grid deforms to one of fixed, finite number of endpoints. This is learned using a spherical geometric deep learning architecture, in an end-to-end unsupervised way, with regularization imposed using a deep Conditional Random Field (CRF). Experiments show that our proposed framework performs competitively, in terms of similarity and areal distortion, relative to the most popular classical surface registration algorithms and generates smoother deformations than other learning-based surface registration methods, even in subjects with atypical cortical morphology.\n"}, {"title": "A Geometry-Constrainted Deformable Attention Network for Aortic Segmentation", "abstract": "Morphological segmentation of the aorta is significant for aortic diagnosis, intervention, and prognosis. However, it is difficult for existing methods to achieve the continuity of spatial information and the integrity of morphological extraction, due to the gradually variable and irregular geometry of the aorta in the long-sequence computed tomography (CT). In this paper, we propose a geometry-constrained deformable attention network (GDAN) to learn the aortic common features through interaction with context information of the anatomical space. The deformable attention extractor in our model can adaptively adjust the position and the size of patches to match different shapes of the aorta. The self-attention mechanism is also helpful to explore the long-range dependency in CT sequences and capture more semantic features. The geometry-constrainted guider simplifies the morphological representation with a high spatial similarity. The guider imposes strong constraints on geometric boundaries, which changes the sensitivity of gradually variable aortic morphology in the network. Guider can assist the correct extraction of semantic features combining deformable attention extractor. In 204 cases of aortic CT dataset, including 42 normal aorta, 45 coarctation of the aorta, and 107 aortic dissection, our method obtained a mean dice similarity coefficient of 0.943 on the test set (20%), outperforming 6 state-of-the-art methods about aortic segmentation.\n"}, {"title": "A Hybrid Propagation Network for Interactive Volumetric Image Segmentation", "abstract": "Interactive segmentation is of great importance in clinical practice for correcting and refining the automated segmentation by involving additional user hints, e.g., scribbles and clicks. \nCurrently, interactive segmentation methods for 2D medical images are well studied, while seldom works are conducted on 3D medical volumetric data.  Given a 3D volumetric image, the user interaction can only be performed on a few slices, thus the key issue is how to propagate the information over the entire volume for spatial-consistent segmentation.\nIn this paper, we propose a novel hybrid propagation network for interactive segmentation of 3D medical images. \nOur proposed method consists of two key designs, including a slice propagation network (denoted as SPN) for transferring user hints to adjacent slices to guide the segmentation slice-by-slice and a volume propagation network (denoted as VPN) for propagating user hints over the entire volume in a global manner and providing spatial-consistent features to boost slice segmentation. Specifically, as for SPN, we adopt a memory-augmented network, \nwhich utilizes the information of segmented slices (memory slices) to propagate interaction information. \nTo use interaction information propagated by VPN, a feature-enhanced memory module is designed, in which the volume segmentation information from the latent space of VPN is introduced into the memory module of SPN. \nIn such a way, the interactive segmentation can leverage both advantages of volume and slice propagation, thus improving the volume segmentation results. We perform experiments on two commonly-used 3D medical datasets, with the experimental results indicating that our method outperforms the state-of-the-art methods.  Our code is available at https://github.com/luyueshi/Hybrid-Propagation.\n"}, {"title": "A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis", "abstract": "Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic information but is limited in practice due to excessive data acquisition time. In this paper, we propose a novel deep-learning model for joint reconstruction and synthesis of multi-modal MRI using incomplete k-space data of several source modalities as inputs. The output of our model includes reconstructed images of the source modalities and high-quality image synthesized in the target modality.\nOur proposed model is formulated as a variational problem that leverages several learnable modality-specific feature extractors and a multimodal synthesis module. We propose a learnable optimization algorithm to solve this model, which induces a multi-phase network whose parameters can be trained using multi-modal MRI data. Moreover, a bilevel-optimization framework is employed for robust parameter training. We demonstrate the effectiveness of our approach using extensive numerical experiments.\n"}, {"title": "A Medical Semantic-Assisted Transformer for Radiographic Report Generation", "abstract": "Automated radiographic report generation is a challenging cross-domain task that aims to automatically generate accurate and semantic-coherence reports to describe medical images. Despite the recent progress in this field, there are still many challenges  at least in the following aspects. First, radiographic images are very similar to each other, and thus it is difficult to capture the fine-grained visual differences using CNN as the visual feature extractor like many existing methods. Further, semantic information has been widely applied to boost the performance of generation tasks (e.g. image captioning), but existing methods often fail to provide effective medical semantic features. Toward solving those problems, in this paper, we propose a memory-augmented sparse attention block utilizing bilinear pooling to capture the higher-order interactions between the input fine-grained image features while producing sparse attention. Moreover, we introduce a novel Medical Concepts Generation Network (MCGN) to predict fine-grained semantic concepts and incorporate them into the report generation process as guidance. Our proposed method shows promising performance on the recently released largest benchmark MIMIC-CXR.  It outperforms multiple state-of-the-art methods in image captioning and medical report generation. Our code is available at https://github.com/zwan0839/MSAT.\n"}, {"title": "A Multi-task Network with Weight Decay Skip Connection Training for Anomaly Detection in Retinal Fundus Images", "abstract": "By introducing the skip connection to bridge the semantic gap between encoder and decoder, U-shape architecture has been proven to be effective for recovering fine-grained details in dense prediction tasks. However, such a mechanism cannot be directly applied to reconstruction-based anomaly detection, since the skip connection might lead the model overfitting to an identity mapping between the input and output. In this paper, we propose a weight decay training strategy to progressively mute the skip connections of U-Net, which effectively adapts U-shape network to anomaly detection task. Thus, we are able to leverage the modeling capabilities of U-Net architecture, and meanwhile prevent the trained model from bypassing low-level features. Furthermore, we formulate an auxiliary task, namely histograms of oriented gradients (HOG) prediction, to encourage the framework to deeply exploit contextual information from fundus images. The HOG feature descriptors with three different resolutions are adopted as the auxiliary supervision signals. The multi-task framework is dedicated to enforce the model to aggregate shared significant commonalities and eventually improve the performance of anomaly detection. Experimental results on Indian Diabetic Retinopathy image Dataset (IDRiD) and Automatic Detection challenge on Age-related Macular degeneration dataset (ADAM) validate the superiority of our method for detecting abnormalities in retinal fundus images. The source code is available at https://github.com/WentianZhang-ML/WDMT-Net.\n"}, {"title": "A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos", "abstract": "Breast lesion detection in ultrasound is critical for breast cancer diagnosis. Existing methods mainly rely on individual 2D ultrasound images or combine unlabeled video and labeled 2D images to train models for breast lesion detection. In this paper, we first collect and annotate an ultrasound video dataset (188 videos) for breast lesion detection. Moreover, we propose a clip-level and video-level feature aggregated network (CVA-Net) for addressing breast lesion detection in ultrasound videos by aggregating video-level lesion classification features and clip-level temporal features. The clip-level temporal features encode local temporal information of ordered video frames and global temporal information of shuffled video frames. In our CVA-Net, an inter-video fusion module is devised to fuse local features from original video frames and global features from shuffled video frames, and an intra-video fusion module is devised to learn the temporal information among adjacent video frames. Moreover, we learn video-level features to classify the breast lesions of the original video as benign or malignant lesions to further enhance the final breast lesion detection performance in ultrasound videos. Experimental results on our annotated dataset demonstrate that our CVA-Net clearly outperforms state-of-the-art methods. The corresponding code and dataset are publicly available at https://github.com/jhl-Det/CVA-Net.\n"}, {"title": "A Novel Deep Learning System for Breast Lesion Risk Stratification in Ultrasound Images", "abstract": "This paper presents a novel deep learning system to classify breast lesions in ultrasound images into benign and malignant and into Breast Imaging Reporting and Data System (BI-RADS) six categories simultaneously. A multitask soft label generating architecture is proposed to improve the classification performance, in which task-correlated labels are obtained from a dual-task teacher network and utilized to guide the training of a student model. In student model, a consistency supervision mechanism is embedded to constrain that a prediction of BI-RADS is consistent with the predicted pathology result. Moreover, a cross-class loss function that penalizes different degrees of misclassified items with different weights is introduced to make the prediction of BI-RADS closer to the annotation. Experiments on our private and two public datasets show that the proposed system outperforms current state-of-the-art methods, demonstrating the great potential of our method in clinical diagnosis.\n"}, {"title": "A Novel Fusion Network for Morphological Analysis of Common Iliac Artery", "abstract": "In endovascular interventional therapy, automatic common\niliac artery morphological analysis can help physicians plan surgical procedures\nand assist in the selection of appropriate stents to improve surgical\nsafety. However, different people have distinct blood vessel shapes,\nand many patients have severe malformations of iliac artery due to hemangiomas.\nBesides, the uneven distribution of contrast media make\nit difficult to make an accurate morphological analysis of the common\niliac artery. In this paper, a novel fusion network, combining CNN and\nTransformer is proposed to address the above issues. The proposed FTU-Net\nconsists of a parallel encoder and a Cross-Fusion module to capture\nand fuse global context information and local representation. Besides, a\nhybrid decoder module is designed to better adapt the fused features.\nExtensive experiments have demonstrated that our proposed method\nsignificantly outperforms the best previously published results for this\ntask and achieves the state-of-the-art results on the common iliac artery\ndataset built by us and two other public medical image datasets. To the\nbest of our knowledge, this is the first approach capable of common iliac\nartery segmentation.\n"}, {"title": "A Novel Knowledge Keeper Network for 7T-Free But 7T-Guided Brain Tissue Segmentation", "abstract": "An increase in signal-to-noise ratio (SNR) and susceptibility-induced contrast at higher field strengths, e.g., 7T, is crucial for medical image analysis by providing better insights for the pathophysiology, diagnosis, and treatment of several disease entities. However, it is difficult to obtain 7T images in real clinical practices due to the high cost and low accessibility. In this paper, we propose a novel knowledge keeper network (KKN) to guide brain tissue segmentation by taking advantage of 7T representations without explicitly using 7T images. By extracting features of a 3T input image substantially and then transforming them to 7T features via knowledge distillation (KD), our method achieves deriving 7T-like representations from a given 3T image and exploits them for tissue segmentation. On two independent datasets, we evaluated our method\u00e2\u0080\u0099s validity in qualitative and quantitative manners on 7T-like image synthesis and 7T-guided tissue segmentation by comparing with the comparative methods in the literature.\n"}, {"title": "A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models", "abstract": "Translating machine learning algorithms into clinical applications requires addressing challenges related to interpretability, such as accounting for the effect of confounding variables (or metadata). Confounding variables affect the relationship between input training data and target outputs. When we train a model on such data, confounding variables will bias the distribution of the learned features. A recent promising solution, MetaData Normalization (MDN), estimates the linear relationship between the metadata and each feature based on a non-trainable closed-form solution. However,  this estimation is confined by the sample size of a mini-batch and thereby may cause the approach to be unstable during training. In this paper, we  extend the MDN method by applying a Penalty approach (referred to as PDMN). We cast the problem into a bi-level nested optimization problem. We then approximate this optimization problem using a penalty method so that the linear parameters within the MDN layer are trainable and learned on all samples. This enables PMDN to be plugged into any architectures, even those unfit to run batch-level operations, such as transformers and recurrent models. We show improvement in model accuracy and greater independence from confounders using PMDN over MDN in a synthetic experiment and a multi-label, multi-site dataset of magnetic resonance images (MRIs). \n"}, {"title": "A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects", "abstract": "The recent development of deep learning combined with compressed sensing enables fast reconstruction of undersampled MR images and has achieved state-of-the-art performance for Cartesian k-space trajectories. However, non-Cartesian trajectories such as the radial trajectory need to be transformed onto a Cartesian grid in each iteration of the network training, slowing down the training process significantly.  Multiple iterations of nonuniform Fourier transformation in the networks offset the advantage of fast inference inherent in deep learning. Current approaches typically either work on image-to-image networks or grid the non-Cartesian trajectories before the network training to avoid the repeated gridding process. However, the image-to-image networks cannot ensure the k-space data consistency in the reconstructed images and the pre-processing of non-Cartesian k-space leads to gridding errors which cannot be compensated by the network training. Inspired by the Transformer network to handle long-range dependencies in sequence transduction tasks, we propose to rearrange the radial spokes to sequential data based on the chronological order of acquisition and use the Transformer network to predict unacquired radial spokes from the acquired data. We propose novel data augmentation methods to generate a large amount of training data from a limited number of subjects. The network can be applied to different anatomical structures. Experimental results show superior performance of the proposed framework compared to state-of-the-art deep neural networks.\n"}, {"title": "A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation", "abstract": "We propose a Transformer architecture for volumetric segmentation, a challenging task that requires keeping a complex balance in encoding local and global spatial cues, and preserving information along all axes of the volume. Encoder of the proposed design benefits from self-attention mechanism to simultaneously encode local and global cues, while the decoder employs a  parallel self and cross attention formulation to capture fine details for boundary refinement. Empirically, we show that the proposed design choices result in a computationally efficient model, with competitive  and  promising results on the Medical Segmentation Decathlon (MSD) brain tumor segmentation (BraTS) Task. We further show that the representations learned by our model are robust against data corruptions. \n"}, {"title": "A Self-Guided Framework for Radiology Report Generation", "abstract": "Automatic radiology report generation is essential to computer-aided diagnosis. Through the success of image captioning, medical report generation has been achievable. However, the lack of annotated disease labels is still the bottleneck of this area. In addition, the image-text data bias problem and complex sentences make it more difficult to generate accurate reports. To address these gaps, we present a self-guided framework (SGF), a suite of unsupervised and supervised deep learning methods to mimic the process of human learning and writing. In detail, our framework obtains the domain knowledge from medical reports without extra disease labels and guides itself to extract fined-grain visual features associated with the text. Moreover, SGF successfully improves the accuracy and length of medical report generation by incorporating a similarity comparison mechanism that imitates the process of human self-improvement through comparative practice. Extensive experiments demonstrate the utility of our SGF in the majority of cases, showing its superior performance over state-of-the-art methods. Our results highlight the capacity of the proposed framework to distinguish fined-grained visual details between words and verify its advantage in generating medical reports.\n"}, {"title": "A Sense of Direction in Biomedical Neural Networks", "abstract": "We describe an approach to making a model be aware of not only intensity but also properties such as direction and scale during forward propagation. Such properties are important in when analysing images containing curvilinear structures such as vessels or fibres.\nWe propose the General Multi-Angle Scale Convolution (G-MASC), whose kernels are arbitrarily rotatable and also fully differentiable like normal convolution. The model manages its directional detectors in sets, and supervises a set\u00e2\u0080\u0099s rotationality with a novel rotation penalty called PoRE. The algorithm works on pyramid representations to enable scale search. Direction and scale can be extracted from the output maps, encoded and analysed separately. Tests were conducted on three public datasets, MoNuSeg, DRIVE, and CHASE-DB1. Good performance is observed while the model requires 1% or fewer parameters than benchmark approaches.\n"}, {"title": "A Spatiotemporal Model for Precise and Efficient Fully-automatic 3D Motion Correction in OCT", "abstract": "Optical coherence tomography (OCT) is a micrometer-scale, volumetric imaging modality that has become a clinical standard in ophthalmology. OCT instruments image by raster-scanning a focused light spot across the retina, acquiring sequential cross-sectional images to generate volumetric data. Patient eye motion during the acquisition poses unique challenges: Non-rigid, discontinuous distortions can occur, leading to gaps in data and distorted topographic measurements. We present a new distortion model and a corresponding fully-automatic, reference-free optimization strategy for computational motion correction in orthogonally raster-scanned, retinal OCT volumes. Using a novel, domain-specific spatiotemporal parametrization of forward-warping displacements, eye motion can be corrected continuously for the first time. Parameter estimation with temporal regularization improves robustness and accuracy over previous spatial approaches. We correct each A-scan individually in 3D in a single mapping, including repeated acquisitions used in OCT angiography protocols. Specialized 3D forward image warping reduces median runtime to < 9 s, fast enough for clinical use. We present a quantitative evaluation on 18 subjects with ocular pathology and demonstrate accurate correction during microsaccades. Transverse correction is limited only by ocular tremor, whereas submicron repeatability is achieved axially (0.51 \u00c2\u00b5m median of medians), representing a dramatic improvement over previous work. This allows assessing longitudinal changes in focal retinal pathologies as a marker of disease progression or treatment response, and promises to enable multiple new capabilities such as supersampled/super-resolution volume reconstruction and analysis of pathological eye motion occuring in neurological diseases.\n"}, {"title": "A Transformer-Based Iterative Reconstruction Model for Sparse-View CT Reconstruction", "abstract": "Sparse-view computed tomography (CT) is one of the primary means to reduce the radiation risk. But the reconstruction of sparse-view CT will be contaminated by severe artifacts. By carefully designing the regularization terms, the iterative reconstruction (IR) algorithm can achieve promising results. With the introduction of deep learning techniques, learned regularization terms with convolution neural network (CNN) attracts much attention and can further improve the performance. In this paper, we propose a learned local-nonlocal regularization-based model called RegFormer to reconstruct CT images. Specifically, we unroll the iterative scheme into a neural network and replace handcrafted regularization terms with learnable kernels. The convolution layers are used to learn local regularization with excellent denoising performance. Simultaneously, transformer encoders and decoders incorporate the learned nonlocal prior into the model, preserving the structures and details. To improve the ability to extract deep features during iteration, we introduce an iteration transmission (IT) module, which can further promote the efficiency of each iteration. The experimental results show that our proposed RegFormer achieves competitive performance in artifact reduction and detail preservation compared to some state-of-the-art sparse-view CT reconstruction methods.\n"}, {"title": "AANet: Artery-Aware Network for Pulmonary Embolism Detection in CTPA Images", "abstract": "Pulmonary embolism (PE) is life-threatening and computed tomography pulmonary angiography (CTPA) is the best diagnostic techniques in clinics. However, PEs usually appear as dark spots among the bright regions of blood arteries in CTPA images, which can be very similar with veins that are less bright and soft tissues. Even for experienced radiologists, the evaluation of PEs in CTPA is a time-consuming and nontrivial task. In this paper, we propose an artery-aware 3D fully convolutional network (AANet) that en-codes artery information as the prior knowledge to detect arteries and PEs at the same time. In our approach, the artery context fusion block (ACF) is proposed to combine the multi-scale feature maps and generate both local and global contexts of vessels as soft attentions to precisely recognize PEs from soft tissues or veins. We evaluate our methods on the CAD-PE dataset with the artery and vein vessel labels. The experimental results with the sen-sitivity of 78.1%, 84.2%, and 85.1% at one, two, and four false positives per scan have been achieved, which shows that our method achieves state-of-the-art performance and demonstrate promising assistance for diagnosis in clinical practice.\n"}, {"title": "Accelerated pseudo 3D dynamic speech MR imaging at 3T using unsupervised deep variational manifold learning", "abstract": "Magnetic resonance imaging (MRI) of vocal tract shaping and surrounding articulators during speaking is a powerful tool in several application areas such as understanding language disorder, informing treatment plans in oro-pharyngeal cancers. However, this is a challenging task due to fundamental tradeoffs between spatio-temporal resolution, organ coverage, and signal-to-noise ratio. Current volumetric vocal tract MR methods are either restricted to image during sustained sounds, or does dynamic imaging at highly compromised spatio-temporal resolutions for slowly moving articulators. In this work, we propose a novel unsupervised deep variational manifold learning approach to recover a pseudo-3D dynamic speech dataset from sequential acquisition of multiple 2D slices during speaking.  We demonstrate pseudo-3D (or time aligned multi-slice 2D) dynamic imaging at a high temporal resolution of 18 ms capable of resolving vocal tract motion for arbitrary speech tasks. This approach jointly learns low-dimensional latent vectors corresponding to the image time frames and parameters of a 3D convolutional neural network based generator that generates volumes of the deforming vocal tract by minimizing a cost function which enforce: a) temporal smoothness on the latent vectors; b) l_1 norm based regularization on generator weights; c) latent vectors of all the slices to have zero mean and unit variance Gaussian distribution; and d) data consistency with measured k-space v/s time data. We evaluate our proposed method using in-vivo vocal tract airway datasets from two normal volunteers producing repeated speech tasks, and compare it against state of the art 2D and 3D dynamic compressed sensing (CS) schemes in speech MRI.  We finally demonstrate (for the first time) extraction of quantitative 3D vocal tract area functions from under-sampled 2D multi-slice datasets to characterize vocal tract shape changes in 3D during speech production. Code: https://github.com/rushdi-rusho/varMRI\n"}, {"title": "Accurate and Explainable Image-based Prediction Using a Lightweight Generative Model", "abstract": "Recent years have seen a growing interest in methods for predicting a variable of interest, such as a subject\u00e2\u0080\u0099s age, from individual brain scans. Although the field has focused strongly on nonlinear discriminative methods using deep learning, here we explore whether linear generative techniques can be used as practical alternatives that are easier to tune, train and interpret. The models we propose consist of (1) a causal forward model expressing the effect of variables of interest on brain morphology, and (2) a latent variable noise model, based on factor analysis, that is quick to learn and invert. In experiments estimating individuals\u00e2\u0080\u0099 age and gender from the UK Biobank dataset, we demonstrate competitive prediction performance even when the number of training subjects is in the thousands - the typical scenario in many potential applications. The method is easy to use as it has only a single hyperparameter, and directly estimates interpretable spatial maps of the underlying structural changes that are driving the predictions.\n"}, {"title": "Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers", "abstract": "Automatically measuring lesion/tumor size with RECIST (Response Evaluation Criteria In Solid Tumors) diameters and segmentation is important for computer-aided diagnosis. Although it has been studied in recent years, there is still space to improve its accuracy and robustness, such as (1) enhancing features by incorporating rich contextual information while keeping a high spatial resolution and (2) involving new tasks and losses for joint optimization. To reach this goal, this paper proposes a transformer-based network (MeaFormer, Measurement transFormer) for lesion RECIST diameter prediction and segmentation (LRDPS). It is formulated as three correlative and complementary tasks: lesion segmentation, heatmap prediction, and keypoint regression. To the best of our knowledge, it is the first time to use keypoint regression for RECIST diameter prediction. MeaFormer can enhance high-resolution features by employing transformers to capture their long-range dependencies. Two consistency losses are introduced to explicitly build relationships among these tasks for better optimization. Experiments show that MeaFormer achieves the state-of-the-art performance of LRDPS on the large-scale DeepLesion dataset and produces promising results of two downstream clinic-relevant tasks, i.e., 3D lesion segmentation and RECIST assessment in longitudinal studies.\n"}, {"title": "Accurate Corresponding Fiber Tract Segmentation via FiberGeoMap Learner", "abstract": "Fiber tract segmentation is a prerequisite for the tract-based statistical analysis and plays a crucial role in understanding brain structure and function. The previous researches mainly consist of two steps: defining and computing the similarity features of fibers, and then adopting machine learning algorithm for clustering or classification. Among them, how to define similarity is the basic premise and assumption of the whole method, and determines its potential reliability and application. The similarity features defined by previous studies ranged from geometric to anatomical, and then to functional characteristics, accordingly, the resulting fiber tracts seem more and more meaningful, while their reliability declined. Therefore, here we still adopt geometric feature for fiber tract segmentation, and put forward a novel descriptor (FiberGeoMap) for representing fiber\u00e2\u0080\u0099s geometric feature, which can depict effectively the shape and position of fiber, and can be inputted into our revised Transformer encoder network, called as FiberGeoMap Learner, which can well fully leverage the fiber\u00e2\u0080\u0099s features. Experimental results showed that the FiberGeoMap combined with FiberGeoMap Learner can effectively express fiber\u00e2\u0080\u0099s geometric features, and can differentiate the various fiber tracts, furthermore, the common fiber tracts among individuals can be identified by this method, thus avoiding additional image registration. The comparative experiments demonstrated that the proposed method had better performance than the existing methods. The code is openly available at https://github.com/Garand0o0/FiberTractSegmentation.\n"}, {"title": "ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training", "abstract": "Unsupervised domain adaptation (UDA) has been vastly explored to address domain shifts between source and target domains, by applying a well-performed model in an unlabeled target domain via supervision of a labeled source domain. Recent literature, however, has indicated that the performance is still far from satisfactory in the presence of significant domain shifts. Nonetheless, delineating a few target samples is usually manageable and particularly worthwhile, due to the substantial performance gain. Inspired by this, we aim to develop semi-supervised domain adaptation (SSDA) for medical image segmentation, which is largely underexplored. We, thus, propose to exploit both labeled source and target domain data, in addition to unlabeled target data in a unified manner. Specifically, we present a novel asymmetric co-training (ACT) framework to integrate these subsets and avoid the domination of the source domain data. Following a divide-and-conquer strategy, we explicitly decouple the label supervisions in SSDA into two asymmetric sub-tasks, including semi-supervised learning (SSL) and UDA, and leverage different knowledge from two segmentors to take into account the distinction of the source and target label supervisions. The knowledge learned in the two modules is then adaptively integrated with ACT, by iteratively teaching each other based on the confidence-aware pseudo-label. In addition, pseudo label noise is well-controlled with an exponential MixUp decay scheme for smooth propagation. Experiments on cross-modality brain tumor MRI segmentation tasks using the BraTS18 database showed, even with limited labeled target samples, ACT yielded marked improvements over UDA and state-of-the-art SSDA methods and approached an ``upper bound\u00e2\u0080\u009d of supervised joint training.\n"}, {"title": "Adaptation of Surgical Activity Recognition Models Across Operating Rooms", "abstract": "Automatic surgical activity recognition enables more intelligent surgical devices and a more efficient workflow. Integration of such technology in new operating rooms has the potential to improve care delivery to patients and decrease costs. Recent works have achieved a promising performance on surgical activity recognition; however, the lack of generalizability of these models is one of the critical barriers to the wide-scale adoption of this technology. In this work, we study the generalizability of surgical activity recognition models across operating rooms. We propose a new domain adaptation method to improve the performance of the surgical activity recognition model in a new operating room for which we only have unlabeled videos. Our approach generates pseudo labels for unlabeled video clips that it is confident about and trains the model on the augmented version of the clips. We extend our method to a semi-supervised domain adaptation setting where a small portion of the target domain is also labeled. In our experiments, our proposed method consistently outperforms the baselines on a dataset of more than 480 long surgical videos collected from two operating rooms.\n"}, {"title": "Adapting the Mean Teacher for keypoint-based lung registration under geometric domain shifts", "abstract": "Recent deep learning-based methods for medical image registration achieve results that are competitive with conventional optimization algorithms at reduced run times. However, deep neural networks generally require plenty of labeled training data and are vulnerable to domain shifts between training and test data. While typical intensity shifts can be mitigated by keypoint-based registration, these methods still suffer from geometric domain shifts, for instance, due to different fields of view. As a remedy, in this work, we present a novel approach to geometric domain adaptation for image registration, adapting a model from a labeled source to an unlabeled target domain. We build on a keypoint-based registration model, combining graph convolutions for geometric feature learning with loopy belief optimization, and propose to reduce the domain shift through self-ensembling. To this end, we embed the model into the Mean Teacher paradigm. We extend the Mean Teacher to this context by 1) adapting the stochastic augmentation scheme and 2) combining learned feature extraction with differentiable optimization. This enables us to guide the learning process in the unlabeled target domain by enforcing consistent predictions of the learning student and the temporally averaged teacher model. We evaluate the method for exhale-to-inhale lung CT registration under two challenging adaptation scenarios (DIR-Lab 4D CT to COPD, COPD to Learn2Reg). Our method consistently improves on the baseline model by 50%/47% while even matching the accuracy of models trained on target data. Source code is available at https://github.com/multimodallearning/registration-da-mean-teacher.\n"}, {"title": "Adaptive 3D Localization of 2D Freehand Ultrasound Brain Images", "abstract": "Two-dimensional (2D) freehand ultrasound is the mainstay in prenatal care and fetal growth monitoring. The task of matching corresponding cross-sectional planes in the 3D anatomy for a given 2D ultrasound brain scan is essential in freehand scanning, but challenging. We propose AdLocUI, a framework that Adaptively Localizes 2D Ultrasound Images in the 3D anatomical atlas without using any external tracking sensor.. We first train a convolutional neural network with 2D slices sampled from co-aligned 3D ultrasound volumes to predict their locations in the 3D anatomical atlas. Next, we fine-tune it with 2D freehand ultrasound images using a novel unsupervised cycle consistency, which utilizes the fact that the overall displacement of a sequence of images in the 3D anatomical atlas is equal to the displacement from the first image to the last in that sequence. We demonstrate that AdLocUI can adapt to three different ultrasound datasets, acquired with different machines and protocols, and achieves significantly better localization accuracy than the baselines. AdLocUI can be used for sensorless 2D freehand ultrasound guidance by the bedside. he source code is available at https://github.com/pakheiyeung/AdLocUI.\n"}, {"title": "AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching", "abstract": "This paper tackles the challenge of forensic medical image matching (FMIM) using deep neural networks (DNNs). FMIM is a particular case of content-based image retrieval (CBIR). The main challenge in FMIM compared to the general case of CBIR, is that the subject to whom a query image belongs may be affected by aging and progressive degenerative disorders, making it difficult to match data on a subject level. CBIR with DNNs is generally solved by minimizing a ranking loss, such as Triplet loss (TL), computed on image representations extracted by a DNN from the original data. TL, in particular, operates on triplets: anchor, positive (similar to anchor) and negative (dissimilar to anchor). Although TL has been shown to perform well in many CBIR tasks, it still has limitations, which we identify and analyze in this work. In this paper, we introduce (i) the AdaTriplet loss \u00e2\u0080\u0093 an extension of TL whose gradients adapt to different difficulty levels of negative samples, and (ii) the AutoMargin method \u00e2\u0080\u0093 a technique to adjust hyperparameters of margin-based losses such as TL and our proposed loss dynamically. Our results are evaluated on two large-scale benchmarks for FMIM based on the Osteoarthritis Initiative and Chest X-ray-14 datasets. The codes allowing replication of this study have been made publicly available at \\url{https://github.com/Oulu-IMEDS/AdaTriplet}.\n"}, {"title": "Addressing Class Imbalance in Semi-supervised Image Segmentation: A Study on Cardiac MRI", "abstract": "Due to the imbalanced and limited data, semi-supervised medical image segmentation methods often fail to produce superior performance for some specific tailed classes. Inadequate training for those particular classes could introduce more noise to the generated pseudo labels, affecting overall learning. To alleviate this shortcoming and identify the under-performing classes, we propose maintaining a confidence array that records class-wise performance during training. A fuzzy fusion of these confidence scores is proposed to adaptively prioritize individual confidence metrics in every sample rather than traditional ensemble approaches, where a set of predefined fixed weights are assigned for all the test cases. Further, we introduce a robust class-wise sampling method and dynamic stabilization for better training strategy. Our proposed method considers all the under-performing classes with dynamic weighting and tries to remove most of the noises during training. Upon evaluation on two cardiac MRI datasets, ACDC and MMWHS, our proposed method shows effectiveness and generalizability and outperforms several state-of-the-art methods found in the literature. \n"}, {"title": "Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation", "abstract": "An organ segmentation method that can generalize to unseen contrasts and scanner settings can significantly reduce the need for retraining of deep learning models. Domain Generalization (DG) aims to achieve this goal. However, most DG methods for segmentation require training data from multiple domains during training. We propose a novel adversarial domain generalization method for organ segmentation trained on data from a \\emph{single} domain. We synthesize the new domains via learning an adversarial domain synthesizer (ADS) and presume that the synthetic domains cover a large enough area of plausible distributions so that unseen domains can be interpolated from synthetic domains. We propose a mutual information regularizer to enforce the semantic consistency between images from the synthetic domains, which can be estimated by patch-level contrastive learning. We evaluate our method for various organ segmentation for unseen modalities, scanning protocols, and scanner sites.\n"}, {"title": "Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs", "abstract": "Few-shot Learning (FSL) methods are being adopted in settings where data is not abundantly available. This is especially seen in medical domains where the annotations are expensive to obtain. Deep Neural Networks have been shown to be vulnerable to adversarial attacks. This is even more severe in the case of FSL due to the lack of a large number of training examples. In this paper, we provide a framework to make few-shot segmentation models adversarially robust in the medical domain where such attacks can severely impact the decisions made by clinicians who use them.\nWe propose a novel robust few-shot segmentation framework, Prototypical Neural Ordinary Differential Equation (PNODE), that provides defense against gradient-based adversarial attacks.\nWe show that our framework is more robust compared to traditional adversarial defense mechanisms such as adversarial training. Adversarial training involves increased training time and shows robustness to limited types of attacks depending on the type of adversarial examples seen during training. Our proposed framework generalises well to common adversarial attacks like FGSM, PGD and SMIA while having the model parameters comparable to the existing few-shot segmentation models. We show the effectiveness of our proposed approach on three publicly available multi-organ segmentation datasets in both in-domain and cross-domain settings by attacking the support and query sets without the need for ad-hoc adversarial training.\n"}, {"title": "Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound", "abstract": "Standard plane (SP) localization is essential in routine clinical ultrasound (US) diagnosis. Compared to 2D US, 3D US can acquire multiple view planes in one scan and provide complete anatomy with the addition of coronal plane. However, manually navigating SPs in 3D US is laborious and biased due to the orientation variability and huge search space. In this study, we introduce a novel reinforcement learning (RL) framework for automatic SP localization in 3D US. Our contribution is three-fold. First, we formulate SP localization in 3D US as a tangent point-based problem in RL to restructure the action space and significantly reduce the search space. Second, we design an auxiliary task learning strategy to enhance the model\u00e2\u0080\u0099s ability to recognize subtle differences crossing Non-SPs and SPs in plane search. Finally, we propose a spatial-anatomical reward to effectively guide learning trajectories by exploiting spatial and anatomical information simultaneously. We explore the efficacy of our approach on localizing four SPs on uterus and fetal brain datasets. The experiments indicate that our approach achieves a high localization accuracy as well as robust performance. \n"}, {"title": "Aggregative Self-Supervised Feature Learning from Limited Medical Images", "abstract": "Limited training data and annotation shortage are the main challenges for the development of automated medical image analysis systems. As a potential solution, self-supervised learning (SSL) causes an increasing attention from the community. The key part in SSL is its proxy task that defines the supervisory signals and drives the learning toward effective feature representations. However, most SSL approaches usually focus on a single proxy task, which greatly limits the expressive power of the learned features and therefore deteriorates the network generalization capacity. In this regard, we hereby propose two strategies of aggregation in terms of complementarity of various forms to boost the robustness of self-supervised learned features. We firstly propose a principled framework of multi-task aggregative self-supervised learning from limited medical samples to form a unified representation, with an intent of exploiting feature complementarity among different tasks. Then, in self-aggregative SSL, we propose to self-complement an existing proxy task with an auxiliary loss function based on a linear centered kernel alignment metric, which explicitly promotes the exploring of where are uncovered by the features learned from a proxy task at hand to further boost the modeling capability. Our extensive experiments on 2D and 3D medical image classification tasks under limited data and annotation scenarios confirm that the proposed aggregation strategies successfully boost the classification accuracy.\n"}, {"title": "An Accurate Unsupervised Liver Lesion Detection Method Using Pseudo-Lesions", "abstract": "Anomaly detection using an unsupervised learning scheme has become a challenging research topic. Unsupervised learning requires only unlabeled normal data for training and can detect anomalies in unseen testing data. In this paper, we propose an unsupervised liver lesion detection framework based on generative adversarial networks. We present a new perspective that learning anomalies positively affect learning normal objects (e.g., liver), even if the anomalies are fake. Our framework uses normal and pseudo-lesions data training, and the pseudo-lesions data comes from normal data augmentation. We train our framework to learn to predict normal features by transferring normal and augmented data into each other. In addition, we introduce a discriminator network containing a U-Net-like architecture that extracts local and global features effectively for providing more informative feedback to the generator. Further, we also propose a novel reconstruction-error score index based on the image gradient perception pyramid. A higher error-index score indicates a lower similarity between input and output images, which means lesions detected. We conduct extensive experiments on different datasets for liver lesion detection. Our proposed method outperforms other state-of-the-art unsupervised anomaly detection methods.\n"}, {"title": "An adaptive network with extragradient for diffusion MRI-based microstructure estimation", "abstract": "Diffusion MRI (dMRI) is a powerful tool for probing tissue microstructural properties. However, advanced dMRI models are commonly nonlinear and complex, which requires densely sampled q-space and is prone to estimation errors. This problem can be resolved using deep learning techniques, especially optimization-based networks. In previous optimization-based methods, the number of iterative blocks was selected empirically. Furthermore, previous network structures were based on the iterative shrinkage-thresholding algorithm (ISTA), which could result in instability during sparse reconstruction. In this work, we proposed an adaptive network with extragradient for diffusion MRI-based microstructure estimation (AEME) by introducing an additional projection of the extra gradient, such that the convergence of the network can be guaranteed. Meanwhile, with the adaptive iterative selection module, the sparse representation process can be modeled flexibly according to specific dMRI models. The network was evaluated on the neurite orientation dispersion and density imaging (NODDI) model on 3T and 7T datasets. AEME showed superior improved accuracy and generalizability compared to other state-of-the-art microstructural estimation algorithms. \n"}, {"title": "An Advanced Deep Learning Framework for Video-based Diagnosis of ASD", "abstract": "Autism spectrum disorder (ASD) is one of the most common neurodevelopmental disorders, which impairs the communication and interaction ability of patients. Intensive intervention in early ASD can effectively improve symptoms, so the diagnosis of ASD children receives significant attention. However, clinical assessment relies on experienced diagnosticians, which makes the diagnosis of ASD children difficult to popularize, especially in remote areas. In this paper, we propose a simple yet effective pipeline to diagnose ASD children, which comprises a convenient and fast strategy of video acquisition and an advanced deep learning framework. In our framework, firstly, we extract sufficient head-related features from the collected videos by a generic toolbox. Secondly, we propose a head-related characteristic (HRC) attention mechanism to select the most discriminative disease-related features adaptively. Finally, a convolutional neural network is used to diagnose ASD children by exploring the temporal information from the selected features. We also build a video dataset based on our strategy of video acquisition that contains 82 children to verify the effectiveness of the proposed pipeline. Experiments on this dataset show that our deep learning framework achieves a superior performance of ASD children diagnosis. The code and dataset will be available at https://github.com/xiaotaiyangcmm/DASD.\n"}, {"title": "An End-to-End Combinatorial Optimization Method for R-band Chromosome Recognition with Grouping Guided Attention", "abstract": "Chromosome recognition is a critical and time-consuming process in karyotyping, especially for R-band chromosomes with poor visualization quality. Existing computer-aided chromosome recognition methods mainly focus on better feature representation of individual chromosomes while neglecting the fact that chromosomes from the same karyotype share some common distribution and are more related compared to chromosomes across different patients. In the light of such observation, we start from a global perspective and propose an end-to-end differential combinatorial optimization method for R-band chromosome recognition. To achieve this, a grouping guided feature interaction module (GFIM) is built for feature aggregation between similar chromosome instances. Specially, a mask matrix is built for self-attention computation according to chromosome length grouping information. Furthermore, a deep assignment module (DAM) is designed for flexible and differentiable label assignment. It exploits the aggregated features to infer class probability distributions between all chromosomes in a cell. Experimental results on both normal and numerically abnormal karyotypes confirmed that our method outperforms state-of-the-art chromosome recognition and label assignment methods. The code is available at: https://github.com/xiabc612/R-band-chromosome-recognition\n"}, {"title": "An Inclusive Task-Aware Framework for Radiology Report Generation", "abstract": "To avoid the tedious and laborious radiology report writing, the automatic generation of radiology reports has drawn great attention recently. Previous studies attempted to directly transfer the image captioning method to radiology report generation given the apparent similarity between these two tasks.\nAlthough these methods can generate fluent descriptions, their accuracy for abnormal structure identification is limited due to the neglecting of the highly structured property and extreme data imbalance of the radiology report generation task. Therefore, we propose a novel task-aware framework to address the above two issues, composed of a task distillation module turning the image-level report to structure-level description, a task-aware report generation module for the generation of structure-specific descriptions, along with a classification token to identify and emphasize the abnormality of each structure, and an auto-balance mask loss to alleviate the serious data imbalance between normal/abnormal descriptions as well as the imbalance among different structures. Comprehensive experiments conducted on two public datasets demonstrate that the proposed method outperforms the state-of-the-art methods by a large margin (3.5% BLEU-1 improvement on MIMIC-CXR dataset) and can effectively improve the accuracy regarding the abnormal structures. The code is available at https://github.com/Reremee/ITA.\n"}, {"title": "An Optimal Control Problem for Elastic Registration and Force Estimation in Augmented Surgery", "abstract": "The nonrigid alignment between a pre-operative biomechanical model and an intra-operative observation is a critical step to track the motion of a soft organ in augmented surgery. While many elastic registration procedures introduce artificial forces into the direct physical model to drive the registration, we propose in this paper a method to reconstruct the surface loading that actually generated the observed deformation. The registration problem is formulated as an optimal control problem where the unknown is the surface force distribution that applies on the organ and the resulting deformation is computed using an hyperelastic model. Advantages of this approach include a greater control over the set of admissible force distributions, in particular the opportunity to choose where forces should apply, thus promoting physically-consistent displacement fields. The optimization problem is solved using a standard adjoint method. We present registration results with experimental phantom data showing that our procedure is competitive in terms of accuracy. In an example of application, we estimate the forces applied by a surgery tool on the organ. Such an estimation is relevant in the context of robotic surgery systems, where robotic arms usually do not allow force measurements, and providing force feedback remains a challenge.\n"}, {"title": "Analyzing and Improving Low Dose CT Denoising Network via HU Level Slicing", "abstract": "The deep convolutional neural network has been extensively studied for medical images denoising, specifically for low dose CT(LDCT) denoising. However, most of them disregard that medical images have a large dynamic range. After normalizing the input image, the difference between two nearby HU levels becomes minimal; furthermore, after multiplying it with the floating-point weight vector, the feature response becomes insensitive to small changes in the input images. As a consequence, the denoised image becomes visually smooth. With this observation, we propose to use HU level slicing for improving the performance of the vanilla convolutional network. In our method, we first use different CT windows to slice the input image into a separate HU range. Then different CNN network is used to process each generated input slice separately. Finally, a feature fusion module combines the feature learned by each network and produces the denoised image. Extensive experiments with different state of the art methods in different training settings (both supervised and unsupervised) in three benchmark low dose CT databases validates HU level slicing can significantly improve the denoising performance of the existing methods."}, {"title": "Analyzing Brain Structural Connectivity as Continuous Random Functions", "abstract": "This work considers a continuous framework to characterize the population-level variability of structural connectivity. Our framework assumes the observed white matter fiber tract endpoints are driven by a latent random function defined over a product manifold domain. To overcome the computational challenges of analyzing such complex latent functions, we develop an efficient algorithm to construct a data-driven reduced-rank function space to represent the latent continuous connectivity. Using real data from the Human Connectome Project, we show that our method outperforms state-of-the-art approaches applied to the traditional atlas-based structural connectivity matrices on connectivity analysis tasks of interest. We also demonstrate how our method can be used to identify localized regions and connectivity patterns on the cortical surface associated with significant group differences. \n"}, {"title": "Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays", "abstract": "Creating a large-scale dataset of abnormality annotation on medical images is a labor-intensive and costly task. Leveraging weak supervision from readily available data such as radiology reports can compensate lack of large-scale data for anomaly detection methods. However, most of the current methods only use image-level pathological observations, failing to utilize the relevant anatomy mentions in reports. Furthermore, Natural Language Processing (NLP)-mined weak labels are noisy due to label sparsity and linguistic ambiguity. We propose an Anatomy-Guided chest X-ray Network (AGXNet) to address these issues of weak annotation. Our framework consists of a cascade of two networks, one responsible for identifying anatomical abnormalities and the second responsible for pathological observations. The critical component in our framework is an anatomy-guided attention module that aids the downstream observation network in focusing on the relevant anatomical regions generated by the anatomy network. We use Positive Unlabeled (PU) learning to account for the fact that lack of mention does not necessarily mean a negative label. Our quantitative and qualitative results on the MIMIC-CXR dataset demonstrate the effectiveness of AGXNet in disease and anatomical abnormality localization. Experiments on the NIH Chest X-ray dataset show that the learned feature representations are transferable and outperform the baselines in classification and localization tasks.\n"}, {"title": "Anomaly-aware multiple instance learning for rare anemia disorder classification", "abstract": "Deep learning-based classification of rare anemia disorders is challenged by the lack of training data and instance-level annotations. Multiple Instance Learning (MIL) has shown to be an effective solution, yet it suffers from low accuracy and limited explainability. Although the inclusion of attention mechanisms has addressed these issues, their effectiveness highly depends on the amount and diversity of cells in the training samples. Consequently, the poor machine learning performance on rare anemia disorder classification from blood samples remains unresolved. In this paper, we propose an interpretable pooling method for MIL to address these limitations. By benefiting from instance-level information of negative bags (i.e., homogeneous benign cells from healthy individuals), our approach increases the contribution of anomalous instances. We show that our strategy outperforms standard MIL classification algorithms and provides a meaningful explanation behind its decisions. Moreover, it can denote anomalous instances of rare blood diseases that are not seen during the training phase.\n"}, {"title": "Assessing the Performance of Automated Prediction and Ranking of Patient Age from Chest X-rays Against Clinicians", "abstract": "Understanding the internal physiological changes accompanying the aging process is an important aspect of medical image interpretation, with the expected changes acting as a baseline when reporting abnormal findings. Deep learning has recently been demonstrated to allow the accurate estimation of patient age from chest X-rays, and shows potential as a health indicator and mortality predictor. In this paper we present a novel comparative study of the relative performance of radiologists versus state-of-the-art deep learning models on two tasks: (a) patient age estimation from a single chest X-ray, and (b) ranking of two time-separated images of the same patient by age. We train our models with a heterogeneous database of 1.8M chest X-rays with ground truth patient ages and investigate the limitations on model accuracy imposed by limited training data and image resolution, and demonstrate generalisation performance on public data. To explore the large performance gap between the models and humans on these age-prediction tasks compared with other radiological reporting tasks seen in the literature, we incorporate our age prediction model into a conditional Generative Adversarial Network (cGAN) allowing visualisation of the semantic features identified by the prediction model as significant to age prediction, comparing the identified features with those relied on by clinicians.\n"}, {"title": "Asymmetry Disentanglement Network for Interpretable Acute Ischemic Stroke Infarct Segmentation in Non-Contrast CT Scans", "abstract": "Accurate infarct segmentation in non-contrast CT (NCCT) images is a crucial step toward computer-aided acute ischemic stroke (AIS) assessment. In clinical practice, bilateral symmetric comparison of brain hemispheres is usually used to locate pathological abnormalities. Recent research has explored asymmetries to assist with AIS segmentation. However, most previous symmetry-based work mixed different types of asymmetries when evaluating their contribution to AIS. In this paper, we propose a novel Asymmetry Disentanglement Network (ADN) to automatically separate pathological asymmetries and intrinsic anatomical asymmetries in NCCTs for more effective and interpretable AIS segmentation. ADN first performs asymmetry disentanglement based on input NCCTs, which produces different types of 3D asymmetry maps. Then a synthetic, intrinsic-asymmetry-compensated and pathology-asymmetry-salient NCCT volume is generated and later used as input to a segmentation network. The training of ADN incorporates domain knowledge and adopts a tissue-type aware regularization loss function to encourage clinically-meaningful pathological asymmetry extraction. Coupled with an unsupervised 3D transformation network, ADN achieves state-of-the-art AIS segmentation performance on a public NCCT dataset. In addition to the superior performance, we believe the learned clinically-interpretable asymmetry maps can also provide insights towards a better understanding of AIS assessment. Our code is available at https://github.com/nihaomiao/MICCAI22_ADN.\n"}, {"title": "Atlas-based Semantic Segmentation of Prostate Zones", "abstract": "Segmentation of the prostate into specific anatomical zones is important for radiological assessment of prostate cancer in magnetic resonance imaging (MRI). Of particular interest is segmenting the prostate into two regions of interest: the central gland (CG) and peripheral zone (PZ). In this paper, we propose to integrate an anatomical atlas of prostate zone shape into a deep learning semantic segmentation framework to segment the CG and PZ in T2-weighted MRI. Our approach in corporates anatomical information in the form of a probabilistic prostate zone atlas and utilizes a dynamically controlled hyperparameter to combine the atlas with the semantic segmentation result. In addition to providing significantly improved segmentation performance, this hyperparameter is capable of being dynamically adjusted during the inference stage to provide users with a mechanism to refine the segmentation. We validate our approach using an external test dataset and demonstrate Dice similarity coefficient values (mean\u00c2\u00b1SD) of 0.91\u00c2\u00b10.05 for the CG and 0.77\u00c2\u00b10.16 for the PZ that significantly improves upon the baseline segmentation results without the atlas. All code is publicly available on GitHub: https://github.com/OnofreyLab/prostate_atlas_segm_miccai2022."}, {"title": "Atlas-powered deep learning (ADL) - application to diffusion weighted MRI", "abstract": "Deep learning has a great potential for estimating biomarkers in diffusion weighted magnetic resonance imaging (dMRI). Atlases, on the other hand, are a unique tool for modeling the spatio-temporal variability of biomarkers. In this paper, we propose the first framework to exploit both deep learning and atlases for biomarker estimation in dMRI. Our framework relies on non-linear diffusion tensor registration to compute biomarker atlases and to estimate atlas reliability maps. We also use non-linear tensor registration to align the atlas to a subject and to estimate the error of this alignment. We use the biomarker atlas, atlas reliability map, and alignment error map, in addition to the dMRI signal, as inputs to a deep learning model for biomarker estimation. We use our framework to estimate fractional anisotropy and neurite orientation dispersion from down-sampled dMRI data on a test cohort of 70 newborn subjects. Results show that our method significantly outperforms standard estimation methods as well as recent deep learning techniques. Our method is also more robust to higher measurement down-sampling factors. Our study shows that the advantages of deep learning and atlases can be synergistically combined to achieve unprecedented biomarker estimation accuracy in dMRI.\n"}, {"title": "Attention mechanisms for physiological signal deep learning: which attention should we take?", "abstract": "Attention mechanisms are widely used to dramatically improve deep learning model performance in various fields. However, their general ability to improve the performance of physiological signal deep learning model is immature. In this study, we experimentally analyze four attention mechanisms (e.g., squeeze-and-excitation, non-local, convolutional block attention module, and multi-head self-attention) and three convolutional neural network (CNN) architectures (e.g., VGG, ResNet, and Inception) for two representative physiological signal prediction tasks: the classification for predicting hypotension and the regression for predicting cardiac output (CO). We evaluated multiple combinations for performance and convergence of physiological signal deep learning model. Accordingly, the CNN models with the spatial attention mechanism showed the best performance in the classification problem, whereas the channel attention mechanism achieved the lowest error in the regression problem. Moreover, the performance and convergence of the CNN models with attention mechanisms were better than stand-alone self-attention models in both problems. Hence, we verified that convolutional operation and attention mechanisms are complementary and provide faster convergence time, despite the stand-alone self-attention models requiring fewer parameters.\n"}, {"title": "Attentional Generative Multimodal Network for Neonatal Postoperative Pain Estimation", "abstract": "Artificial Intelligence (AI)-based methods allow for automatic assessment of pain intensity based on continuous monitoring and processing of subtle changes in sensory signals, including facial expression, body movements, and crying frequency. Currently, there is a large and growing need for expanding current AI-based approaches to the assessment of postoperative pain in the neonatal intensive care unit (NICU). In contrast to acute procedural pain in the clinic, the NICU has neonates emerging from postoperative sedation, usually intubated, and with variable energy reserves for manifesting forceful pain responses. Here, we present a novel multi-modal approach designed, developed, and validated for assessment of neonatal postoperative pain in the challenging NICU setting. Our approach includes a robust network capable of efficient reconstruction of missing modalities (e.g., obscured facial expression due to intubation) using an unsupervised spatio-temporal feature learning with a generative model for learning the joint features. Our approach generates the final pain score along with the intensity using an attentional cross-modal feature fusion. Using experimental dataset from postoperative neonates in the NICU, our pain assessment approach achieves superior performance (AUC 0.906, accuracy 0.820) as compared to the state-of-the-art approaches.\n"}, {"title": "Attention-enhanced Disentangled Representation Learning for Unsupervised Domain Adaptation in Cardiac Segmentation", "abstract": "To overcome the barriers of multimodality and scarcity of annotations in medical image segmentation, many unsupervised domain adaptation (UDA) methods have been proposed, especially in cardiac segmentation. However, these methods may not completely avoid the interference of domain-specific information. To tackle this problem, we propose a novel Attention-enhanced Disentangled Representation (ADR) learning model for UDA in cardiac segmentation. To sufficiently remove domain shift and mine more precise domain-invariant features, we first put forward a strategy from image-level coarse alignment to fine removal of remaining domain shift. Unlike previous dual path disentanglement methods, we present channel-wise disentangled representation learning to promote mutual guidance between domain-invariant and domain-specific features. Meanwhile, Hilbert-Schmidt independence criterion (HSIC) is adopted to establish the independence between the disentangled features. Furthermore, we propose an attention bias for adversarial learning in the output space to enhance the learning of task-relevant domain-invariant features. To obtain more accurate predictions during inference, an information fusion calibration (IFC) is also proposed. Extensive experiments on the MMWHS 2017 dataset demonstrate the superiority of our method. Code is available at https://github.com/Sunxy11/ADR.\n"}, {"title": "Attentive Symmetric Autoencoder for Brain MRI Segmentation", "abstract": "Self-supervised learning methods based on image patch reconstruction have witnessed great success in training auto-encoders, whose pre-trained weights can be transferred to fine-tune other downstream tasks of image understanding. However, existing methods seldom study the various importance of reconstructed patches and the symmetry of anatomical structures, when they are applied to 3D medical images. In this paper we propose a novel Attentive Symmetric Auto-encoder (ASA) based on Vision Transformer (ViT) for 3D brain MRI segmentation tasks. We conjecture that forcing the auto-encoder to recover informative image regions can harvest more discriminative representations, than to recover smooth image patches. Then we adopt a gradient based metric to estimate the importance of each image patch. In the pre-training stage, the proposed auto-encoder pays more attention to reconstruct the informative patches according to the gradient metrics. Moreover, we resort to the prior of brain structures and develop a Symmetric Position Encoding (SPE) method to better exploit the correlations between long-range but spatially symmetric regions to obtain effective features. Experimental results show that our proposed attentive symmetric auto-encoder outperforms the state-of-the-art self-supervised learning methods and medical image segmentation models on three brain MRI segmentation benchmarks. All codes and model weights will be made available.\n"}, {"title": "Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging", "abstract": "Image corruption by motion artifacts is an ingrained problem in Magnetic Resonance Imaging (MRI). In this work, we propose a neural network-based regularization term to enhance Autofocusing, a classic optimization-based method to remove motion artifacts. The method takes the best of both worlds: the optimization-based routine iteratively executes the blind demotion and deep learning-based prior penalizes for unrealistic restorations and speeds up the convergence. We validate the method on three models of motion trajectories, using synthetic and real noisy data. The method proves resilient to noise and anatomic structure variation, outperforming the state-of-the-art demotion methods."}, {"title": "AutoGAN-Synthesizer: Neural Architecture Search for Cross-Modality MRI Synthesis", "abstract": "Considering the difficulty to obtain complete multi-modality MRI scans in some real-world data acquisition situations, synthesizing MRI data is a highly relevant and important topic to complement diagnosis information in clinical practice. In this study, we present a novel MRI synthesizer, called AutoGAN-Synthesizer, which automatically discovers generative networks for cross-modality MRI synthesis. Our AutoGAN-Synthesizer adopts gradient-based search strategies to explore the generator architecture by determining how to fuse multi-resolution features and utilizes GAN-based perceptual searching losses to handle the trade-off between model complexity and performance. Our AutoGAN-Synthesizer can search for a remarkable and light-weight architecture with 6.31 Mb parameters only occupying 12 GPU hours. Moreover, to incorporate richer prior knowledge for MRI synthesis, we derive K-space features containing the low- and high-spatial frequency information and incorporate such features into our model. To our best knowledge, this is the first work to explore AutoML for cross-modality MRI synthesis, and our approach is also capable of tailoring networks given either different multiple modalities or just a single modality as input.  Extensive experiments show that our AutoGAN-Synthesizer outperforms the state-of-the-art MRI synthesis methods both quantitatively and qualitatively. Code will be made publicly available.\n"}, {"title": "AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy", "abstract": "Computer-assisted minimally invasive surgery has great potential in benefiting modern operating theatres. The video data streamed from the endoscope provides rich information to support context-awareness for next-generation intelligent surgical systems. To achieve accurate perception and automatic manipulation during the procedure, learning based technique is a promising way, which enables advanced image analysis and scene understanding in recent years. However, learning such models highly relies on large-scale, high-quality, and multi-task labelled data. This is currently a bottleneck for the topic, as available public dataset is still extremely limited in the field of CAI. In this paper, we present and release the first integrated dataset (named AutoLaparo) with multiple image-based perception tasks to facilitate learning-based automation in hysterectomy surgery. Our AutoLaparo dataset is developed based on full-length videos of entire hysterectomy procedures. Specifically, three different yet highly correlated tasks are formulated in the dataset, including surgical workflow recognition, laparoscope motion prediction, and instrument and key anatomy segmentation. In addition, we provide experimental results with state-of-the-art models as reference benchmarks for further model developments and evaluations on this dataset. The dataset is available at \\url{https://autolaparo.github.io}.\n"}, {"title": "Automated Classification of General Movements in Infants Using Two-stream Spatiotemporal Fusion Network", "abstract": "The assessment of general movements (GMs) in infants is a useful tool in the early diagnosis of neurodevelopmental disorders. However, its evaluation in clinical practice relies on visual inspection by experts, and an automated solution is eagerly awaited. Recently, video-based GMs classification has attracted attention, but this approach would be strongly affected by irrelevant information, such as background clutter in the video. Furthermore, for reliability, it is necessary to properly extract the spatiotemporal features of infants during GMs. In this study, we propose an automated GMs classification method, which consists of preprocessing networks that remove unnecessary background information from GMs videos and adjust the infant\u00e2\u0080\u0099s body position, and a subsequent motion classification network based on a two-stream structure. The proposed method can efficiently extract the essential spatiotemporal features for GMs classification while preventing overfitting to irrelevant information for different recording environments. We validated the proposed method using videos obtained from 100 infants. The experimental results demonstrate that the proposed method outperforms several baseline models and the existing methods.\n"}, {"title": "Automatic Detection of Steatosis in Ultrasound Images with Comparative Visual Labeling", "abstract": "A common difficulty in computer-assisted diagnosis is acquiring accurate and representative labeled data, required to train, test and monitor models. Concerning liver steatosis detection in ultrasound (US) images, labeling images with human annotators can be error-prone because of subjectivity and decision boundary biases. To overcome these limits, we propose comparative visual labeling (CVL), where an annotator labels the relative degree of a pathology in image pairs, that is combined with a RankNet to give per-image diagnostic scores. In a multi-annotator evaluation on a public steatosis dataset, CVL+RankNet  significantly improves label quality compared to conventional single-image visual labeling (SVL)  (0.97 versus 0.87 F1-score respectively, 95% CI significance). This is the first application of CVL for diagnostic medical image labeling, and it may stimulate more research for other diagnostic labeling tasks. We also show that Deep Learning (DL) models trained with CVL+RankNet or histopathology labels attain similar performance. Our code and data will be made publicly available. \n"}, {"title": "Automatic identification of segmentation errors for radiotherapy using geometric learning", "abstract": "Automatic segmentation of organs-at-risk (OARs) in CT scans using convolutional neural networks (CNNs) is being introduced into the radiotherapy workflow. However, these segmentations still require manual editing and approval by clinicians prior to clinical use, which can be time consuming. The aim of this work was to develop a tool to automatically identify errors in 3D OAR segmentations without a ground truth. Our tool uses a novel architecture combining a CNN and graph neural network (GNN) to leverage the segmentation\u00e2\u0080\u0099s appearance and shape. The proposed model was trained using data-efficient learning using a synthetically-generated dataset of segmentations of the parotid gland with realistic contouring errors. The effectiveness of our model was assessed with ablation tests, evaluating the efficacy of different portions of the architecture as well as the use of transfer learning from a custom pretext task. Our best performing model predicted errors on the parotid gland with a precision of 85.0% & 89.7% for internal and external errors respectively, and recall of 66.5% & 68.6%. This offline QA tool could be used in the clinical pathway, potentially decreasing the time clinicians spend correcting contours by detecting regions which require their attention. All our code is publicly available at https://github.com/rrr-uom-projects/contour_auto_QATool\n"}, {"title": "Automatic Segmentation of Hip Osteophytes in DXA Scans using U-Nets", "abstract": "Osteophytes are distinctive radiographic features of osteoarthritis (OA) in the form of small bone spurs protruding from joints that contribute significantly to symptoms. Identifying the genetic determinants of osteophytes would improve the understanding of their biological pathways and contributions to OA. To date, this has not been possible due to the costs and challenges associated with manually outlining osteophytes in sufficiently large datasets. Automatic systems that can segment osteophytes would pave the way for this research and also have potential clinical applications. We propose, to the best of our knowledge, the first work on automating pixel-wise segmentation of osteophytes in hip dual-energy x-ray absorptiometry scans (DXAs). Based on U-Nets, we developed an automatic system to detect and segment osteophytes at the superior and the inferior femoral head, and the lateral acetabulum. The system achieved sensitivity, specificity, and average Dice scores (\u00c2\u00b1std) of (0.98, 0.92, 0.71\u00c2\u00b10.19) for the superior femoral head [793 DXAs], (0.96, 0.85, 0.66\u00c2\u00b10.24) for the inferior femoral head [409 DXAs], and (0.94, 0.73, 0.64\u00c2\u00b10.24) for the lateral acetabulum [760 DXAs]. This work enables large-scale genetic analyses of the role of osteophytes in OA, and opens doors to using low-radiation DXAs for screening for radiographic hip OA.\n"}, {"title": "Automating Blastocyst Formation and Quality Prediction in Time-Lapse Imaging with Adaptive Key Frame Selection", "abstract": "Effective approaches for accurately predicting the developmental potential of embryos and selecting suitable embryos for blastocyst culture are critically needed. Many deep learning (DL) based methods for time-lapse monitoring (TLM) videos have been proposed to tackle this problem. Although fruitful, these methods are either ineffective when processing long TLM videos, or need extra annotations to determine the morphokinetics parameters of embryos. In this paper, we propose Adaptive Key Frame Selection (AdaKFS), a new framework that adaptively selects informative frames on per-input basis to predict blastocyst formation using TLM videos at the cleavage stage on day 3. For each time step, a policy network decides whether to use or skip the current frame. Further, a prediction network generates prediction using the morphokinetics features of the selected frames. We efficiently train and enhance the frame selection process by using a Gumbel-Softmax sampling approach and a reward function, respectively. Comprehensive experiments on a large TLM video dataset verify the performance superiority of our new method over state-of-the-art methods.\n"}, {"title": "Automation of clinical measurements on radiographs of children's hips", "abstract": "Developmental dysplasia of the hip (DDH) and cerebral palsy (CP) related hip migration are two of the most common orthopaedic diseases in children, each affecting around 1-2 in 1000 children. For both of these conditions, early detection is a key factor in long term outcomes for patients. However, early signs of the disease are often missed and manual monitoring of routinely collected radiographs is time-consuming and susceptible to inconsistent measurement. We propose an automatic system for calculating acetabular index (AcI) and Reimer\u00e2\u0080\u0099s migration percentage (RMP) from paediatric hip radiographs. The system applies Random Forest regression-voting to fully automatically locate the landmark points necessary for the calculation of the clinical metrics. We show that the fully automatically obtained AcI and RMP measurements are in agreement with manual measurements obtained by clinical experts, and have replicated these findings in a clinical dataset. Such a system allows for the reliable and consistent monitoring of DDH and CP patients, aiming to improve patient outcomes through hip surveillance programmes.\n"}, {"title": "BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video", "abstract": "Predicting fetal weight at birth is an important aspect of perinatal care, particularly in the context of antenatal management, which includes the planned timing and the mode of delivery. Accurate prediction of weight using prenatal ultrasound is challenging as it requires images of specific fetal body parts during advanced pregnancy which is difficult to capture due to poor quality of images caused by the lack of amniotic fluid. As a consequence, predictions which rely on standard methods often suffer from significant errors. In this paper we propose the Residual Transformer Module which extends a 3D ResNet-based network for analysis of 2D+t spatio-temporal ultrasound video scans. Our end-to-end method, called BabyNet, automatically predicts fetal birth weight based on fetal ultrasound video scans. We evaluate BabyNet using a dedicated clinical set comprising 225 2D fetal ultrasound videos of pregnancies from 75 patients performed one day prior to delivery. Experimental results show that BabyNet outperforms several state-of-the-art methods and estimates the weight at birth with accuracy comparable to human experts. Furthermore, combining estimates provided by human experts with those computed by BabyNet yields the best results, outperforming either of other methods by a significant margin. The source code of BabyNet is available at https://github.com/SanoScience/BabyNet.\n"}, {"title": "Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery", "abstract": "This paper reports a CPU-level real-time stereo matching method for surgical images (10 Hz on 640*480 image with a single core of i5-9400). The proposed method is built on the fast LK algorithm, which estimates the disparity of the stereo images patch-wisely and in a coarse-to-fine manner. We propose a Bayesian framework to evaluate the probability of the optimized patch disparity at different scales. Moreover, we introduce a spatial Gaussian mixed probability distribution to address the pixel-wise probability within the patch. In-vivo and synthetic experiments show that our method can handle ambiguities resulted from the textureless surfaces and the photometric inconsistency caused by the non-Lambertian reflectance. Our Bayesian method correctly balances the probability of the patch for stereo images at different scales. Experiments indicate that the estimated depth has similar accuracy and fewer outliers than the baseline methods in the surgical scenario with real-time performance. The C++ code is attached.\n"}, {"title": "Bayesian Pseudo Labels: Expectation Maximization for Robust and Efficient Semi-Supervised Segmentation", "abstract": "This paper concerns pseudo labelling in segmentation. Our contribution is fourfold. Firstly, we present a new formulation of pseudo-labelling as an Expectation-Maximization (EM) algorithm for clear statistical interpretation. Secondly, we propose a semi-supervised medical image segmentation method purely based on the original pseudo labelling, namely SegPL. We demonstrate SegPL is a competitive approach against state-of-the-art consistency regularisation based methods on semi-supervised segmentation on a 2D multi-class MRI brain tumour segmentation task and a 3D binary CT lung vessel segmentation task. The simplicity of SegPL allows less computational cost comparing to prior methods. Thirdly, we demonstrate that the effectiveness of SegPL may originate from its robustness against out-of-distribution noises and adversarial attacks. Lastly, under the EM framework, we introduce a probabilistic generalisation of SegPL via variational inference, which learns a dynamic threshold for pseudo labelling during the training. We show that SegPL with variational inference can perform uncertainty estimation on par with the gold-standard method Deep Ensemble.\n"}, {"title": "Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology", "abstract": "DOI: "}, {"title": "BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis", "abstract": "Vision-and-language (V&L) models take image and text as input and learn to capture the associations between them. These models can potentially deal with the tasks that involve understanding medical images along with their associated text. However, applying V&L models in the medical domain is challenging due to the expensiveness of data annotations and the requirements of domain knowledge. In this paper, we identify that the visual representation in general V&L models is not suitable for processing medical data. To overcome this limitation, we propose BERTHop, a transformer-based model based on PixelHop++ and VisualBERT for better capturing the associations between clinical notes and medical images."}, {"title": "Bi-directional Encoding for Explicit Centerline Segmentation by Fully-Convolutional Networks", "abstract": "Localization of tube-shaped objects is an important topic in medical imaging. Previously it was mainly addressed via dense segmentation that may produce inconsistent results for long and narrow objects. In our work, we propose a point-based approach for explicit centerline segmentation that can be learned by fully-convolutional networks. We propose a new bi-directional encoding scheme that does not require any autoregressive blocks and is robust to various shapes and orientations of lines, being adaptive to the number of points in their centerlines. We present extensive evaluation of our approach on synthetic and real data (chest x-ray and coronary angiography) and show its advantage over the state-of-the-art segmentation models.\n"}, {"title": "BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes", "abstract": "Fetal growth assessment from ultrasound is based on a few biometric measurements that are performed manually and assessed relative to the expected gestational age. Reliable biometry estimation depends on the precise detection of landmarks in standard ultrasound planes. Manual annotation can be a time-consuming and operator dependent task, and may results in high measurements variability. Existing methods for automatic fetal biometry rely on initial automatic fetal structure segmentation followed by  geometric landmark detection. However, segmentation annotations are time-consuming and may be inaccurate, and landmark detection requires developing measurement-specific geometric methods. \nThis paper describes BiometryNet, an end-to-end landmark regression framework for fetal biometry estimation that overcomes these limitations. It includes a novel Dynamic Orientation Determination (DOD) method for enforcing measurement-specific orientation consistency during network training. DOD reduces variabilities in network training, increases landmark localization accuracy, thus yields accurate and robust biometric measurements.\nTo validate our method, we assembled a dataset of 3,398 ultrasound images from 1,829 subjects acquired in three clinical sites with seven different ultrasound machines. Comparison and cross-validation of three different biometric measurements on two independent datasets\nshows that BiometryNet is robust and yields accurate measurements whose errors are lower than the clinically permissible errors, outperforming other existing automated biometry estimation methods. Code is available at https://github.com/netanellavisdris/fetalbiometry.\n"}, {"title": "BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning", "abstract": "We propose a method for estimating the bone mineral density (BMD) from a plain x-ray image. Dual-energy X-ray absorptiometry (DXA) and quantitative computed tomography (QCT) provide high accuracy in diagnosing osteoporosis; however, these modalities require special equipment and scan protocols. Measuring BMD from an x-ray image provides an opportunistic screening, which is potentially useful for early diagnosis. The previous methods that directly learn the relationship between x-ray images and BMD require a large training dataset to achieve high accuracy because of large intensity variations in the x-ray images. Therefore, we propose an approach using the QCT for training a generative adversarial network (GAN) and decomposing an x-ray image into a projection of bone-segmented QCT. The proposed hierarchical learning improved the robustness and accuracy of quantitatively decomposing a small-area target. The evaluation of 200 patients with osteoarthritis using the proposed method, which we named BMD-GAN, demonstrated a Pearson correlation coefficient of 0.888 between the predicted and ground truth DXA-measured BMD. Besides not requiring a large-scale training database, another advantage of our method is its extensibility to other anatomical areas, such as the vertebrae and rib bones.\n"}, {"title": "Boundary-Enhanced Self-Supervised Learning for Brain Structure Segmentation", "abstract": "To alleviate the demand for large amount of annotated data by deep learning methods, this paper explores self-supervised learning (SSL) for brain structure segmentation. Most SSL methods treat all pixels equally, failing to emphasize the boundaries that are important clues for segmentation. We propose Boundary-Enhanced Self-Supervised Learning (BE-SSL), leveraging supervoxel segmentation and registration as two related proxy tasks. The former task enables capture boundary information by reconstructing distance transform map transformed from supervoxels. The latter task further enhances the boundary with semantics by aligning tissues and organs in registration. Experiments on CANDI and LPBA40 datasets have demonstrate that our method outperforms current SOTA methods by  0.89\\% and 0.47\\% respectively. Our code is available at https://github.com/changfeng3168/BE-SSL.\n"}, {"title": "BoxPolyp: Boost Generalized Polyp Segmentation using Extra Coarse Bounding Box Annotations", "abstract": "Accurate polyp segmentation is of great importance for colorectal cancer diagnosis and treatment. However, due to the high cost of producing accurate mask annotations, existing polyp segmentation methods suffer from severe data shortage and impaired model generalization. Reversely, coarse polyp bounding box annotations are more accessible. Thus, in this paper, we propose a boosted BoxPolyp model to make full use of both accurate mask and extra coarse box annotations. In practice, box annotations are applied to alleviate the over-fitting issue of previous polyp segmentation models, which generate fine-grained polyp area through the iterative boosted segmentation model. To achieve this goal, a fusion filter sampling (FFS) module is firstly proposed to generate pixel-wise pseudo labels from box annotations with less noise, leading to significant performance improvements. Besides, considering the appearance consistency of the same polyp, an image consistency (IC) loss is designed.  Such IC loss explicitly narrows the distance between features extracted by two different networks, which improves the robustness of the model. Note that our BoxPolyp is a plug-and-play model, which can be merged into any appealing backbone. Quantitative and qualitative experimental results on five challenging benchmarks confirm that our proposed model outperforms previous state-of-the-art methods by a large margin.\n"}, {"title": "Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer\u00e2\u0080\u0099s Disease", "abstract": "We propose a novel framework for Alzheimer\u00e2\u0080\u0099s disease (AD) detection using brain MRIs. The framework starts with a data augmentation method called Brain-Aware Replacements (BAR), which leverages a standard brain parcellation to replace medically-relevant 3D brain regions in an anchor MRI from a randomly picked MRI to create synthetic samples. Ground truth "}, {"title": "Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training", "abstract": "When reading images, radiologists generate text reports describing the findings therein. Current state-of-the-art computer-aided diagnosis tools utilize a fixed set of predefined categories automatically extracted from these medical reports for training. This form of supervision limits the potential usage of models as they are unable to pick up on anomalies outside of their predefined set, thus, making it a necessity to retrain the classifier with additional data when faced with novel classes. In contrast, we investigate direct text supervision to break away from this closed set assumption. By doing so, we avoid noisy label extraction via text classifiers and incorporate more contextual information.\n    We employ a contrastive global-local dual-encoder architecture to learn concepts directly from unstructured medical reports while maintaining its ability to perform free form classification. \n    We investigate relevant properties of open set recognition for radiological data and propose a method to employ currently weakly annotated data into training.\n    We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR, CheXpert, and ChestX-Ray14 for disease classification. We show that despite using unstructured medical report supervision, we perform on par with direct label supervision through a sophisticated inference setting. \n"}, {"title": "Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection", "abstract": "Ischemic strokes are often caused by large vessel occlusions (LVOs), which can be visualized and diagnosed with Computed Tomography Angiography scans. As time is brain, a fast, accurate and automated diagnosis of these scans is desirable. Human readers compare the left and right hemispheres in their assessment of strokes. A large training data set is required for a standard deep learning-based model to learn this strategy from data. As labeled medical data in this field is rare, other approaches need to be developed. To both include the prior knowledge of side comparison and increase the amount of training data, we propose an augmentation method that generates artificial training samples by recombining vessel tree segmentations of the hemispheres or hemisphere subregions from different patients. The subregions cover vessels commonly affected by LVOs, namely the internal carotid artery (ICA) and middle cerebral artery (MCA). In line with the augmentation scheme, we use a 3D-DenseNet fed with task-specific input, fostering a side-by-side comparison between the hemispheres. Furthermore, we propose an extension of that architecture to process the individual hemisphere subregions. All configurations predict the presence of an LVO, its side, and the affected subregion. We show the effect of recombination as an augmentation strategy in a 5-fold cross validated ablation study. We enhanced the AUC for patient-wise classification regarding the presence of an LVO of all investigated architectures. For one variant, the proposed method improved the AUC from 0.73 without augmentation to 0.89. The best configuration detects LVOs with an AUC of 0.91, LVOs in the ICA with an AUC of 0.96, and in the MCA with 0.91 while accurately predicting the affected side. \n"}, {"title": "CACTUSS: Common Anatomical CT-US Space for US examinations", "abstract": "Abdominal aortic aneurysm (AAA) is a vascular disease in\nwhich a section of the aorta enlarges, weakening its walls and potentially\nrupturing the vessel. Abdominal ultrasound has been utilized for\ndiagnostics, but due to its limited image quality and operator dependency,\nCT scans are usually required for monitoring and treatment planning.\nRecently, abdominal CT datasets have been successfully utilized to\ntrain deep neural networks for automatic aorta segmentation. Knowledge\ngathered from this solved task could therefore be leveraged to improve\nUS segmentation for AAA diagnosis and monitoring. To this end, we\npropose CACTUSS: a common anatomical CT-US space, which acts as\na virtual bridge between CT and US modalities to enable automatic\nAAA screening sonography. CACTUSS makes use of publicly available\nlabelled data to learn to segment based on an intermediary representation\nwhich inherits properties from both US and CT. We train a segmentation\nnetwork in this new representation and employ an additional\nimage-to-image translation network which enables our model to perform\non real B-mode images. Quantitative comparisons against fully supervised\nmethods demonstrate the capabilities of CACTUSS in terms of\nDice Score and diagnostic metrics, showing that our method also meets\nthe clinical requirements for AAA scanning and diagnosis.\n"}, {"title": "Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation", "abstract": "Segmentation of 3D knee MR images is important for the assessment of osteoarthritis. Like other medical data, the volume-wise labeling of knee MR images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL), particularly barely-supervised learning, is highly desirable for training with insufficient labeled data. We observed that the class imbalance problem is severe in the knee MR images as the cartilages only occupy 6% of foreground volumes, and the situation becomes worse without sufficient labeled data. To address the above problem, we present a novel framework for barely-supervised knee segmentation with noisy and imbalanced labels. Our framework leverages label distribution to encourage the network to put more effort into learning cartilage parts. Specifically, we utilize 1.) label quantity distribution for modifying the objective loss function to a class-aware weighted form and 2.) label position distribution for constructing a cropping probability mask to crop more sub-volumes in cartilage areas from both labeled and unlabeled inputs. In addition, we design dual uncertainty-aware sampling supervision to enhance the supervision of low-confident categories for efficient unsupervised learning. Experiments show that our proposed framework brings significant improvements by incorporating the unlabeled data and alleviating the problem of class imbalance. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting.\n"}, {"title": "Calibration of Medical Imaging Classification Systems with Weight Scaling", "abstract": "Calibrating neural networks is crucial in medical analysis applications where the decision making depends on the predicted probabilities. Modern neural networks are not well calibrated and they tend to overestimate probabilities when compared to the expected accuracy. This results in a misleading reliability that corrupts our decision policy. We define a weight scaling calibration method that computes a convex combination of the network output class distribution and the uniform distribution. The weights control the confidence of the calibrated prediction. The most suitable weight is found as a function of the given confidence. We derive an optimization method that is based on a closed form solution for the optimal weight scaling in each bin of a discretized value of the prediction confidence. We report experiments on a variety of medical image datasets and network architectures. This approach achieves state-of-the-art calibration with a guarantee that the classification accuracy is not altered.\n"}, {"title": "Camera Adaptation for Fundus-Image-Based CVD Risk Estimation", "abstract": "Recent studies have validated the association between cardiovascular disease (CVD) risk and retinal fundus images. Combining deep learning (DL) and portable fundus cameras will enable CVD risk estimation in various scenarios and improve healthcare democratization. However, there are still significant issues to be solved. One of the top priority issues is the different camera differences between the databases for research material and the samples in the production environment. Most high-quality retinography databases ready for research are collected from high-end fundus cameras, and there is a significant domain discrepancy between different cameras. To fully explore the domain discrepancy issue, we first collect a Fundus Camera Paired (FCP) dataset containing pair-wise fundus images captured by the high-end Topcon retinal camera and the low-end Mediwork portable fundus camera of the same patients. Then, we propose a cross-laterality feature alignment pre-training scheme and a self-attention camera adaptor module to improve the model robustness. The cross-laterality feature alignment training encourages the model to learn common knowledge from the same patient\u00e2\u0080\u0099s left and right fundus images and improve model generalization. Meanwhile, the device adaptation module learns feature transformation from the target domain to the source domain. We conduct comprehensive experiments on both the UK Biobank database and our FCP data. The experimental results show that the CVD risk regression accuracy and the result consistency over two cameras are improved with our proposed method. The code is available here: https://github.com/linzhlalala/CVD-risk-based-on-retinal-fundus-images\n"}, {"title": "Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction", "abstract": "Reconstructing 3D objects from 2D images is both challenging for our brains and machine learning algorithms. To support this spatial reasoning task, contextual information about the overall shape of an object is critical. However, such information is not captured by established loss terms (e.g. Dice loss). We propose to complement geometrical shape information by including multi-scale topological features, such as connected components, cycles, and voids, in the reconstruction loss. Our method uses cubical complexes to calculate topological features of 3D volume data and employs an optimal transport distance to guide the reconstruction process. This topology-aware loss is fully differentiable, computationally efficient, and can be added to any neural network. We demonstrate the utility of our loss by incorporating it into SHAPR, a model for predicting the 3D cell shape of individual cells based on 2D microscopy images. Using a hybrid loss that leverages both geometrical and topological information of single objects to assess their shape, we find that topological information substantially improves the quality of reconstructions, thus highlighting its ability to extract more relevant features from image datasets.\n"}, {"title": "Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis", "abstract": "The increasing energy consumption and carbon footprint of deep learning (DL) due to growing compute requirements has become a cause of concern. In this work, we focus on the carbon footprint of developing DL models for medical image analysis (MIA), where volumetric images of high spatial resolution are handled. \nIn this study, we present and compare  the features of four tools from literature to quantify the carbon footprint of DL. Using one of these tools we estimate the carbon footprint of medical image segmentation pipeline. We choose nnU-net as the proxy for a medical image segmentation pipeline and experiment on three common datasets. With our work we hope to inform on the increasing energy costs incurred by MIA. We discuss simple strategies to cut-down the environmental impact that can make model selection and training processes more efficient.\n"}, {"title": "CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data", "abstract": "Vision-based segmentation of the robotic tool during robot-assisted surgery enables downstream applications, such as augmented reality feedback, while allowing for inaccuracies in robot kinematics. With the introduction of deep learning, many methods were presented to solve instrument segmentation directly and solely from images. While these approaches made remarkable progress on benchmark datasets, fundamental challenges pertaining to their robustness remain. We present CaRTS, a causality-driven robot tool segmentation algorithm, that is designed based on a complementary causal model of the robot tool segmentation task. Rather than directly inferring segmentation masks from observed images, CaRTS iteratively aligns tool models with image observations by updating the initially incorrect robot kinematic parameters through forward kinematics and differentiable rendering to optimize image feature similarity end-to-end. We benchmark CaRTS with competing techniques on both synthetic as well as real data from the dVRK, generated in precisely controlled scenarios to allow for counterfactual synthesis. On training-domain test data, CaRTS achieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when tested on counterfactually altered test data, exhibiting low brightness, smoke, blood, and altered background patterns. This compares favorably to Dice scores of 95.0 and 86.7, respectively, of the SOTA image-based method. Future work will involve accelerating CaRTS to achieve video framerate and estimating the impact occlusion has in practice. Despite these limitations, our results are promising: In addition to achieving high segmentation accuracy, CaRTS provides estimates of the true robot kinematics, which may benefit applications such as force estimation. Code is available at: https://github.com/hding2455/CaRTS\n"}, {"title": "CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis", "abstract": "Modeling temporal changes in subcortical structures is crucial for a better understanding of the progression of Alzheimer\u00e2\u0080\u0099s disease (AD). Given their flexibility to adapt to heterogeneous sequence lengths, mesh-based transformer architectures have been proposed in the past for predicting hippocampus deformations across time. However, one of the main limitations of transformers is the large amount of trainable parameters, which makes the application on small datasets very challenging. In addition, current methods do not include relevant non-image information that can help  to identify AD-related patterns in the progression.\nTo this end, we introduce CASHformer, a transformer-based framework to model longitudinal shape trajectories in AD. \nCASHformer incorporates the idea of pre-trained transformers as universal compute engines that generalize across a wide range of tasks by freezing most layers during fine-tuning. \nThis reduces the number of parameters by over 90% with respect to the original model and therefore enables the application of large models on small datasets without overfitting.In addition, CASHformer models cognitive decline to reveal AD atrophy patterns in the temporal sequence.Our results show that CASHformer reduces the reconstruction error by 73% compared to previously proposed methods. \nMoreover, the accuracy of detecting patients progressing to AD increases by 3% with imputing missing longitudinal shape data.\n"}, {"title": "Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images", "abstract": "Survival time prediction from medical images is important for treatment planning, where accurate estimations can improve healthcare quality. One issue affecting the training of survival models is censored data. Most of the current survival prediction approaches are based on Cox models that can deal with censored data, but their application scope is limited because they output a hazard function instead of a survival time. On the other hand, methods that predict survival time usually ignore censored data, resulting in an under-utilization of the training set. In this work, we propose a new training method that predicts survival time using all censored and uncensored data. We propose to treat censored data as samples with a lower-bound time to death and estimate pseudo labels to semi-supervise a censor-aware survival time regressor. We evaluate our method on pathology and x-ray images from the TCGA-GM and NLST datasets. Our results establish the state-of-the-art survival prediction accuracy on both datasets.\n"}, {"title": "CephalFormer: Incorporating Global Structure Constraint into Visual Features for General Cephalometric Landmark Detection", "abstract": "Accurate cephalometric landmark detection is a crucial step in orthodontic diagnosis and therapy planning. However, existing deep learning-based methods lack the ability to explicitly model the complex dependencies among visual features and landmarks. Therefore, they fail to adaptively encode the landmark\u00e2\u0080\u0099s global structure constraint into the representation of visual concepts and suffer from large biases in landmark localization. In this work, we propose CephalFormer, which exploits the correlations between visual concepts and landmarks to provide meaningful guidance for accurate 2D and 3D cephalometric landmark detection. CephalFormer explores local-global anatomical contents in a coarse-to-fine fashion and consists of two stages: (1) a new efficient Transformer-based architecture for coarse landmark localization; (2) a novel paradigm based on self-attention to represent visual clues and landmarks in one coherent feature space for fine-scale landmark detection. We evaluated CephalFormer on two public cephalometric landmark detection benchmarks and a real-patient dataset consisting of 150 skull CBCT volumes. Experiments show that CephalFormer significantly outperforms the state-of-the-art methods, demonstrating its generalization capability and stability to naturally handle both 2D and 3D scenarios under a unified framework.\n"}, {"title": "Cerebral Microbleeds Detection Using a 3D Feature Fused Region Proposal Network with Hard Sample Prototype Learning", "abstract": "Cerebral Microbleeds (CMBs) are chronic deposits of small blood products in the brain tissues, which have explicit relation to cerebrovascular diseases, including cognitive decline, intracerebral hemorrhage, and cerebral infarction. However, manual detection of the CMBs is a time-consuming and error-prone process be-cause of the sparse and tiny properties of CMBs. Also, the detection of CMBs is commonly affected by the existence of many CMB mimics that cause a high false-positive rate (FPR), such as calcification, iron depositions, and pial vessels. This paper proposes an efficient single-stage deep learning framework for the au-tomatic detection of CMBs. The structure consists of a 3D U-Net employed as a backbone and Region Proposal Network (RPN). To significantly reduce the FPs, we developed a new scheme, containing Feature Fusion Module (FFM) that greatly detects small candidates utilizing contextual information and Hard Sample Prototype Learning (HSPL) that mines CMB mimics and generates additional loss term called concentration loss using Convolutional Prototype Learning (CPL). The proposed network utilizes Susceptibility-Weighted Imaging (SWI) and phase images as 3D input to efficiently capture 3D information. The proposed model was trained and tested using data containing 114 subjects with 365 CMBs. The performance of vanilla RPN shows a sensitivity of 93.33% and an average number of false positives per subject (FPavg) of 14.73. In contrast, the proposed Feature Fused RPN that utilizes the HSPL outperforms the vanilla RPN and achieves a sensitivity of 94.66% and FPavg of 0.86.\n"}, {"title": "CFDA: Collaborative Feature Disentanglement and Augmentation for Pulmonary Airway Tree Modeling of COVID-19 CTs", "abstract": "Detailed modeling of the airway tree from CT scan is important for 3D navigation involved in endobronchial intervention including for those patients infected with the novel coronavirus. Deep learning methods have the potential for automatic airway segmentation but require large annotated datasets for training, which is difficult for a small patient population and rare cases. Due to the unique attributes of noisy COVID-19 CTs (e.g., ground-glass opacity and consolidation), vanilla 3D Convolutional Neural Networks (CNNs) trained on clean CTs are difficult to be generalized to noisy CTs. In this work, a Collaborative Feature Disentanglement and Augmentation framework (CFDA) is proposed to harness the intrinsic topological knowledge of the airway tree from clean CTs incorporated with unique bias features extracted from the noisy CTs. Firstly, we utilize the clean CT scans and a small amount of labeled noisy CT scans to jointly acquire a bias-discriminative encoder. Feature-level augmentation is then designed to perform feature sharing and augmentation, which diversifies the training samples and increases the generalization ability. Detailed evaluation results on patient datasets demonstrated considerable improvements in the CFDA network. It has been shown that the proposed method achieves superior segmentation performance of airway in COVID-19 CTs against other state-of-the-art transfer learning methods.\n"}, {"title": "Characterization of brain activity patterns across states of consciousness based on variational auto-encoders", "abstract": "Decoding the levels of consciousness from cortical activity recording is a major challenge in neuroscience. The spontaneous fluctuations of brain activity through different patterns across time are monitored using resting-state functional MRI. The different dynamic functional configurations of the brain during resting-state are also called \u00e2\u0080\u009cbrain states\u00e2\u0080\u009d. The specific structure of each pattern, lifetime, and frequency have already been studied but the overall organization remains unclear. Recent studies showed that low-dimensional models are adequate to capture the correlation structure of neural activity during rest. One remaining question addressed here is the characterization of the latent feature space.\nWe trained a dense Variational Auto-Encoder (dVAE) to find a low two-dimensional representation that maps dynamic functional connectivity to probability distributions. A two-stage approach for latent feature space characterization is proposed to facilitate the results\u00e2\u0080\u0099 interpretation. In this approach, we first dissect the topography of the brain states and then perform a receptive field analysis to track the effect of each connection. The proposed framework instill interpretability and explainability of the latent space, unveiling biological insights on the states of consciousness. \nIt is applied on a non-human primate dataset acquired under different experimental conditions (awake state, anesthesia induced loss of consciousness).\n"}, {"title": "CheXRelNet: An Anatomy-Aware Model for Tracking Longitudinal Relationships between Chest X-Rays", "abstract": "Despite the progress in utilizing deep learning to automate chest radiograph interpretation and disease diagnosis tasks, change between sequential Chest X-rays (CXRs) has received limited attention. Monitoring the progression of pathologies that are visualized through chest imaging poses several challenges in anatomical motion estimation and image registration, \\ie spatially aligning the two images and modeling temporal dynamics in change detection.\nIn this work, we propose \\modelname, a neural model that can track longitudinal pathology change relations between two CXRs. \\modelname incorporates local and global visual features, utilizes inter-image and intra-image anatomical information, and learns dependencies between anatomical region attributes, to accurately predict disease change for a pair of CXRs. Experimental results on the Chest ImaGenome dataset show increased downstream performance compared to baselines. Code is available at https://github.com/PLAN-Lab/ChexRelNet.\n"}, {"title": "ChrSNet: Chromosome Straightening using Self-attention Guided Networks", "abstract": "Karyotyping is an important procedure to assess the possible existence of chromosomal abnormalities. However,  because of the non-rigid nature, chromosomes are usually heavily curved in microscopic images and such deformed shapes hinder the chromosome analysis for cytogeneticists. In this paper, we present a self-attention guided framework to erase the curvature of chromosomes. The proposed framework extracts spatial information and local textures to preserve banding patterns in a regression module. With complementary information from the bent chromosome, a refinement module is designed to further improve fine details. In addition, we propose two dedicated geometric constraints to maintain the length and restore the distortion of chromosomes. To train our framework, we create a synthetic dataset where curved chromosomes are generated from the real-world straight chromosomes by grid-deformation. Quantitative and qualitative experiments are conducted on synthetic and real-world data. Experimental results show that our proposed method can effectively straighten bent chromosomes while keeping banding details and length.\n"}, {"title": "CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction", "abstract": "Spiculations/lobulations, sharp/curved spikes on the surface of lung nodules, are good predictors of lung cancer malignancy and hence, are routinely assessed and reported by radiologists as part of the standardized Lung-RADS clinical scoring criteria. Given the 3D geometry of the nodule and 2D slice-by-slice assessment by radiologists, manual spiculation/lobulation annotation is a tedious task and thus no public datasets exist to date for probing the importance of these clinically-reported features in the SOTA malignancy prediction algorithms. As part of this paper, we release a large-scale Clinically-Interpretable Radiomics Dataset, CIRDataset, containing 956 radiologist QA/QC\u00e2\u0080\u0099ed spiculation/lobulation annotations on segmented lung nodules from two public datasets, LIDC-IDRI (N=883) and LUNGx (N=73). We also present an end-to-end deep learning model based on multi-class Voxel2Mesh extension to segment nodules (while preserving spikes), classify spikes (sharp/spiculation and curved/lobulation), and perform malignancy prediction. Previous methods have performed malignancy prediction for LIDC and LUNGx datasets but without robust attribution to any clinically reported/actionable features (due to known hyperparameter sensitivity issues with general attribution schemes). With the release of this comprehensively-annotated CIRDataset and end-to-end deep learning baseline, we hope that malignancy prediction methods can validate their explanations, benchmark against our baseline, and provide clinically-actionable insights. Dataset, code, pretrained models, and docker containers are available at \\url{https://github.com/nadeemlab/CIR}.\n"}, {"title": "Class Impression for Data-free Incremental Learning", "abstract": "Standard deep learning-based classification approaches require collecting all samples from all classes in advance and are trained offline. This paradigm may not be practical in real-world clinical applications, where new classes are incrementally introduced through the addition of new data. Class incremental learning is a strategy allowing learning from such data. However, a major challenge is catastrophic forgetting, i.e., performance degradation on previous classes when adapting a trained model to new data. Prior methodologies to alleviate this challenge save a portion of training data require perpetual storage of such data that may introduce privacy issues. Here, we propose a novel data-free class incremental learning framework that first synthesizes data from the model trained on previous classes to generate a class impression. Subsequently, it updates the model by combining the synthesized data with new class data. To do so, we incorporate a cosine normalized Cross-entropy loss to mitigate the adverse effects of the imbalance, a margin loss to increase separation among previous classes and new ones, and an intra-domain contrastive loss to generalize the model trained on the synthesized data to real data. We compare our proposed framework with state-of-the-art methods in class incremental learning, where we demonstrate improvement in accuracy for the classification of 11,062 echocardiography cine series of patients.  Code is available at https://github.com/sanaAyrml/Class-Impresion-for-Data-free-Incremental-Learning\n"}, {"title": "Classification-aided High-quality PET Image Synthesis via Bidirectional Contrastive GAN with Shared Information Maximization", "abstract": "Positron emission tomography (PET) is a pervasively adopted nuclear imaging technique, however, its inherent tracer radiation inevitably causes potential health hazards to patients. To obtain high-quality PET image while reducing radiation exposure, this paper proposes an algorithm for high-quality standard-dose PET (SPET) synthesis from low-dose PET (LPET) image. Specifically, considering that LPET images and SPET images come from the same subjects, we argue that there is abundant shared content and structural information between LPET and SPET domains, which are helpful for improving synthesis performance. To this end, we innovatively propose a bi-directional contrastive generative adversarial network (BiC-GAN), containing a master network and an auxiliary network. Both networks implement intra-domain reconstruction and inter-domain synthesis tasks, aiming to extract shared information from LPET and SPET domains, respectively. Meanwhile, the contrastive learning strategy is also introduced to two networks for enhancing feature representation capability and acquiring more domain-independent information. To maximize the shared information extracted from two domains, we further design a domain alignment module to constrain the consistency of the shared information extracted from the two domains. On the other hand, since synthesized PET images can be used to assist disease diagnosis, such as mild cognitive impairment (MCI) identification, the MCI classification task is incorporated into PET image synthesis to further improve clinical applicability of the synthesized PET image through direct feedback from the classification task. Evaluated on a Real Human Brain dataset, our proposed method is demonstrated to achieve state-of-the-art performance quantitatively and qualitatively.\n"}, {"title": "Clinical-realistic Annotation for Histopathology Images with Probabilistic Semi-supervision: A Worst-case Study", "abstract": "Acquiring pixel-level annotation has been a major challenge for machine learning methods in medical image analysis. Such difficulty mainly comes from two sources: localization requiring high expertise, and delineation requiring tedious and time-consuming work. Existing methods of easing the annotation effort mostly focus on the latter one, the extreme of which is replacing the delineation with a single label for all cases. We postulate that under a clinical-realistic setting, such methods alone may not always be effective in reducing the annotation requirements from conventional classification/detection algorithms, because the major difficulty can come from localization, which is often neglected but can be critical in medical domain, especially for histopathology images. In this work, we performed a worst-case scenario study to identify the information loss from missing detection. To tackle the challenge,  we 1) proposed a different annotation strategy to image data with different levels of disease severity, 2) combined semi- and self-supervised representation learning with probabilistic weakly supervision to make use of the proposed annotations, and 3) illustrated its effectiveness in recovering useful information under the same worst-case scenario. As a shift from previous convention, it can potentially save significant time for experts\u00e2\u0080\u0099 annotation for AI model development.\n"}, {"title": "CLTS-GAN: Color-Lighting-Texture-Specular Reflection Augmentation for Colonoscopy", "abstract": "Automated analysis of optical colonoscopy (OC) video frames (to assist endoscopists during OC) is challenging due to variations in color, lighting, texture, and specular reflections. Previous methods either remove some of these variations via preprocessing (making pipelines cumbersome) or add diverse training data with annotations (but expensive and time-consuming). We present CLTS-GAN, a new deep learning model that gives fine control over color, lighting, texture, and specular reflection synthesis for OC video frames. We show that adding these colonoscopy-specific augmentations to the training data can improve state-of-the-art polyp detection/segmentation methods as well as drive next generation of OC simulators for training medical students. The code and pre-trained models for CLTS-GAN are available on Computational Endoscopy Platform GitHub (\\url{https://github.com/nadeemlab/CEP}). \n"}, {"title": "Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration", "abstract": "Image registration is useful for quantifying morphological changes in longitudinal MR images from prostate cancer patients. This paper describes a development in improving the learning-based registration algorithms, for this challenging clinical application often with highly variable yet limited training data. First, we report that the latent space can be clustered into a much lower dimensional space than that commonly found as bottleneck features at the deep layer of a trained registration network. Based on this observation, we propose a hierarchical quantization method, discretizing the learned feature vectors using a jointly-trained dictionary with a constrained size, in order to improve the generalisation of the registration networks. Furthermore, a novel collaborative dictionary is independently optimised to incorporate additional prior information, such as the segmentation of the gland or other regions of interest, in the latent quantized space. Based on 216 real clinical images from 86 prostate cancer patients, we show the efficacy of both the designed components. Improved registration accuracy was obtained with statistical significance, in terms of both Dice on gland and target registration error on corresponding landmarks, the latter of which achieved 5.46 mm, an improvement of 28.7\\% from the baseline without quantization. Experimental results also show that the difference in performance was indeed minimised between training and testing data. \n"}, {"title": "Combining mixed-format labels for AI-based pathology detection pipeline in a large-scale knee MRI study", "abstract": "Labeling for pathology detection is a laborious task, performed by highly trained and expensive experts. Datasets often have mixed formats, including a mix of pathology positional labels and categorical labels. Successfully combining mixed-format data from multiple institutions for model training and evaluation is critical for model generalization. Herein, we describe a novel machine-learning method to augment a categorical dataset with positional information. This is inspired by the emerging data-centric AI paradigm, which focuses on systematically changing data to improve performance, rather than changing the model. In order to improve on a baseline of reducing the positional labels to categorical data, we propose a generalizable two-stage method that directs model attention to regions where pathologies are highly likely to occur, exploiting all the mixed-format data. The proposed approach was evaluated using four different knee MRI pathology detection tasks, including anterior cruciate ligament (ACL) integrity and injury age (5082 cases), and medial compartment cartilage (MCC) high-grade defects and subchondral edema detection (4251 cases). For these tasks, we achieved specificities and sensitivities between 90-94% and 78-93%, respectively, which were comparable to the inter-reader agreement results. On all tasks, we report an increase in AUC score, and an average of 8% specificity and 4% sensitivity improvement, as compared to the baseline approach. Combining a UNet network with a morphological peak-finding algorithm, our method also provides defect localization, with average accuracies between 4.3-5.1 mm. In addition, we demonstrate that our model generalizes well on a publicly available ACL tear dataset of 717 cases, without re-training, achieving 90% specificity and 100% sensitivity. The proposed method can be used to optimize image classification tasks in other medical or non-medical domains, which often have a mixture of categorical and positional labels. \n"}, {"title": "Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport", "abstract": "Connectomics is a popular approach for understanding the brain with neuroimaging data. \n Yet, a connectome generated from one atlas is different in size, topology, and scale compared to a connectome generated from  another atlas.\n These differences hinder interpreting, generalizing, and combining connectomes and downstream results from different atlases.\n Recently, it was proposed that a mapping between atlases can be estimated such that connectomes from one atlas (\\textit{i.e.}, source atlas) can be reconstructed into a connectome from a different atlas (\\textit{i.e.}, target atlas) without re-processing the data.\n This approach used optimal transport to estimate the mapping between one source atlas and one target atlas. \n Yet, \n restricting the optimal transport problem to only a single source atlases ignores additional information when multiple source atlases are available, which is likely.\nHere, we propose a novel optimal transport based solution to combine information from multiple source atlases to better estimate connectomes for the target atlas.\nReconstructed connectomes based on multiple source atlases are  more similar to their ``gold-standard\u00e2\u0080\u0099\u00e2\u0080\u0099 counterparts and better at predicting IQ than reconstructed connectomes based on a single source mapping. \nImportantly, these results hold for a wide-range of different atlases. \nOverall, our approach  promises to increase the generalization of connectome-based results across different atlases."}, {"title": "Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance", "abstract": "Although deep learning algorithms have been intensively developed for computer-aided tuberculosis diagnosis (CTD), they mainly depend on carefully annotated datasets, leading to much time and resource consumption. Weakly supervised learning (WSL), which leverages coarse-grained labels to accomplish fine-grained tasks, has the potential to solve this problem. In this paper, we first propose a new largescale tuberculosis (TB) chest X-ray dataset, namely tuberculosis chest Xray attribute dataset (TBX-Att), and then establish an attribute-assisted weakly supervised framework to classify and localize TB by leveraging the attribute information to overcome the insufficiency of supervision in WSL scenarios. Specifically, first, the TBX-Att dataset contains 2000 X-ray images with seven kinds of attributes for TB relational reasoning, which are annotated by experienced radiologists. It also includes the public TBX11K dataset with 11200 X-ray images to facilitate weakly supervised detection. Second, we exploit a multi-scale feature interaction model for TB area classification and detection with attribute relational reasoning. The proposed model is evaluated on the TBXAtt dataset and will serve as a solid baseline for future research. The code and data will be available at https://github.com/GangmingZhao/tb-attribute-weak-localization.\n"}, {"title": "Conditional Generative Data Augmentation for Clinical Audio Datasets", "abstract": "In this work, we propose a novel data augmentation method for clinical audio datasets based on a conditional Wasserstein Generative Adversarial Network with Gradient Penalty (cWGAN-GP), operating on log-mel spectrograms. To validate our method, we created a clinical audio dataset which was recorded in a real-world operating room during Total Hip Arthroplasty (THA) procedures and contains typical sounds which resemble the different phases of the intervention. We demonstrate the capability of the proposed method to generate realistic class-conditioned samples from the dataset distribution and show that training with the generated augmented samples outperforms classical audio augmentation methods in terms of classification performance. The performance was evaluated using a ResNet-18 classifier which shows a mean Macro F1-score improvement of 1.70% in a 5-fold cross validation experiment using the proposed augmentation method. Because clinical data is often expensive to acquire, the development of realistic and high-quality data augmentation methods is crucial to improve the robustness and generalization capabilities of learning-based algorithms which is especially important for safety-critical medical applications. Therefore, the proposed data augmentation method is an important step towards improving the data bottleneck for clinical audio-based machine learning systems.\n"}, {"title": "Conditional VAEs for confound removal and normative modelling of neurodegenerative diseases", "abstract": "Understanding pathological mechanisms for heterogeneous brain disorders is a difficult challenge. Normative modelling provides a statistical description of the `normal\u00e2\u0080\u0099 range that can be used at subject level to detect deviations, which relate to disease presence, disease severity or disease subtype. Here we trained a conditional Variational Autoencoder (cVAE) on structural MRI data from healthy controls to create a normative model conditioned on confounding variables such as age. The cVAE allows us to use deep learning to identify complex relationships that are independent of these confounds which might otherwise inflate pathological effects. We propose a latent deviation metric and use it to quantify deviations in individual subjects with neurological disorders and, in an independent Alzheimer\u00e2\u0080\u0099s disease dataset, subjects with varying degrees of pathological ageing. Our model is able to identify these disease cohorts as deviations from the normal brain in such a way that reflect disease severity. Code and trained models are publicly available at https://anonymous.4open.science/r/normativecVAE-395C.\n"}, {"title": "Consistency-based Semi-supervised Evidential Active Learning for Diagnostic Radiograph Classification", "abstract": "Deep learning approaches achieve state-of-the-art performance for classifying radiology images, but rely on large labelled datasets that require resource-intensive annotation by specialists. Both semi-supervised learning and active learning can be utilised to mitigate this annotation burden. However, there is limited work on combining the advantages of semi-supervised and active learning approaches for multi-label medical image classification. Here, we introduce a novel Consistency-based Semi-supervised Evidential Active Learning framework (CSEAL). Specifically, we leverage predictive uncertainty based on theories of evidence and subjective logic to develop an end-to-end integrated approach that combines consistency-based semi-supervised learning with uncertainty-based active learning. We apply our approach to enhance four leading consistency-based semi-supervised learning methods: Pseudo-labelling, Virtual Adversarial Training, Mean Teacher and NoTeacher. Extensive evaluations on multi-label Chest X-Ray classification tasks demonstrate that CSEAL achieves substantive performance improvements over two leading semi-supervised active learning baselines. Further, a class-wise breakdown of results shows that our approach can substantially improve accuracy on rarer abnormalities with fewer labelled samples.\n"}, {"title": "Consistency-preserving Visual Question Answering in Medical Imaging", "abstract": "Visual Question Answering (VQA) models take an image and a natural-language question as input and infer the answer to the question. Recently, VQA systems in medical imaging have gained popularity thanks to potential advantages such as patient engagement and second opinions for clinicians. While most research efforts have been focused on improving architectures and overcoming data-related limitations, answer consistency has been overlooked even though it plays a critical role in establishing trustworthy models. In this work, we propose a novel loss function and corresponding training procedure that allows the inclusion of relations between questions into the training process. Specifically, we consider the case where implications between perception and reasoning questions are known a-priori. To show the benefits of our approach, we evaluate it on the clinically relevant task of Diabetic Macular Edema (DME) staging from fundus imaging. Our experiments show that our method outperforms state-of-the-art baselines, not only by improving model consistency, but also in terms of overall model accuracy. Our code and data are available at https://github.com/sergiotasconmorales/consistency_vqa.\n"}, {"title": "Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading", "abstract": "This paper proposes a novel transformer-based model architecture to solve medical imaging problems involving analysis of vertebrae. It also considers two applications of such models: (a) detection of spinal metastases and the related conditions of vertebral fractures and cord compression (b) radiological grading of common degenerative changes in intervertebral disks. Our contributions are as follows:\n(i) We propose Spinal Context Transformer (SCT), a deep-learning architecture suited for the analysis of repeated anatomical structures in medical imaging such as vertebral bodies (VBs). Unlike previous methods, SCT considers all VBs as shown in all available image modalities together, making predictions for each based on the context from the rest of the spinal column.\n(ii) We apply the architecture to a novel but important task - detecting spinal metastases and related conditions of cord compression and vertebral fractures/collapse from multi-series spinal MR studies. This is done using annotations extracted from free-text radiological reports as opposed to bespoke annotation. However the model shows strong agreement with vertebral-level bespoke annotations from a radiologist on the test set.\n(iii) We also apply SCT to an existing problem - radiological grading of inter-vertebral discs (IVDs) in lumbar MR scans for common degenerative changes. We show that by considering the context of vertebral bodies in the image, SCT improves the accuracy for several gradings compared to previously published models.\n"}, {"title": "Context-aware Voxel-wise Contrastive Learning for Label Efficient Multi-organ Segmentation", "abstract": "Medical image segmentation is a prerequisite for many clinical applications including disease diagnosis, surgical planning and computer assisted interventions. Due to the challenges in obtaining expert-level accurate, densely annotated multi-organ dataset, the existing datasets for multi-organ segmentation either have small number of samples, or only have annotations of a few organs instead of all organs, which are termed as partially labeled data. There exist previous attempts to develop label efficient segmentation method to make use of these partially labeled dataset for improving the performance of multi-organ segmentation. However, most of these methods suffer from the limitation that they only use the labeled information in the dataset without taking advantage of the large amount of unlabeled data. To this end, we propose a context-aware voxel-wise contrastive learning method to take full advantage of both labeled and unlabeled data in partially labeled dataset for an improvement of multi-organ segmentation performance. Experimental Results demonstrated that our proposed method achieved superior performance than other state-of-the-art methods.\n"}, {"title": "ConTrans: Improving Transformer with Convolutional Attention for Medical Image Segmentation", "abstract": "Over the past few years, convolution neural networks (CNNs) and vision transformers (ViTs)  have been two dominant architectures in medical image segmentation. Although CNNs can efficiently capture local representations, they experience difficulty establishing long-distance dependencies. Comparably, ViTs achieve impressive success owing to their powerful global contexts modeling capabilities, but they may not generalize well on insufficient datasets due to the lack of inductive biases inherent to CNNs. To inherit the merits of these two different design paradigms while avoiding their respective limitations, we propose a concurrent structure termed ConTrans, which can couple detailed localization information with global contexts to the maximum extent. ConTrans consists of two parallel encoders, i.e., a Swin Transformer encoder and a CNN encoder. Specifically, the CNN encoder is progressively stacked by the novel Depthwise Attention Block (DAB), with the aim to provide the precise local features we need. Furthermore, a well-designed Spatial-Reduction-Cross-Attention (SRCA) module is embedded in the decoder to form a comprehensive fusion of these two distinct feature representations and eliminate the semantic divergence between them. This allows to obtain accurate semantic information and ensure the up-sampling features with semantic consistency in a hierarchical manner. Extensive experiments across four typical tasks show that ConTrans significantly outperforms state-of-the-art methods on ten famous benchmarks.\n"}, {"title": "ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration", "abstract": "Establishing voxelwise semantic correspondence across distinct imaging modalities is a foundational yet formidable computer vision task. Current multi-modality registration techniques maximize hand-crafted inter-domain similarity functions, are limited in modeling nonlinear intensity-relationships and deformations, and may require significant re-engineering or underperform on new tasks, datasets, and domain pairs. This work presents ContraReg, an unsupervised contrastive representation learning approach to multi-modality deformable registration. By projecting learned multi-scale local patch features onto a jointly learned inter-domain embedding space, ContraReg obtains representations useful for non-rigid multi-modality alignment. Experimentally, ContraReg achieves accurate and robust results with smooth and invertible deformations across a series of baselines and ablations on a neonatal T1-T2 brain MRI registration task with all methods validated over a wide range of deformation regularization strengths.\n"}, {"title": "Contrast-free Liver Tumor Detection using Ternary Knowledge Transferred Teacher-student Deep Reinforcement Learning", "abstract": "Contrast-free liver tumor detection technology has a significant impact on clinics due to its ability to eliminate contrast agents (CAs) administration in the current tumor diagnosis. In this paper, we proposed a novel ternary knowledge transferred teacher-student DRL (Ts-DRL) as a safe, speedy, and inexpensive contrast-free technology for liver tumor detection. Ts-DRL leverages a teacher network to learn tumor knowledge after CAs administration, and create a pipeline to transfer teacher\u00e2\u0080\u0099s knowledge to guide a student network learning of tumor without CAs, thereby realizing contrast-free liver tumor detection. Importantly, Ts-DRL possesses a new ternary knowledge set (actions, rewards, and features of driven actions), which for the first time, allows the teacher network to not only inform the student network what to do, but also teach the student network why to do. Moreover, Ts-DRL possesses a novel progressive hierarchy transferring strategy to progressively adjust the knowledge rationing between teachers and students during training to couple with knowledge smoothly and effectively transferring. Evaluation on 325 patients including different types of tumors from two MR scanners, Ts-DRL significantly improves performance (Dice by at least 7%) when comparing the five most recent state-of-the-art methods. The results proved that our Ts-DRL has greatly promoted the development and deployment of contrast-free liver tumor technology.\n"}, {"title": "Contrastive Functional Connectivity Graph Learning for Population-based fMRI Classification", "abstract": "Contrastive self-supervised learning has recently benefited fMRI classification with inductive biases. Its weak label reliance prevents overfitting on small medical datasets and tackles the high intraclass variances. Nonetheless, existing contrastive methods generate resemblant pairs only on pixel-level features of 3D medical images, while the functional connectivity that reveals critical cognitive information is under-explored. Additionally, existing methods predict labels on individual contrastive representation without recognizing neighbouring information in the patient group, whereas interpatient contrast can act as a similarity measure suitable for population-based classification. We hereby proposed contrastive functional connectivity graph learning for population-based fMRI classification. Representations on the functional connectivity graphs are \u00e2\u0080\u009crepelled\u00e2\u0080\u009d for heterogeneous patient pairs meanwhile homogeneous pairs \u00e2\u0080\u009cattract\u00e2\u0080\u009d each other. Then a dynamic population graph that strengthens the connections between similar patients is updated for classification. Experiments on a multi-site dataset ADHD200 validate the superiority of the proposed method on various metrics. We initially visualize the population relationships and exploit potential subtypes.\n"}, {"title": "Contrastive learning for echocardiographic view integration", "abstract": "In this work, we aimed to tackle the challenge of fusing information from multiple echocardiographic views, mimicking cardiologists making diagnoses with an integrative approach. For this purpose, we used the available information provided in the CAMUS dataset to experiment combining 2D complementary views to derive 3D information of left ventricular (LV) volume. We proposed intra-subject and inter-subject volume contrastive losses with varying margin to encode heterogeneous input views to a shared view-invariant volume-relevant feature space, where feature fusion can be facilitated. The results demonstrated that the proposed contrastive losses successfully improved the integration of complementary information from the input views, achieving significantly better volume predictive performance (MAE: 10.96 ml, RMSE: 14.75 ml, R2: 0.88) than that of the late-fusion baseline without contrastive losses (MAE: 13.17 ml, RMSE: 17.91 ml, R2: 0.83). Code available at: https://github.com/LishinC/VCN.\n"}, {"title": "Contrastive Masked Transformers for Forecasting Renal Transplant Function", "abstract": "Renal transplantation appears as the most effective solution for end-stage renal disease. However, it may lead to renal allograft rejection or dysfunction within 15%-27% of patients in the first 5 years post-transplantation. Resulting from a simple blood test, serum creatinine is the primary clinical indicator of kidney function by calculating the Glomerular Filtration Rate. These characteristics motivate the challenging task of predicting serum creatinine early post-transplantation while investigating and exploring its correlation with imaging data. In this paper, we propose a sequential architecture based on transformer encoders to predict the renal function 2-years post-transplantation. Our method uses features generated from Dynamic Contrast-Enhanced Magnetic Resonance Imaging from 4 follow-ups during the first year after the transplant surgery. To deal with missing data, a key mask tensor exploiting the dot product attention mechanism of the transformers is used. Moreover, different contrastive schemes based on cosine similarity distance are proposed to handle the limited amount of available data. Trained on 69 subjects, our best model achieves 96.3% F1 score and 98.9% ROC AUC in the prediction of serum creatinine threshold on a separated test set of 20 subjects. Thus, our experiments highlight the relevance of considering sequential imaging data for this task and therefore in the study of chronic dysfunction mechanisms in renal transplantation, setting the path for future research in this area. Our code is available at https://github.com/leomlck/renal_transplant_imaging.\n"}, {"title": "Contrastive Re-localization and History Distillation in Federated CMR Segmentation", "abstract": "Federated learning (FL) has shown value in multi-center multi-sequence cardiac magnetic resonance (CMR) segmentation, due to imbalanced CMR distributions and privacy preservation in clinical practice. However, the larger heterogeneity among multi-center multi-sequence CMR brings challenges to the FL framework: (1) Representation bias in the model fusion. The FL server model, which is generated by an average fusion of heterogeneous client models, is biased to representation close to the mean distribution and away from the long-distance distribution. Hence, the FL has poor representation ability. (2) Optimization stop in the model replacing. The heterogeneous server model replacing client model in FL directly causes the long-distance clients to utilize worse optimization to replace the original optimization. The client has to recover the optimization with the worse initialization, hence it lacks the continuous optimization ability. In this work, a cross-center cross-sequence medical image segmentation FL framework (FedCRLD) is proposed for the first time to facilitate multi-center multi-sequence CMR segmentation. (1) The contrastive re-localization module (CRL) of FedCRLD enables the correct representation from the heterogeneous model by embedding a novel contrastive difference metric of mutual information into a cross-attention localization transformer to transfer client-correlated knowledge from server model without bias. (2) The momentum distillation strategy (MD) of FedCRLD enables continuous optimization by conducting self-training on a dynamically updated client momentum bank to refine optimization by local correct optimization history. FedCRLD is validated on 420 CMR images 6 clients from 2 public datasets scanned by different hospitals, devices and contrast agents. Our FedCRLD achieves superior performance on multi-center multi-sequence CMR segmentation (average dice 85.96%). https://github.com/JerryQseu/FedCRLD.\n"}, {"title": "Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection", "abstract": "Current polyp detection methods from colonoscopy videos use exclusively normal (i.e., healthy) training images, which i) ignore the importance of temporal information in consecutive video frames, and ii) lack knowledge about the polyps. Consequently, they often have high detection errors, especially on challenging polyp cases (e.g., small, flat and partially visible polyps). In this work, we formulate polyp detection as a weakly-supervised anomaly detection task that uses video-level labelled training data to detect frame-level polyps. In particular, we propose a novel convolutional transformer-based multiple instance learning method designed to identify abnormal frames (i.e., frames with polyps) from anomalous videos (i.e., videos containing at least one frame with polyp). In our method, local and global temporal dependencies are seamlessly captured while we simultaneously optimise video and snippet-level anomaly scores. A contrastive snippet mining method is also proposed to enable an effective modelling of the challenging polyp cases. The resulting method achieves a detection accuracy that is substantially better than current state-of-the-art approaches on a new large-scale colonoscopy video dataset introduced in this work. Our code and dataset will be publicly available upon acceptance. \n"}, {"title": "Coronary R-CNN: Vessel-wise Method for Coronary Artery Lesion Detection and Analysis in Coronary CT Angiography", "abstract": "In recent decades, coronary artery disease (CAD) is the leading cause of death worldwide. Therefore, automatic diagnostic methods are strongly necessary with the progressively increasing number of CAD patients. However, it is difficult for physicians to recognize the lesion from Coronary CT Angiography (CCTA) scans as the coronary plaques have complicated appearance and patterns. Previous studies are mostly based on the single image patch around a lesion, which are often limited by the field of view of the local sample patch. To address this problem, in this paper we propose a novel vessel-wise object detection method. Different with previous approaches, we directly input the whole curved planar reformation (CPR) volume along the coronary artery centerline into our deep learning network, and then predict the plaque type and stenosis degree simultaneously. This enables the network to learn the dependencies between distant locations. In addition, two cascade modules are used to decompose the challenging problem into two simpler tasks and this also yields better interpretability. We evaluated our method on a dataset of 1031 CCTA images. The experimental results demonstrated the efficacy of our presented approach.\n"}, {"title": "CorticalFlow++: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability", "abstract": "The problem of Cortical Surface Reconstruction from magnetic resonance imaging has been traditionally addressed using lengthy pipelines of image processing techniques like FreeSurfer, CAT, or CIVET. These frameworks require very long runtimes deemed unfeasible for real-time applications and unpractical for large-scale studies. Recently, supervised deep learning approaches have been introduced to speed up this task cutting down the reconstruction time from hours to seconds. Using the state-of-the-art CorticalFlow model as a blueprint, this paper proposes three modifications to improve its accuracy and interoperability with existing surface analysis tools, while not sacrificing its fast inference time and low GPU memory consumption. First, we employ a more accurate ODE solver to reduce the diffeomorphic mapping approximation error. Second, we devise a routine to produce smoother template meshes avoiding mesh artifacts caused by sharp edges in CorticalFlow\u00e2\u0080\u0099s convex-hull based template. Last, we recast pial surface prediction as the deformation of the predicted white surface leading to a one-to-one mapping between white and pial surface vertices. This mapping is essential to many existing surface analysis tools for cortical morphometry. We name the resulting method CorticalFlow++. Using large-scale datasets, we demonstrate the proposed changes provide more geometric accuracy and surface regularity while keeping the reconstruction time and GPU memory requirements almost unchanged.\n"}, {"title": "CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation", "abstract": "Accurate uncertainty estimation is a critical need for the medical imaging community. A variety of methods have been proposed, all direct extensions of classification uncertainty estimations techniques. The independent pixel-wise uncertainty estimates, often based on the probabilistic interpretation of neural networks, do not take into account anatomical prior knowledge and consequently provide sub-optimal results to many segmentation tasks. For this reason, we propose CRISP a ContRastive Image Segmentation for uncertainty Prediction method. At its core, CRISP implements a contrastive method to learn a joint latent space which encodes a distribution of valid segmentations and their corresponding images. We use this joint latent space to compare predictions to thousands of latent vectors and provide anatomically consistent uncertainty maps. Comprehensive studies performed on four medical image databases involving different modalities and organs underlines the superiority of our method compared to state-of-the-art approaches.\n"}, {"title": "CS2: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention", "abstract": "The destitution of image data and corresponding expert annotations limit the training capacities of AI diagnostic models and potentially inhibit their performance. To address such a problem of data and label scarcity, generative models have been developed to augment the training datasets. Previously proposed generative models usually require manually adjusted annotations (e.g., segmentation masks) or need pre-labeling. However, studies have found that these pre-labeling based methods can induce hallucinating artifacts, which might mislead the downstream clinical tasks, while manual adjustment could be onerous and subjective. To avoid manual adjustment and pre-labeling, we propose a novel controllable and simultaneous synthesizer (dubbed CS2) in this study to generate both realistic images and corresponding annotations at the same time. Our CS2 model is trained and validated using high resolution CT (HRCT) data collected from COVID-19 patients to realize an efficient infections segmentation with minimal human intervention. Our contributions include 1) a conditional image synthesis network that receives both style information from reference CT images and structural information from unsupervised segmentation masks, and 2) a corresponding segmentation mask synthesis network to automatically segment these synthesized images simultaneously. Our experimental studies on HRCT scans collected from COVID-19 patients demonstrate that our CS2 model can lead to realistic synthesized datasets and promising segmentation results of COVID infections compared to the state-of-the-art nnUNet trained and fine-tuned in a fully supervised manner.\n"}, {"title": "Curvature-enhanced Implicit Function Network for High-quality Tooth Model Generation from CBCT Images", "abstract": "In digital dentistry, high-quality tooth models are essential for dental diagnosis and treatment. 3D CBCT images and intra-oral scanning models are widely used in dental clinics to obtain tooth models. However, CBCT image is volumetric data often with limited resolution (about 0.3-1.0mm spacing), while intra-oral scanning model is high-resolution tooth crown surface (about 0.03mm spacing) without root information. Hence, dentists usually scan and combine these two modalities of data to build high-quality tooth models, which is time-consuming and easily affected by various patient conditions or acquisition artifacts. To address this problem, we propose a learning-based framework to generate high-quality tooth models with both fine-grained tooth crown details and root information only from CBCT images. Specifically, we first introduce a tooth segmentation network to extract individual teeth from CBCT images. Then, we utilize an implicit function network to generate tooth models at arbitrary resolution in a continuous learning space. Moreover, to capture fine-grained crown details, we further explore a curvature enhancement module in our framework. Experimental results show that our proposed framework outperforms other state-of-the-art methods quantitatively and qualitatively, demonstrating the effectiveness of our method and its potential applicability in clinical practice.\n"}, {"title": "DA-Net: Dual Branch Transformer and Adaptive Strip Upsampling for Retinal Vessels Segmentation", "abstract": "Since the morphology of retinal vessels plays a pivotal role in clinical diagnosis of eye-related diseases and diabetic retinopathy, retinal vessels segmentation is an indispensable step for the screening and diagnosis of retinal diseases, yet it is still a challenging problem due to the complex structure of retinal vessels. Current retinal vessels segmentation approaches roughly fall into image-level and patches-level methods based on the input type, while each has its own strengths and weaknesses. To benefit from both of the input forms, we introduce a {Dual Branch Transformer Module}  (DBTM) that can simultaneously and fully enjoy the patches-level local information and the image-level global context. Besides, the retinal vessels are long-span, thin, and distributed in strips, making the square kernel of classic convolutional neural network false as it is only suitable for most natural objects with bulk shape. To better capture context information, we further design an Adaptive Strip Upsampling Block (ASUB) to adapt to the striped distribution of the retinal vessels. Based on the above innovations, we propose a retinal vessels segmentation Network with Dual Branch Transformer and Adaptive Strip Upsampling (DA-Net). Experiments validate that our {DA-Net} outperforms other state-of-the-art methods on both DRIVE and CHASE-DB1 datasets. \n"}, {"title": "D'ARTAGNAN: Counterfactual Video Generation", "abstract": "Causally-enabled machine learning frameworks could help clinicians to identify the best course of treatments by answering counterfactual questions. We explore this path for the case of echocardiograms by looking into the variation of the Left Ventricle Ejection Fraction, the most essential clinical metric gained from these examinations. We combine deep neural networks, twin causal networks and generative adversarial methods for the first time to build D\u00e2\u0080\u0099ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel causal generative model. We demonstrate the soundness of our approach on a synthetic dataset before applying it to cardiac ultrasound videos to answer the question: \u00e2\u0080\u009cWhat would this echocardiogram look like if the patient had a different ejection fraction?\u00e2\u0080\u009d. To do so, we generate new ultrasound videos, retaining the video style and anatomy of the original patient, while modifying the Ejection Fraction conditioned on a given input. We achieve an SSIM score of 0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models are available at: https://github.com/HReynaud/dartagnan.\n"}, {"title": "Data-Driven Deep Supervision for Skin Lesion Classification", "abstract": "Automatic classification of pigmented, non-pigmented, and depigmented non-melanocytic skin lesions have garnered lots of attention in recent years. However, imaging variations in skin texture, lesion shape, depigmentation contrast, lighting condition, etc. hinder robust feature extraction, affecting classification accuracy. In this paper, we propose a new deep neural network that exploits input data for robust feature extraction. Specifically, we analyze the convoutional network\u00e2\u0080\u0099s behavior (field-of-view) to find the location of deep supervision for improved feature extraction. To achieve this, first we perform activation mapping to generate an object mask, highlighting the input regions most critical for classification output generation. Then the network layer whose layer-wise effective receptive field matches the approximated object shape in the object mask is selected as our focus for deep supervision. Utilizing different types of convolutional feature extractors and classifiers on three melanoma detection datasets and two vitiligo detection datasets, we verify the effectiveness of our new method.\n"}, {"title": "Data-driven Multi-Modal Partial Medical Image Preregistration by Template Space Patch Mapping", "abstract": "Image registration is an essential part of Medical Image Analysis. Traditional local search methods (e.g., Mean Square Errors (MSE) and Normalized Mutual Information (NMI)) achieve accurate registration but require good initialization. However, finding a good initialization is difficult in partial image matching. Recent deep learning methods such as images-to-transformation directly solve the registration problem but need images of mostly same sizes and already roughly aligned. This work presents a learning-based method to provide good initialization for partial image registration. A light and efficient network learns the mapping from a small patch of an image to a position in the template space for each modality. After computing such mapping for a set of patches, we compute a rigid transformation matrix that maps the patches to the corresponding target positions. We tested our method to register a 3DRA image of a partial brain to a CT image of a whole brain. The result shows that MSE registration with our initialization significantly outperformed baselines including naive initialization and recent deep learning methods without template. Our dataset and source code will be published online.\n"}, {"title": "DDPNet: A novel dual-domain parallel network for low-dose CT reconstruction", "abstract": "The low-dose computed tomography (CT) scan is clinically significant to reduce the radiation risk for radiologists and patients, especially in repeative examination. However, it inherently introduce more noise due to the radiation exposure.\nNowadays, the existing LDCT reconstruction methods mainly focus on single domain of sinogram and image, or their cascade. But there still has limitations that the insufficient information in single domain, and the accumulation error in cascaded dual-domain. Though dual-domain can provide more information in reconstruction, how to effectively organize dual-domain and make complementary fusion still remain open challenges. Besides, the details inter-pixel in reconstructed LDCT is essential for structure maintenance.\nWe propose a Dual-domain parallel network (DDPNet) for high-quality reconstruction from widely accessible LDCT, which is the first powerful work making parallel optimization between sinogram and image domains to eliminate the accumulation error, and fusing dual-domain reconstructions for complementary. DDPNet is constituted by three special designs: 1) a dual-domain parallel architecture to make joint mutual optimization with interactive information flow; 2) a unified fusion block to complement multi-results and further refine final reconstruction with triple-cross attention; 3) a pair of coupled patch-discriminators to drive the reconstruction towards both realistic anatomic content and accurate inner-details with image-based and inter-pixel gradient-based adversarial constraints, respectively. The extensive experiment validated on public available Mayo dataset show that our DDPNet achieves promising PSNR up to 45.29 dB, SSIM up to 98.24%, and MAE down to 13.54 HU. in quantitative evaluations, as well as gains high-quality readable visualizations in qualitative assessments. All of these findings reveal our method a great clinical potential in CT imaging.\n"}, {"title": "Decoding Task Sub-type States with Group Deep Bidirectional Recurrent Neural Network", "abstract": "Decoding brain states under different task conditions from functional mag-netic resonance imaging (tfMRI) data has attracted more and more atten-tions in neuroimaging studies. Although various methods have been devel-oped, existing methods do not fully consider the temporal dependencies be-tween adjacent fMRI data points which limits the model performance. In this paper, we propose a novel group deep bidirectional recurrent neural net-work (Group-DBRNN) model for decoding task sub-type states from individ-ual fMRI volume data points. Specifically, we employed the bidirectional re-current neural network layer to characterize the temporal dependency feature from both directions effectively. We further developed a multi-task interac-tion layer (MTIL) to effectively capture the latent temporal dependencies of brain sub-type states under different tasks. Besides, we modified the training strategy to train the classification model in group data fashion for the indi-vidual task. The basic idea is that relational tfMRI data may provide external information for brain decoding. The proposed Group-DBRNN model has been tested on the task fMRI datasets of HCP 900 subject\u00e2\u0080\u0099s release, and the average classification accuracy of 24 sub-type brain states is as high as 91.34%. The average seven-task classification accuracy is 95.55% which is significantly higher than other state-of-the-art methods. Extensive experi-mental results demonstrated the superiority of the proposed Group-DBRNN model in automatically learning the discriminative representation features and effectively distinguishing brain sub-type states across different task fMRI datasets. \n"}, {"title": "Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation", "abstract": "Distributed learning has shown great potentials in medical image analysis. It allows to use multi-center training data with privacy protection. However, data distributions in local centers can vary from each other due to different imaging vendors, and annotation protocols. Such variation degrades the performance of learning-based methods. To mitigate the influence, two groups of methods have been proposed for different aims, i.e., the global methods and the personalized methods. The formers are aimed to improve the performance of a single global model for all test data from unseen centers (known as generic data); while the latters target multiple models, of which each for one center (denoted as local data). However, little has been researched to achieve both goals simultaneously. In this work, we propose a new framework of distributed learning that bridges the gap between two groups, and improves the performance for both generic and local data. Specifically, our method decouples the predictions for generic data and local data, via distribution-conditioned adaptation matrices. Results on multi-center left atrial (LA) MRI segmentation showed that our method demonstrated superior performance over existing methods on both generic and local data.\n"}, {"title": "Deep filter bank regression for super-resolution of anisotropic MR brain images", "abstract": "In 2D multi-slice magnetic resonance (MR) acquisition, the through-plane signals are typically of lower resolution than the in-plane signals. While contemporary super-resolution (SR) methods aim to recover the underlying high-resolution volume, the estimated high-frequency information is implicit via end-to-end data-driven training rather than being explicitly stated and sought. To address this, we reframe the SR problem statement in terms of perfect reconstruction filter banks, enabling us to identify and directly estimate the missing information. In this work, we propose a two-stage approach to approximate the completion of a perfect reconstruction filter bank corresponding to the anisotropic acquisition of a particular scan. In stage 1, we estimate the missing filters using gradient descent and in stage 2, we use deep networks to learn the mapping from coarse coefficients to detail coefficients. In addition, the proposed formulation does not rely on external training data, circumventing the need for domain shift correction. Under our approach, SR performance is improved particularly in \u00e2\u0080\u009cslice gap\u00e2\u0080\u0099\u00e2\u0080\u0099 scenarios, likely due to the constrained solution space imposed by the framework.\n"}, {"title": "Deep Geometric Supervision Improves Spatial Generalization in Orthopedic Surgery Planning", "abstract": "Careful surgical planning facilitates the precise and safe placement of implants and grafts in reconstructive orthopedics. Current attempts to (semi-)automatic planning separate the extraction of relevant anatomical structures on X-ray images and perform the actual positioning step using geometric post-processing. Such separation requires optimization of a proxy objective different from the actual planning target, limiting generalization to complex image impressions and the positioning accuracy that can be achieved. We address this problem by translating the geometric steps to a continuously differentiable function, enabling end-to-end gradient flow. Combining this companion objective function with the original proxy formulation improves target positioning directly while preserving the geometric relation of the underlying anatomical structures. We name this concept Deep Geometric Supervision. The developed method is evaluated for graft fixation site identification in medial patellofemoral ligament (MPFL) reconstruction surgery on (1) 221 diagnostic and (2) 89 intra-operative knee radiographs. Using the companion objective reduces the median Euclidean Distance error for MPFL insertion site localization from (1) 2.29 mm to 1.58 mm and (2) 8.70 px to 3.44 px, respectively. Furthermore, we empirically show that our method improves spatial generalization for strongly truncated anatomy.\n"}, {"title": "Deep is a Luxury We Don't Have", "abstract": "Medical images come in high resolutions. A high resolution is vital for finding malignant tissues at an early stage. Yet, this resolution presents a challenge in terms of modeling long range dependencies. Shallow transformers eliminate this problem, but they suffer from quadratic complexity. In this paper, we tackle this complexity by leveraging a linear self-attention approximation. Through this approximation, we propose an efficient vision model called HCT that stands for High resolution Convolutional Transformer. HCT brings  transformers\u00e2\u0080\u0099 merits  to high resolution images at a significantly lower cost. We evaluate HCT using a high resolution mammography dataset. HCT is significantly superior to its CNN counterpart. Furthermore, we demonstrate HCT\u00e2\u0080\u0099s fitness for medical images by evaluating its effective receptive field. Code available at https://bit.ly/3ykBhhf\n"}, {"title": "Deep Laparoscopic Stereo Matching with Transformers", "abstract": "The self-attention mechanism,  successfully employed with the transformer structure is shown promise in many computer vision problems including image recognition, and object detection. Despite the surge,  the use of the transformer for the problem of stereo matching remains relatively unexplored. In this paper, we comprehensively investigate the use of the transformer for the problem of stereo matching, especially for laparoscopic videos, and propose a new hybrid deep stereo matching framework  (HybridStereoNet)  that combines the best of the CNN and the transformer in a unified design. To be specific, we investigate several ways to introduce transformers to volumetric stereo matching pipelines by analyzing the loss landscape of the designs and in-domain/cross-domain accuracy. Our analysis suggests employing transformers for feature representation learning, while using CNNs for cost aggregation. Our extensive experiments on Sceneflow, SCARED2019 and dVPNdatasets demonstrate the superior performance of our HybridStereoNet.\n"}, {"title": "Deep Learning based Modality-Independent Intracranial Aneurysm Detection", "abstract": "Early detection of intracranial aneurysms (IAs) allows early treatment and therefore a better outcome for the patient. \nDeep learning-based models trained and executed on angiographic scans can highlight possible IA locations, which could increase visual detection sensitivity and substantially reduce the assessment time. Thus far methods were mostly trained and tested on single modality, while their reported performances within and across modalities seems insufficient for clinical application. This paper presents a modality-independent method for detection of IAs on MRAs and CTAs. First, the vascular surface meshes were automatically extracted from the CTA and MRA angiograms, using nnUnet approach. For IA detection purpose, the extracted surfaces were randomly parcellated into local patches and then a translation, rotation and scale invariant classifier based on deep neural network (DNN) was trained. Test stage proceeded by mimicking the surface extraction and parcellation, and the results across parcels were aggregated into IA detection heatmap of the entire vascular surface. Using 200 MRAs  and 300 CTAs we trained and tested three models, two in cross modality setting (training on MRAs/CTAs and testing on CTAs/MRAs, respectively), while the third was a mixed-modality model, trained and tested on both modalities. The best model resulted in a 96% sensitivity at 0.81 false positive detections per image. Experimental results show that proposed approach not only significantly improved detection sensitivity and specificity compared to state-of-the-art methods, but is also modality agnostic, may aggregate information across modalities and thus seems better suited for clinical application. \n"}, {"title": "Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement", "abstract": "Simulating facial appearance change following bony movement is a critical step in orthognathic surgical planning for patients with jaw deformities. Conventional biomechanics-based methods such as the finite-element method (FEM) are labor intensive and computationally inefficient. Deep learning-based approaches can be promising alternatives due to their high computational efficiency and strong modeling capability. However, the existing deep learning-based method ignores the physical correspondence between facial soft tissue and bony segments and thus is significantly less accurate compared to FEM. In this work, we propose an Attentive Correspondence assisted Movement Transformation network (ACMT-Net) to estimate the facial appearance by transforming the bony movement to facial soft tissue through a point-to-point attentive correspondence matrix. Experimental results on patients with jaw deformity show that our proposed method can achieve comparable facial change prediction accuracy compared with the state-of-the-art FEM-based approach with significantly improved computational efficiency.\n"}, {"title": "Deep learning-based Head and Neck Radiotherapy Planning Dose Prediction via Beam-wise Dose Decomposition", "abstract": "Accurate dose map prediction is key to external radiotherapy. Previous methods have achieved promising results; however, most of these methods learn the dose map as a black box without considering the beam-shaped radiation for treatment delivery in clinical practice. The accuracy is usually limited, especially on beam paths. To address this problem, this paper describes a novel \u00e2\u0080\u009cdisassembling-then-assembling\u00e2\u0080\u009d strategy to consider the dose prediction task from the nature of radiotherapy. Specifically, a global-to-beam network is designed to first predict dose values of the whole image space and then utilize the proposed innovative beam masks to decompose the dose map into multiple beam-based sub-fractions in a beam-wise manner. This can disassemble the difficult task to a few easy-to-learn tasks.  Furthermore, to better capture the dose distribution in region-of-interest (ROI), we introduce two novel value-based and criteria-based dose volume histogram (DVH) losses to supervise the framework. Experimental results on the public OpenKBP challenge dataset show that our method outperforms the state-of-the-art methods, especially on beam paths, creating a trustable and interpretable AI solution for radiotherapy treatment planning."}, {"title": "Deep Motion Network for Freehand 3D Ultrasound Reconstruction", "abstract": "Freehand 3D ultrasound (US) has important clinical value due to its low cost and unrestricted field of view. Recently deep learning algorithms have removed its dependence on bulky and expensive external positioning devices. However, improving reconstruction accuracy is still hampered by difficult elevational displacement estimation and large cumulative drift. In this context, we propose a novel deep motion network (MoNet) that integrates images and a lightweight sensor known as the inertial measurement unit (IMU) from a velocity perspective to alleviate the obstacles mentioned above. Our contribution is two-fold. First, we introduce IMU acceleration for the first time to estimate elevational displacements outside the plane. We propose a temporal and multi-branch structure to mine the valuable information of low signal-to-noise ratio (SNR) acceleration. Second, we propose a multi-modal online self-supervised strategy that leverages IMU information as weak labels for adaptive optimization to reduce drift errors and further ameliorate the impacts of acceleration noise. Experiments show that our proposed method achieves the superior reconstruction performance, exceeding state-of-the-art methods across the board.\n"}, {"title": "Deep Multimodal Guidance for Medical Image Classification", "abstract": "Medical imaging is a cornerstone of therapy and diagnosis in modern medicine. However, the choice of imaging modality for a particular theranostic task typically involves trade-offs between the feasibility of using a particular modality (e.g., short wait times, low cost, fast acquisition, reduced radiation/invasiveness) and the expected performance on a clinical task (e.g., diagnostic accuracy, efficacy of treatment planning and guidance). In this work, we aim to apply the knowledge learnt from the less feasible but better-performing (superior) modality to guide the utilization of the more-feasible yet under-performing (inferior) modality and steer it towards improved performance. We focus on the application of deep learning for image-based diagnosis. We develop a light-weight guidance model that leverages the latent representation learned from the superior modality, when training a model that consumes only the inferior modality. We examine the advantages of our method in the context of two clinical applications: multi-task skin lesion classification from clinical and dermoscopic images and brain tumor classification from multi-sequence magnetic resonance imaging (MRI) and histopathology images. For both these scenarios we show a boost in diagnostic performance of the inferior modality without requiring the superior modality. Furthermore, in the case of brain tumor classification, our method outperforms the model trained on the superior modality while being comparable to the model that uses both modalities during inference.\n"}, {"title": "Deep Regression with Spatial-Frequency Feature Coupling and Image Synthesis for Robot-Assisted Endomicroscopy", "abstract": "Probe-based confocal laser endomicroscopy (pCLE) allows in-situ visualisation of cellular morphology for intraoperative tissue characterisation. Robotic manipulation of the pCLE probe can maintain the probe-tissue contact within micrometre working range to achieve the precision and stability required to capture good quality microscopic information. In this paper, we propose the first approach to automatically regress the distance between a pCLE probe and the tissue surface during robotic tissue scanning. The Spatial-Frequency Feature Coupling network (SFFC-Net) was designed to regress probe-tissue distance by extracting an enhanced data representation based on the fusion of spatial and frequency domain features. Image-level supervision is used in a novel fashion in regression to enable the network to effectively learn the relationship between the sharpness of the pCLE image and its distance from the tissue surface. Consequently, a novel Feedback Training (FT) module has been designed to synthesise unseen images to incorporate feedback into the training process. The first pCLE regression dataset (PRD) was generated which includes ex-vivo images with corresponding probe-tissue distance. Our performance evaluation verifies that the proposed network outperforms other state-of-the-art (SOTA) regression networks.\n"}, {"title": "Deep Reinforcement Learning for Detection of Inner Ear Abnormal Anatomy in Computed Tomography", "abstract": "Detection of abnormalities within the inner ear is a challenging task that, if automated, could provide support for the diagnosis and clinical management of various otological disorders.\nInner ear malformations are rare and present great anatomical variation, which challenges the design of deep learning frameworks to automate their detection. \nWe propose a framework for inner ear abnormality detection, based on a deep reinforcement learning model for landmark detection trained in normative data only. \nWe derive two abnormality measurements: the first is based on the variability of the predicted configuration of the landmarks in a subspace formed by the point distribution model of the normative landmarks using Procrustes shape alignment and Principal Component Analysis projection. The second measurement is based on the distribution of the predicted Q-values of the model for the last ten states before the landmarks are located.\nWe demonstrate an outstanding performance for this implementation on both an artificial (0.96 AUC) and a real clinical CT dataset of various malformations of the inner ear (0.87 AUC). Our approach could potentially be used to solve other complex anomaly detection problems.\n"}, {"title": "Deep Reinforcement Learning for Small Bowel Path Tracking using Different Types of Annotations", "abstract": "Small bowel path tracking is a challenging problem considering its many folds and contact along its course. For the same reason, it is very costly to achieve the ground-truth (GT) path of the small bowel in 3D. In this work, we propose to train a deep reinforcement learning tracker using datasets with different types of annotations. Specifically, we utilize CT scans that have only GT small bowel segmentation as well as ones with the GT path. It is enabled by designing a unique environment that is compatible for both, including a reward definable even without the GT path. The performed experiments proved the validity of the proposed method. The proposed method holds a high degree of usability in this problem by being able to utilize the scans with weak annotations, and thus by possibly reducing the required annotation cost.\n"}, {"title": "Deep treatment response assessment and prediction of colorectal cancer liver metastases", "abstract": "Evaluating treatment response is essential in patients who develop colorectal liver metastases to decide the necessity for second-line treatment or the admissibility for surgery. Currently, RECIST1.1 is the most widely used criteria in this context. However, it involves time-consuming, precise manual delineation and size measurement of main liver metastases from Computed Tomography (CT) images. Moreover, an early prediction of the treatment response given a specific chemotherapy regimen and the initial CT scan would be of tremendous use to clinicians. To overcome these challenges, this paper proposes a deep learning-based treatment response assessment pipeline and its extension for prediction purposes. Based on a newly designed 3D Siamese classification network, our method assigns a response group to patients given CT scans from two consecutive follow-ups during the treatment period. Further, we extended the network to predict the treatment response given only the image acquired at first time point. The pipelines are trained on the PRODIGE20 dataset collected from a phase-II multi-center clinical trial in colorectal cancer with liver metastases and exploit an in-house dataset to integrate metastases delineations derived from a U-Net inspired network as additional information. Our approach achieves overall accuracies of 94.94% and 86.86% for treatment response assessment and early prediction respectively, suggesting that both treatment response assessment and prediction issues can be effectively solved with deep learning.\n"}, {"title": "DeepCRC: Colorectum and Colorectal Cancer Segmentation in CT Scans via Deep Colorectal Coordinate Transform", "abstract": "We propose DeepCRC, a topology-aware deep learning-based approach for automated colorectum and colorectal cancer (CRC) segmentation in routine abdominal CT scans. Compared with MRI and CT Colonography, regular CT has a broader application but is more challenging. Standard segmentation algorithms often induce discontinued colon prediction, leading to inaccurate or completely failed CRC segmentation. To tackle this issue, we establish a new 1D colorectal coordinate system that encodes the position information along the colorectal elongated topology. In addition to the regular segmentation task, we propose an auxiliary regression task that directly predicts the colorectal coordinate for each voxel. This task integrates the global topological information into the network embedding and thus improves the continuity of the colorectum and the accuracy of the tumor segmentation. To enhance the model\u00e2\u0080\u0099s architectural ability of modeling global context, we add self-attention layers to the model backbone, and found it complementary to the proposed algorithm. We validate our approach on a cross-validation of 107 cases and outperform nnUNet by an absolute margin of 1.3% in colorectum segmentation and 8.3% in CRC segmentation. Notably, we achieve comparable tumor segmentation performance with the human inter-observer (DSC: 0.646 vs. 0.639), indicating that our method has similar reproducibility as a human observer.\n"}, {"title": "Deep-learning Based T1 and T2 Quantification from Undersampled Magnetic Resonance Fingerprinting Data to Track Tracer Kinetics in Small Laboratory Animals", "abstract": "In human MRI studies, magnetic resonance fingerprinting (MRF) allows simultaneous T1 and T2 mapping in 10 seconds using 48-fold undersampled data. However, when \u00e2\u0080\u009creverse translated\u00e2\u0080\u009d to preclinical research involving small laboratory animals, the undersampling capacity of the MRF method decreases to 8 fold because of the low SNR associated with high spatial resolution.  In this study, we aim to develop a deep-learning based method to reliably quantify T1 and T2 in the mouse brain from highly undersampled MRF data, and to demonstrate its efficacy in tracking T1 and T2 variations induced by MR tracers. The proposed method employs U-Net as the backbone for spatially constrained T1 and T2 mapping. Several strategies to improve the robustness of mapping results are evaluated, including feature extraction with sliding window averaging, implementing physics-guided training objectives, and implementing data-consistency constraint to iteratively refine the inferred maps by a cascade of U-Nets. The quantification network is trained using mouse-brain MRF datasets acquired before and after Manganese (Mn2+) enhancement. Experimental results show that robust T1 and T2 mapping can be achieved from MRF data acquired in 30 s (4-fold further acceleration), by using a simple combination of sliding window averaging for feature extraction and U-Net for parametric quantification.  Meanwhile, the T1 variations induced by Mn2+ in mouse brain are faithfully detected. Code is available at https://github.com/guyn-idealab/Mouse-MRF-DL/.\n"}, {"title": "DeepMIF: Deep learning based cell profiling for multispectral immunofluorescence images with graphical user interface", "abstract": "Multispectral immunofluorescence (M-IF) analysis is used to investigate the cellular landscape of tissue sections and spatial interaction of cells. However, complex makeup of markers in the images hinders the accurate quantification of cell phenotypes. We developed DeepMIF, a new deep learning (DL) based tool with a graphical user interface (GUI) to detect and quantify cell phenotypes on M-IF images, and visualize whole slide image (WSI) and cell phenotypes. To identify cell phenotypes, we detected cells on the deconvoluted images followed by co-expression analysis to classify cells expressing single or multiple markers. We trained, tested and validated our model on > 50k expert single-cell annotations from multiple immune panels on 15 samples of follicular lymphoma patients. Our algorithm obtained a cell classification accuracy and area under the curve (AUC) \u00e2\u0089\u00a5 0.98 on an independent validation panel. The cell phenotype identification took on average 27.5 minutes per WSI, and rendering of the WSI took on average 0.07 minutes. DeepMIF is optimized to run on local computers or high-performance clusters independent of the host platform. These suggest that the DeepMIF is an accurate and efficient tool for the analysis and visualization of M-IF images, leading to the identification of novel prognostic cell phenotypes in tumours.\n"}, {"title": "DeepPyramid: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos", "abstract": "Semantic segmentation in cataract surgery has a wide range of applications contributing to surgical outcome enhancement and clinical risk reduction. However, the varying issues in segmenting the different relevant structures in these surgeries make the designation of a unique network quite challenging. This paper proposes a semantic segmentation network, termed DeepPyramid, that can deal with these challenges using three novelties: (1) a Pyramid View Fusion module which provides a varying-angle global view of the surrounding region centering at each pixel position in the input convolutional feature map; (2) a Deformable Pyramid Reception module which enables a wide deformable receptive field that can adapt to geometric transformations in the object of interest; and (3) a dedicated Pyramid Loss that adaptively supervises multi-scale semantic feature maps. Combined, we show that these modules can effectively boost semantic segmentation performance, especially in the case of transparency, deformability, scalability, and blunt edges in objects. We demonstrate that our approach performs at a state-of-the-art level and outperforms a number of existing methods with a large margin (3.66% overall improvement in intersection over union compared to the best rival approach).\n"}, {"title": "DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction via A Structure-Specific Generative Method", "abstract": "Joint 2D cardiac segmentation and 3D volume reconstruction are fundamental to building statistical cardiac anatomy models and understanding functional mechanisms from motion patterns. However, due to the low through-plane resolution of cine MR and high inter-subject variance, accurately segmenting cardiac images and reconstructing the 3D volume are challenging. In this study, we propose an end-to-end latent-space-based framework, DeepRecon, that generates multiple clinically essential outcomes, including accurate image segmentation, synthetic high-resolution 3D image, and 3D reconstructed volume. Our method identifies the optimal latent representation of the cine image that contains accurate semantic information for cardiac structures. In particular, our model jointly generates synthetic images with accurate semantic information and segmentation of the cardiac structures using the optimal latent representation. We further explore downstream applications of 3D shape reconstruction and 4D motion pattern adaptation by the different latent-space manipulation strategies. The simultaneously generated high-resolution images present a high interpretable value to assess the cardiac shape and motion. Experimental results demonstrate the effectiveness of our approach on multiple fronts including 2D segmentation, 3D reconstruction, downstream 4D motion pattern adaption performance.\n"}, {"title": "Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration", "abstract": "Recently, deep-learning-based approaches have been widely studied for deformable image registration task. However, most efforts directly map the composite image representation to spatial transformation through the convolutional neural network, ignoring its limited ability to capture spatial correspondence. On the other hand, Transformer can better characterize the spatial relationship with attention mechanism, its long-range dependency may be harmful to the registration task, where voxels with too large distances are unlikely to be corresponding pairs. In this study, we propose a novel Deformer module along with a multi-scale framework for the deformable image registration task. The Deformer module is designed to facilitate the mapping from image representation to spatial transformation by formulating the displacement vector prediction as the weighted summation of several bases. With the multi-scale framework to predict the displacement fields in a coarse-to-fine manner, superior performance can be achieved compared with traditional and learning-based approaches. Comprehensive experiments on two public datasets are conducted to demonstrate the effectiveness of the proposed Deformer module as well as the multi-scale framework.\n"}, {"title": "Degradation-invariant Enhancement of Fundus Images via Pyramid Constraint Network", "abstract": "As an economical and efficient fundus imaging modality, retinal fundus images have been widely adopted in clinical fundus examination. Unfortunately, fundus images often suffer from quality degradation caused by imaging interferences, leading to misdiagnosis. Despite impressive enhancement performances that state-of-the-art methods have achieved, challenges remain in clinical scenarios. For boosting the clinical deployment of fundus image enhancement, this paper proposes the pyramid constraint to develop a degradation-invariant enhancement network (PCE-Net), which mitigates the demand for clinical data and stably enhances unknown data.  Firstly, high-quality images are randomly degraded to form sequences of low-quality ones sharing the same content (SeqLCs). Then individual low-quality images are decomposed to Laplacian pyramid features (LPF) as the multi-level input for the enhancement. Subsequently, a feature pyramid constraint (FPC) for the sequence is introduced to enforce the PCE-Net to learn a degradation-invariant model. Extensive experiments have been conducted under the evaluation metrics of enhancement and segmentation. The effectiveness of the PCE-Net was demonstrated in comparison with state-of-the-art methods and the ablation study. The source code of this study is publicly available at https://github.com/HeverLaw/PCENet-Image-Enhancement.\n"}, {"title": "Delving into Local Features for Open-Set Domain Adaptation in Fundus Image Analysis", "abstract": "Unsupervised domain adaptation (UDA) has received significant attention in medical image analysis when labels are only available for the source domain data but not for the target domain. Previous UDA methods mainly focused on the closed-set scenario, assuming that only the domain distribution shifts across domains while the label space is the same. However, in the practice of medical imaging, the disease categories of training data in source domain are usually limited, and the open-world target domain data may have many unknown} classes private to the source domain. Thus, open-set domain adaptation (OSDA) has great potential in this area. In this paper, we explore the OSDA problem by delving into local features for fundus disease recognition. We propose a collaborative regional clustering and alignment method to identify the common local feature patterns which are category-agnostic. Then, a cluster-aware contrastive adaptation loss is introduced to adapt the distributions based on the common local features. We also construct the first fundus image benchmark for OSDA to evaluate our methods and carry out extensive experiments for comparison. It shows that our model achieves consistent improvements over the state-of-the-art methods.\n"}, {"title": "Denoising for Relaxing: Unsupervised Domain Adaptive Fundus Image Segmentation without Source Data", "abstract": "Recently, unsupervised domain adaptation (UDA) has been actively explored for multi-site fundus image segmentation with domain discrepancy. Despite relaxing the requirement of target labels, typical UDA still requires the labeled source data to achieve distribution alignment during adaptation. Unfortunately, due to privacy concerns, the vendor side often cannot provide the source data to the targeted client side in clinical practice, making the adaptation more challenging. To address this, in this work, we present a novel uncertainty-rectified denoising-for-relaxing (U-D4R) framework, aiming at completely relaxing the source data and effectively adapting the pretrained source model to the target domain. Considering the unreliable source model predictions on the target domain, we first present an adaptive class-dependent threshold strategy as the coarse denoising process to generate the pseudo labels. Then, the uncertainty-rectified label soft correction is introduced for fine denoising by taking advantage of estimating the joint distribution matrix between the observed and latent labels. Extensive experiments on cross-domain fundus image segmentation showed that our approach significantly outperforms the state-of-the-art source-free methods and encouragingly achieves comparable or even better performances over the leading source-dependent methods.\n"}, {"title": "Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence", "abstract": "Small lesions in magnetic resonance imaging (MRI) images are crucial for clinical diagnosis of many kinds of diseases. However, the MRI quality can be easily degraded by various noise, which can greatly affect the accuracy of diagnosis of small lesion. Although some methods for denoising MR images have been proposed, task-specific denoising methods for improving the diagnosis confidence of small lesions are lacking. In this work, we propose a voxel-wise hybrid residual MLP-CNN model to denoise three-dimensional (3D) MR images with small lesions. We combine basic deep learning architecture, MLP and CNN, to obtain an appropriate inherent bias for the image denoising and integrate each output layers in MLP and CNN by adding residual connections to leverage long-range information. We evaluate the proposed method on 720 T2-FLAIR brain images with small lesions at different noise levels. The results show the superiority of our method in both quantitative and visual evaluations on testing dataset compared to state-of-the-art methods. Moreover, two experienced radiologists agreed that at moderate and high noise levels, our method outperforms other methods in terms of recovery of small lesions and overall image denoising quality. The implementation of our method is available at https://github.com/laowangbobo/Residual_MLP_CNN_Mixer.\n"}, {"title": "DentalPointNet: Landmark Localization on High-Resolution 3D Digital Dental Models", "abstract": "Dental landmark localization is an essential step for analyzing dental models in orthodontic treatment planning and orthognathic surgery. Typically, more than 60 landmarks need to be manually digitized on a 3D dental surface model. However, most existing landmark localization methods are unable to perform reliably especially for partially edentulous patients with missing landmarks.  In this work, we propose a deep learning framework, DentalPointNet, to automatically locate 68 landmarks on high-resolution dental surface models. Landmark area proposals are first predicted by a curvature-constrained region proposal network. Each proposal is then refined for landmark localization using a bounding box refinement network. Evaluation using 77 real-patient high-resolution dental surface models indicates that our approach achieves an average localization error of 0.24 mm, a false positive rate of 1 % and a false negative rate of 2 % on subjects both with or without partial edentulous, significantly outperforming relevant start-of-the-art methods.\n"}, {"title": "DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation", "abstract": "Self-supervised learning (SSL), enabling advanced performance with few annotations, has demonstrated a proven successful in medical image segmentation. Usually, SSL relies on measuring the similarity of features obtained at the deepest layer to attract the features of positive pairs or repulse the features of negative pairs, and then may suffer from the weak supervision at shallow layers. To address this issue, we reformulate SSL in a Deep Self-Distillation (DeSD) manner to improve the representation quality of both shallow and deep layers. Specifically, the DeSD model is composed of an online student network and a momentum teacher network, both being stacked by multiple sub-encoders. The features produced by each sub-encoder in the student network are trained to match the features produced by the teacher network. Such a deep self-distillation supervision is able to improve the representation quality of all sub-encoders, including both shallow ones and deep ones. We pre-train the DeSD model on a large-scale unlabeled dataset and evaluate it on seven downstream segmentation tasks. Our results indicate that the proposed DeSD model achieves superior pre-training performance over existing SSL methods, setting the new state of the art. The code is available at https://github.com/yeerwen/DeSD\n"}, {"title": "DeStripe: A Self2Self Spatio-Spectral Graph Neural Network with Unfolded Hessian for Stripe Artifact Removal in Light-sheet Microscopy", "abstract": "Light-sheet fluorescence microscopy (LSFM) is a cuttingedge volumetric imaging technique that allows for three-dimensional imaging of mesoscopic samples with decoupled illumination and detection paths. Although the selective excitation scheme of such a microscope provides intrinsic optical sectioning that minimizes out-of-focus fluorescence background and sample photodamage, it is prone to light absorption and scattering effects, which results in uneven illumination and striping artifacts in the images adversely. To tackle this issue, in this paper, we propose a blind stripe artifact removal algorithm in LSFM, called DeStripe, which combines a self-supervised spatio-spectral graph neural network with unfolded Hessian prior. Specifically, inspired by the desirable properties of Fourier transform in condensing striping information into isolated values in the frequency domain, DeStripe firstly localizes the potentially corrupted Fourier coefficients by exploiting the structural difference between unidirectional stripe artifacts and more isotropic foreground images. Affected Fourier coefficients can then be fed into a graph neural network for recovery, with a Hessian regularization unrolled to further ensure structures in the standard image space are well preserved. Since in realistic, stripe-free LSFM barely exits with a standard image acquisition protocol, DeStripe is equipped with a Self2Self denoising loss term, enabling artifact elimination without access to stripe free ground truth images. Competitive experimental results demonstrate the efficacy of DeStripe in recovering corrupted biomarkers in LSFMs with both synthetic and real stripe artifacts.\n"}, {"title": "Detecting Aortic Valve Pathology from the 3-Chamber Cine Cardiac MRI View", "abstract": "Cardiac magnetic resonance (CMR) is the gold standard for quantification of cardiac volumes, function, and blood flow. Tailored MR pulse sequences define the contrast mechanisms, acquisition geometry and timing which can be applied during CMR to achieve unique tissue characterisation. It is impractical for each patient to have every possible acquisition option. We target the aortic valve in the three-chamber (3-CH) cine CMR view. Two major types of anomalies are possible in the aortic valve. Stenosis: the narrowing of the valve which prevents an adequate outflow of blood, and insufficiency (regurgitation): the inability to stop the back-flow of blood into the left ventricle. We develop and evaluate a deep learning system to accurately classify aortic valve abnormalities to enable further directed imaging for patients who require it. Inspired by low level image processing tasks, we propose a multi-level network that generates heat maps to locate the aortic valve leaflets\u00e2\u0080\u0099 hinge points and aortic stenosis or regurgitation jets. We trained and evaluated all our models on a dataset of clinical CMR studies obtained from three NHS hospitals (n = 1,017 patients). Our results (mean accuracy = 0.93 and F1 score = 0.91), show that an expert-guided deep learning-based feature extraction and a classification model provide a feasible strategy for prescribing further, directed imaging, thus improving the efficiency and utility of CMR scanning\n"}, {"title": "DEUE: Delta Ensemble Uncertainty Estimation for a More Robust Estimation of Ejection Fraction", "abstract": "Left Ventricular Ejection Fraction (LVEF) as a critical clinical index is widely used to measure the functionality of the cyclic contraction of the left ventricle of the heart. Limited amount of available specialist-annotated data, low and variable quality of captured ultrasound images, and substantial inter/intra-observer variability in gold-standard measurements impose challenges on the robust data-driven automated estimation of LVEF in echocardiography (echo). Deep learning algorithms have recently shown state-of-the-art performance in cardiovascular image analysis. However, these algorithms are usually over-confident in their outputs even if they provide any measure of their output uncertainty. In addition, most of the uncertainty estimation methods in deep learning literature are either exclusively designed for classification tasks or are too memory/time expensive to be deployed on mobile devices or in clinical workflows that demand real-time memory-efficient estimations. In this work, we propose Delta Ensemble Uncertainty Estimation, a novel sampling-free method for estimating the epistemic uncertainty of deep learning algorithms for regression tasks. Our approach provides high-quality, architecture-agnostic and memory/time-efficient estimation of epistemic uncertainty with a single feed-forward pass through the network. We validate our proposed method on the task of LVEF estimation on EchoNet-Dynamic, a publicly available echo dataset, by performing a thorough comparison with multiple baseline methods.\n"}, {"title": "DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification", "abstract": "Multiple Instance Learning (MIL) is widely used in analyzing histopathological Whole Slide Images (WSIs). However, existing MIL methods do not explicitly model the data distribution, and instead they only learn a bag-level or instance-level decision boundary discriminatively by training a classifier. In this paper, we propose DGMIL: a feature distribution guided deep MIL framework for WSI classification and positive patch localization. Instead of designing complex discriminative network architectures, we reveal that the inherent feature distribution of histopathological image data can serve as a very effective guide for instance classification. We propose a cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy so that in the final feature space the positive and negative instances can be easily separated. Experiments on the CAMELYON16 dataset show that our method achieves new SOTA for both global classification and positive patch localization tasks.\n"}, {"title": "Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs", "abstract": "As deep networks require large amounts of accurately labeled training data, a strategy to collect sufficiently large and accurate annotations is as important as innovations in recognition methods. This is especially true for building Computer Aided Detection (CAD) systems for chest X-rays where domain expertise of radiologists is required to annotate the presence and location of abnormalities on X-ray images. However, there lacks concrete evidence that provides guidance on how much resource to allocate for data annotation such that the resulting CAD system reaches desired performance. Without this knowledge, practitioners often fall back to the strategy of collecting as much detail as possible on as much data as possible which is cost inefficient. In this work, we investigate how the cost of data annotation ultimately impacts the CAD model performance on classification and segmentation of chest abnormalities in frontal-view X-ray images. We define the cost of annotation with respect to the following three dimensions: quantity, quality and granularity of labels. Throughout this study, we isolate the impact of each dimension on the resulting CAD model performance on detecting 10 chest abnormalities in X-rays. On a large scale training data with over 120K X-ray images with gold-standard annotations, we find that cost-efficient annotations provide great value when collected in large amounts and lead to competitive performance when compared to models trained with only gold-standard annotations. We also find that combining large amounts of cost efficient annotations with only small amounts of expensive labels leads to competitive CAD models at a much lower cost.\n"}, {"title": "Diffusion Deformable Model for 4D Temporal Medical Image Generation", "abstract": "Temporal volume images with 3D+t (4D) information are often used in medical imaging to statistically analyze temporal dynamics or capture disease progression. Although deep-learning-based generative models for natural images have been extensively studied, approaches for temporal medical image generation such as 4D cardiac volume data are limited. In this work, we present a novel deep learning model that generates intermediate temporal volumes between source and target volumes. Specifically, we propose a diffusion deformable model (DDM) by adapting the denoising diffusion probabilistic model that has recently been widely investigated for realistic image generation. Our proposed DDM is composed of the diffusion and the deformation modules so that DDM can learn spatial deformation information between the source and target volumes and provide a latent code for generating intermediate frames along a geodesic path. Once our model is trained, the latent code estimated from the diffusion module is simply interpolated and fed into the deformation module, which enables DDM to generate temporal frames along the continuous trajectory while preserving the topology of the source image. We demonstrate the proposed method with the 4D cardiac MR image generation between the diastolic and systolic phases for each subject. Compared to the existing deformation methods, our DDM achieves high performance on temporal volume generation.\n"}, {"title": "Diffusion Models for Medical Anomaly Detection", "abstract": "In medical applications,  weakly supervised anomaly detection methods are of great interest, as only image-level annotations are required for training. Current anomaly detection methods mainly rely on generative adversarial networks or autoencoder models. Those models are often complicated to train or have difficulties to preserve fine details in the image. We present a novel weakly supervised anomaly detection method based on denoising diffusion implicit models. We combine the deterministic iterative noising and denoising scheme with classifier guidance for\nimage-to-image translation between diseased and healthy subjects. Our method generates very detailed anomaly maps without the need for a complex training procedure. We evaluate our method on the BRATS2020 dataset for brain tumor detection and the CheXpert dataset for detecting pleural effusions.\n"}, {"title": "Digestive Organ Recognition in Video Capsule Endoscopy based on Temporal Segmentation Network", "abstract": "The interpretation of video capsule endoscopy (VCE) usually takes more than an hour, which can be a tedious process for clinicians. To shorten the reading time of VCE, algorithms that automatically detect lesions in the small bowel are being actively developed, however, it is still necessary for clinicians to manually mark anatomic transition points in VCE. Therefore, anatomical temporal segmentation must first be performed automatically at the full-length VCE level for the fully automated reading. This study aims to develop an automated organ recognition method in VCE based on a temporal segmentation network. For temporal locating and classifying organs includ-ing the stomach, small bowel, and colon in long untrimmed videos, we use MS-TCN++ model containing temporal convolution layers. To improve tem-poral segmentation performance, a hybrid model of two state-of-the-art fea-ture extraction models (i.e., timeSformer and I3D) is used. Extensive exper-iments showed the effectiveness of the proposed method in capturing long-range dependencies and recognizing temporal segments of organs. For train-ing and validation of the proposed model, the dataset of 200 patients (100 normal and 100 abnormal patients) was used. For the test set of 40 patients (20 normal and 20 abnormal patients), the proposed method showed accura-cy of 96.15, F1-score@{50,75,90} of {96.17, 93.61, 86.80}, and segmental edit distance of 95.83 in the three-class classification of organs including the stomach, small bowel, and colon in the full-length VCE video.\n"}, {"title": "Discrepancy and Gradient-guided Multi-Modal Knowledge Distillation for Pathological Glioma Grading", "abstract": "The fusion of multi-modal data, e.g., pathology slides and genomic profiles, can provide complementary information and benefit glioma grading. However, genomic profiles are difficult to obtain due to the high costs and technical challenges, thus limiting the clinical applications of multi-modal diagnosis. In this work, we address the clinically relevant problem where paired pathology-genomic data are available during training, while only pathology slides are accessible for inference. To improve the performance of pathological grading models, we present a discrepancy and gradient-guided distillation framework to transfer the privileged knowledge from the multi-modal teacher to the pathology student. For the teacher side, to prepare useful knowledge, we propose a Discrepancy-induced Contrastive Distillation (DC-Distill) module that explores reliable contrastive samples with teacher-student discrepancy to regulate the feature distribution of the student. For the student side, as the teacher may include incorrect information, we propose a Gradient-guided Knowledge Refinement (GK-Refine) module that builds a knowledge bank and adaptively absorbs the reliable knowledge according to their agreement in the gradient space. Experiments on the TCGA GBM-LGG dataset show that our proposed distillation framework improves the pathological glioma grading significantly and outperforms other KD methods. Notably, with the sole pathology slides, our method achieves comparable performance with existing multi-modal methods. The code is available at https://github.com/CityU-AIM-Group/MultiModal-learning.\n"}, {"title": "Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images", "abstract": "Weakly supervised methods, such as class activation maps (CAM) based, have been applied to achieve bleeding segmentation with low annotation efforts in Wireless Capsule Endoscopy (WCE) images. However, the CAM labels tend to be extremely noisy, and there is an irreparable gap between CAM labels and ground truths for medical images. This paper proposes a new Discrepancy-basEd Active Learning (DEAL) approach to bridge the gap between CAMs and ground truths with a few annotations. Specifically, to liberate labor, we design a novel discrepancy decoder model and a CAMPUS (CAM, Pseudo-label and groUnd-truth Selection) criterion to replace the noisy CAMs with accurate model predictions and a few human labels. The discrepancy decoder model is trained with a unique scheme to generate standard, coarse and fine predictions. And the CAMPUS criterion is proposed to predict the gaps between CAMs and ground truths based on model divergence and CAM divergence. We evaluate our method on the WCE dataset and results show that our method outperforms the state-of-the-art active learning methods and reaches comparable performance to those trained with full annotated datasets with only 10% of the training data labeled. Codes will be available soon.\n"}, {"title": "Disentangle then Calibrate: Selective Treasure Sharing for Generalized Rare Disease Diagnosis", "abstract": "Annotated images for rare disease diagnosis are extremely hard to collect. Therefore, identifying rare diseases based on scarce amount of data is of far-reaching significance. Existing methods target only at rare diseases diagnosis, while neglect to preserve the performance of common disease diagnosis. To address this issue, we first disentangle the features of common diseases into a disease-shared part and a diseasespecific part, and then employ the disease-shared features alone to enrich rare-disease features, without interfering the discriminability of common diseases. In this paper, we propose a new setting, i.e., generalized rare disease diagnosis to simultaneously diagnose common and rare diseases. A novel selective treasure sharing (STS) framework is devised under this setting, which consists of a gradient-induced disentanglement (GID) module and a distribution-targeted calibration (DTC) module. The GID module disentangles the common-disease features into disease-shared channels and disease-specific channels based on the gradient agreement across different diseases. Then, the DTC module employs only disease-shared channels to enrich rare-disease features via distribution calibration. Hence, abundant rare-disease features are generated to alleviate model overfitting and ensure a more accurate decision boundary. Extensive experiments conducted on two medical image classification datasets demonstrate the superior performance of the proposed STS framework.\n"}, {"title": "DisQ: Disentangling Quantitative MRI Mapping of the Heart", "abstract": "Quantitative MRI (qMRI) of the heart has become an important clinical tool for examining myocardial tissue properties. Because heart is a moving object, it is usually imaged with electrocardiogram and respiratory gating during acquisition, to \u00e2\u0080\u009cfreeze\u00e2\u0080\u009d its motion. In reality, gating is more-often-than-not imperfect given the heart rate variability and non-ideal breath-hold. qMRI of the heart, consequently, is characteristic of varying image contrast as well as residual motion, the latter compromising the quality of quantitative mapping. Motion correction is an important step prior to parametric mapping, however, a long-standing difficulty for registering the dynamic sequence is that the contrast across frames varies wildly: depending on the acquisition scheme some frames can have extremely poor contrast, which fails both traditional optimization-based and modern learning-based registration methods. In this work, we propose a novel framework named DisQ, which Disentangles Quantitative mapping sequences into the latent space of contrast and anatomy, fully unsupervised. The disentangled latent spaces serve for the purpose of generating a series of images with identical contrast, which enables easy and accurate registration of all frames. We applied our DisQ method to the modified Look-Locker inversion recovery (MOLLI) sequence, and demonstrated improved performance of T1 mapping. In addition, we showed the possibility of generating a dynamic series of baseline images with exactly the same shape, strictly registered and perfectly \u00e2\u0080\u009cfrozen\u00e2\u0080\u009d. Our proposed DisQ methodology readily extends to other types of cardiac qMRI such as T2 mapping and perfusion.\n"}, {"title": "Distilling Knowledge from Topological Representations for Pathological Complete Response Prediction", "abstract": "In breast radiology, pathological Complete Response (pCR) predicts the treatment  response  after  neoadjuvant  chemotherapy, and therefore is a vital indicator for both personalized treatment and prognosis. Current prevailing  approaches for  pCR prediction either require complex feature engineering or employ sophisticated topological computation, which are not efficient while yielding limited performance boosts. In this paper, we present a simple yet effective technique implementing persistent homology to extract multi-dimensional topological representations from 3D data, making the computation much faster. To incorporate  the  extracted  topological information, we then propose a novel approach to distill the extracted topological knowledge into deep neural networks with response-based knowledge distillation. Our experimental results quantitatively show that the proposed approach achieves superior performance by increasing the accuracy from previously 85.1% to 90.5% in the pCR prediction and reducing the topological computation time by about 66% on a public dataset for breast DCE-MRI images.\n"}, {"title": "Domain Adaptive Mitochondria Segmentation via Enforcing Inter-Section Consistency", "abstract": "Deep learning-based methods for mitochondria segmentation require sufficient annotations on Electron Microscopy (EM) volumes, which are often expensive and time-consuming to collect. Recently, Unsupervised Domain Adaptation (UDA) has been proposed to avoid annotating on target EM volumes by exploiting annotated source EM volumes. However, existing UDA methods for mitochondria segmentation only address the intra-section gap between source and target volumes but ignore the inter-section gap between them, which restricts the generalization capability of the learned model on target volumes. In this paper, for the first time, we propose a domain adaptive mitochondria segmentation method via enforcing inter-section consistency. The key idea is to learn an inter-section residual on the segmentation results of adjacent sections using a CNN. The inter-section residuals predicted from source and target volumes are then aligned via adversarial learning. Meanwhile, guided by the learned inter-section residual, we can generate pseudo labels to supervise the segmentation of adjacent sections inside the target volume, which further enforces inter-section consistency. Extensive experiments demonstrate the superiority of our proposed method on four representative and diverse EM datasets. Code and models are available at https://github.com/weih527/DA-ISC.\n"}, {"title": "Domain Adaptive Nuclei Instance Segmentation and Classification via Category-aware Feature Alignment and Pseudo-labelling", "abstract": "Unsupervised domain adaptation (UDA) methods have been broadly utilized to improve the models\u00e2\u0080\u0099 adaptation ability in general computer vision. However, different from the natural images, there exist huge semantic gaps for the nuclei from different categories in histopathology images. It is still under-explored how could we build generalized UDA models for precise segmentation or classification of nuclei instances across different datasets. In this work, we propose a novel deep neural network, namely Category-Aware feature alignment and Pseudo-Labelling Network (CAPL-Net) for UDA nuclei instance segmentation and classification. Specifically, we first propose a category-level feature alignment module with dynamic learnable trade-off weights. Second, we propose to facilitate the model performance on the target data via self-supervised training with pseudo labels based on nuclei-level prototype features. Comprehensive experiments on cross-domain nuclei instance segmentation and classification tasks demonstrate that our approach outperforms state-of-the-art UDA methods with a remarkable margin.\n"}, {"title": "Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation", "abstract": "Although deep learning models have achieved remarkable success in medical image segmentation, the domain shift issue caused mainly by the highly variable quality of medical images is a major hurdle that prevents these models from being deployed for real clinical practices, since no one can predict the performance of a `well-trained\u00e2\u0080\u0099 model on a set of unseen clinical data. Previously, many methods have been proposed based on, for instance, CycleGAN or the Fourier transform to address this issue, which, however, suffer from either an inadequate ability to preserve anatomical structures or unexpectedly introduced artifacts. In this paper, we propose a multi-source-domain unsupervised domain adaptation (UDA) method called Domain specific Convolution and high frequency Reconstruction (DoCR) for medical image segmentation. We design an auxiliary high frequency reconstruction (HFR) task to facilitate UDA, and hence avoid the interference of the artifacts generated by the low-frequency component replacement. We also construct the domain specific convolution (DSC) module to boost the segmentation model\u00e2\u0080\u0099s ability to domain-invariant features extraction. We evaluate DoCR on a benchmark fundus image dataset. Our results indicate that the proposed DoCR achieves superior performance over other UDA methods in multi-domain joint optic cup and optic disc segmentation.\n"}, {"title": "Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach", "abstract": "Medical image synthesis has attracted increasing attention\nbecause it could generate missing image data, improving diagnosis and\nbenefits many downstream tasks. However, so far the developed synthesis\nmodel is not adaptive to unseen data distribution that presents domain\nshift, limiting its applicability in clinical routine. This work focuses on exploring domain adaptation (DA) of 3D image-to-image synthesis models.\nFirst, we highlight the technical difference in DA between classification,\nsegmentation and synthesis models. Second, we present a novel efficient\nadaptation approach based on 2D variational autoencoder which approximates 3D distributions. Third, we present empirical studies on the effect of the amount of adaptation data and the key hyper-parameters.\nOur results show that the proposed approach can significantly improve\nthe synthesis accuracy on unseen domains in a 3D setting. The code\nis publicly available at https://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis\n"}, {"title": "Domain-Prior-Induced Structural MRI Adaptation for Clinical Progression Prediction of Subjective Cognitive Decline", "abstract": "Growing evidence shows that subjective cognitive decline (SCD) among elderly individuals is the possible pre-clinical stage of Alzheimer\u00e2\u0080\u0099s disease (AD). To prevent the potential disease conversion, it is critical to investigate biomarkers for SCD progression. Previous learning-based methods employ T1-weighted magnetic resonance imaging (MRI) data to aid the future progression prediction of SCD, but often fail to build reliable models due to the insufficient number of subjects and imbalanced sample classes. A few studies suggest building a model on a large-scale AD-related dataset and then applying it to another dataset for SCD progression via transfer learning. Unfortunately, they usually ignore significant data distribution gaps between different centers/domains. With the prior knowledge that SCD is at increased risk of underlying AD pathology, we propose a domain-prior-induced structural MRI adaptation (DSMA) method for SCD progression prediction by mitigating the distribution gap between SCD and AD groups. The proposed DSMA method consists of two parallel feature encoders for MRI feature learning in the labeled source domain and unlabeled target domain, an attention block to locate potential disease-associated brain regions, and a feature adaptation module based on maximum mean discrepancy (MMD) for cross-domain feature alignment. Experimental results on the public ADNI dataset and an SCD dataset demonstrate the superiority of our method over several state-of-the-arts.\n"}, {"title": "DOMINO: Domain-aware Model Calibration in Medical Image Segmentation", "abstract": "Model calibration measures the agreement between the predicted probability estimates and the true correctness likelihood. Proper model calibration is vital for high-risk applications. Unfortunately, modern deep neural networks are poorly calibrated, compromising trustworthiness and reliability. Medical image segmentation particularly suffers from this due to the natural uncertainty of tissue boundaries. This is exasperated by their loss functions, which favor overconfidence in the majority classes. We address these challenges with DOMINO, a domain-aware model calibration method that leverages the semantic confusability and hierarchical similarity between class labels. Our experiments demonstrate that our DOMINO-calibrated deep neural networks outperform non-calibrated models and state-of-the-art morphometric methods in head image segmentation. Our results show that our method can consistently achieve better calibration, higher accuracy, and faster inference times than these methods, especially on rarer classes. This performance is attributed to our domain-aware regularization to inform semantic model calibration. These findings show the importance of semantic ties between class labels in building confidence in deep learning models. The framework has the potential to improve the trustworthiness and reliability of generic medical image segmentation models. The code for this article is available at: https://github.com/lab-smile/DOMINO.\n"}, {"title": "Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration", "abstract": "In order to tackle the difficulty associated with the ill-posed nature of the image registration problem, regularization is often used to constrain the solution space. For most learning-based registration approaches, the regularization usually has a fixed weight and only constrains the spatial transformation. Such convention has two limitations: (i) Besides the laborious grid search for the optimal fixed weight, the regularization strength of a specific image pair should be associated with the content of the images, thus the ``one value fits all\u00e2\u0080\u0099\u00e2\u0080\u0099 training scheme is not ideal; (ii) Only spatially regularizing the transformation may neglect some informative clues related to the ill-posedness. In this study, we propose a mean-teacher based registration framework, which incorporates an additional temporal consistency regularization term by encouraging the teacher model\u00e2\u0080\u0099s prediction to be consistent with that of the student model. More importantly, instead of searching for a fixed weight, the teacher enables automatically adjusting the weights of the spatial regularization and the temporal consistency regularization by taking advantage of the transformation uncertainty and appearance uncertainty. Extensive experiments on the challenging abdominal CT-MRI registration show that our training strategy can promisingly advance the original learning-based method in terms of efficient hyperparameter tuning and a better tradeoff between accuracy and smoothness.\n"}, {"title": "DRGen: Domain Generalization in Diabetic Retinopathy Classification", "abstract": "Domain Generalization is a challenging problem in deep learning especially in medical image analysis because of the huge diversity between different datasets. Existing papers in the literature tend to optimize performance on single target domains, without regards to model generalizability on other domains or distributions. High discrepancy in the number of images and major domain shifts, can therefore cause  single-source trained models to under-perform during testing. In this paper, we address the problem of domain generalization in Diabetic Retinopathy (DR) classification. The baseline for comparison is set as joint training on different datasets, followed by testing on each dataset individually. We therefore introduce a method that encourages seeking a flatter minima during training while imposing a regularization. This reduces gradient variance from different domains and therefore yields satisfactory results on out-of-domain DR classification. We show that adopting DR-appropriate augmentations enhances model performance and in-domain generalizability. By performing our evaluation on 4 open-source DR datasets, we show that the proposed domain generalization method outperforms separate and joint training strategies as well as well-established methods.\n"}, {"title": "DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal MRI Synthesis Network", "abstract": "Contrast-enhanced T1 (T1ce) is one of the most essential magnetic resonance imaging (MRI) modalities for diagnosing and analyzing brain tumors, especially gliomas. In clinical practice, common MRI modalities such as T1, T2, and fluid attenuation inversion recovery are relatively easy to access while T1ce is more challenging considering the additional cost and potential risk of allergies to the contrast agent. Therefore, it is of great clinical necessity to develop a method to synthesize T1ce from other common modalities. Current paired image translation methods typically have the issue of requiring a large amount of paired data and do not focus on specific regions of interest, e.g., the tumor region, in the synthesization process. To address these issues, we propose a Difficulty-perceived common-to-T1ce Semi-Supervised multimodal MRI Synthesis network (DS3-Net), involving both paired and unpaired data together with dual-level knowledge distillation. DS3-Net predicts a difficulty map to progressively promote the synthesis task. Specifically, a pixelwise constraint and a patchwise contrastive constraint are guided by the predicted difficulty map. Through extensive experiments on the publiclyavailable BraTS2020 dataset, DS3-Net outperforms its supervised counterpart in each respect. Furthermore, with only 5% paired data, the proposed DS3-Net achieves competitive performance with state-of-theart image translation methods utilizing 100% paired data, delivering an average SSIM of 0.8947 and an average PSNR of 23.60.\n"}, {"title": "DSP-Net: Deeply-Supervised Pseudo-Siamese Network for Dynamic Angiographic Image Matching", "abstract": "During percutaneous coronary intervention (PCI), severe elastic deformation of coronary arteries caused by cardiac movement is a serious disturbance to physicians. It increases the difficulty of estimating the relative position between interventional instruments and vessels, leading to inaccurate operation and higher intraoperative mortality. Providing doctors with dynamic angiographic images can be helpful. However, it often faces the challenges of indistinguishable features between consecutive frames and multiple modalities caused by individual differences. In this paper a novel deeply-supervised pseudo-siamese network (DSP-Net) is developed to solve the problem. A pseudo siamese attention dense (PSAD) block is designed to extract salient features from X-ray images with noisy background, and the deep supervision architecture is integrated to accelerate convergence. Evaluations are conducted on the CVM X-ray Database built by us, which consists of 51 sequences, showing that the proposed network can not only achieve state-of-the-art matching performance of 3.48 Hausdorff distance and 84.09% guidewire recall rate, but also demonstrate the great generality to images with different heart structures or fluoroscopic angles. Exhaustive experiment results indicate that our DSP-Net has the potential to assist doctors to overcome the visual misjudgment caused by the elastic deformation of the arteries and achieve safer procedure. \n"}, {"title": "DSR: Direct Simultaneous Registration for Multiple 3D Images", "abstract": "This paper presents a novel algorithm named Direct Simultaneous Registration (DSR) that registers a collection of 3D images in a simultaneous fashion without specifying any reference image, feature extraction and matching, or information loss or reuse. The algorithm optimizes the global poses of local image frames by maximizing the similarity between a predefined panoramic image and local images. Although we formulate the problem as a Direct Bundle Adjustment (DBA) that jointly optimizes the poses of local frames and the intensities of the panoramic image, by investigating the independence of pose estimation from the panoramic image in the solving process, DSR is proposed to solve the poses only and proved to be able to obtain the same optimal poses as DBA. The proposed method is particularly suitable for the scenarios where distinct features are not available, such as Transesophageal Echocardiography (TEE) images. DSR is evaluated by comparing it with four widely used methods via simulated and in-vivo 3D TEE images. It is shown that the proposed method outperforms these four methods in terms of accuracy and requires much fewer computational resources than the state-of-the-art accumulated pairwise estimates (APE). Codes of DSR are available at https://github.com/ZH-Mao/DSR.\n"}, {"title": "Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT", "abstract": "Single-photon emission computed tomography (SPECT) is a widely applied imaging approach for diagnosis of coronary artery diseases. Attenuation maps (\u00ce\u00bc-maps) derived from computed tomography (CT) are utilized for attenuation correction (AC) to improve diagnostic accuracy of cardiac SPECT. However, SPECT and CT are obtained sequentially in clinical practice, which potentially induces misregistration between the two scans. Convolutional neural networks (CNN) are powerful tools for medical image registration. Previous CNN-based methods for cross-modality registration either directly concatenated two input modalities as an early feature fusion or extracted image features using two separate CNN modules for a late fusion. These methods do not fully extract or fuse the cross-modality information. Besides, deep-learning-based rigid registration of cardiac SPECT and CT-derived \u00ce\u00bc-maps has not been investigated before. In this paper, we propose a Dual-Branch Squeeze-Fusion-Excitation (DuSFE) module for the registration of cardiac SPECT and CT-derived \u00ce\u00bc-maps. DuSFE fuses the knowledge from multiple modalities to recalibrate both channel-wise and spatial features for each modality. DuSFE can be embedded at multiple convolutional layers to enable feature fusion at different spatial dimensions. Our studies using clinical data demonstrated that a network embedded with DuSFE generated substantial lower registration errors and therefore more accurate AC SPECT images than previous methods.\n"}, {"title": "Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays", "abstract": "Chest X-ray (CXR) is the most typical radiological exam for diagnosis of various diseases. Due to the expensive and time-consuming annotations, detecting anomalies in CXRs in an unsupervised fashion is very promising. However, almost all of the existing methods consider anomaly detection as a one-class classification (OCC) problem. They model the distribution of only known normal images during training and identify the samples not conforming to normal profile as anomalies in the testing phase. A large number of unlabeled images containing anomalies are thus ignored in the training phase, although they are easy to obtain in clinical practice. In this paper, we propose a novel strategy, Dual-distribution Discrepancy for Anomaly Detection (DDAD), utilizing both known normal images and unlabeled images. The proposed method consists of two modules. During training, one module takes both known normal and unlabeled images as inputs, capturing anomalous features from unlabeled images in some way, while the other one models the distribution of only known normal images. Subsequently, inter-discrepancy between the two modules, and intra-discrepancy inside the module that is trained on only normal images are designed as anomaly scores to indicate anomalies. Experiments on three CXR datasets demonstrate that the proposed DDAD achieves consistent, significant gains and outperforms state-of-the-art methods. Code is available at https://github.com/caiyu6666/DDAD.\n"}, {"title": "Dual-graph Learning Convolutional Networks for Interpretable Alzheimer\u00e2\u0080\u0099s Disease Diagnosis", "abstract": "In this paper, we propose a dual-graph learning convolutional network (dGLCN) to achieve interpretable Alzheimer\u00e2\u0080\u0099s disease (AD) diagnosis, by jointly investigating subject graph learning and feature graph learning in the graph convolution network (GCN) framework. Specifically, we first construct two initial graphs to consider both the subject diversity and the feature diversity. We further fuse these two initial graphs into the GCN framework so that they can be iteratively updated (i.e., dual-graph learning) while conducting representation learning. As a result, the dGLCN achieves interpretability in both subjects and brain regions through the subject importance and the feature importance, and the generalizability by overcoming the issues, such as limited subjects and noisy subjects. Experimental results on the Alzheimer\u00e2\u0080\u0099s disease neuroimaging initiative (ADNI)  datasets show that our dGLCN outperforms all comparison methods for binary classification.  The codes of dGLCN are available on https://github.com/xiaotingsong/dGLCN.\n"}, {"title": "Dual-HINet: Dual Hierarchical Integration Network of Multigraphs for Connectional Brain Template Learning", "abstract": "A connectional brain template (CBT) is a normalized representation of a population of brain multigraphs, where two anatomical regions of interest (ROIs) are connected by multiple edges. Each edge captures a particular type of interaction between pairs of ROIs (e.g., structural/functional). Learning a well-centered and representative CBT of a particular brain multigraph population (e.g., healthy or atypical) is a means of modeling complex and varying ROI interactions in a holistic manner. Existing methods generate CBTs by locally integrating heterogeneous multi-edge attributes (e.g., weights and features). However, such methods are agnostic to brain network modularity as they ignore the hierarchical structure of neural interactions. Furthermore, they only perform node-level integration at the individual level without learning the multigraph representation at the group level in a layer-wise manner. To address these limitations, we propose Dual Hiearchical Integration Network (dual-HINet) for connectional brain template estimation, which simultaneously learns the node-level and cluster-level integration processes using a dual graph neural network architecture. We also propose a novel loss objective to jointly learn the clustering assignment across different edge types and the centered CBT representation of the population multigraphs. Our Dual-HINet significantly outperforms state-of-the-art methods for learning CBTs on both small-scale and large-scale multigraph connectomic datasets.\n"}, {"title": "DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging", "abstract": "Multi-contrast magnetic resonance imaging (MC-MRI) has been widely used for the diagnosis and characterization of tumors and lesions, as multi-contrast MR images are capable of providing complementary information for more comprehensive diagnosis and evaluation. However, it usually suffers from long scanning time to acquire multi-contrast MR images; in addition, long scanning time may lead to motion artifacts, degrading the image quality. Recently, many studies have proposed to employ the fully-sampled image of one contrast with short acquisition time to guide the reconstruction of the other contrast with long acquisition time so as to speed up the scanning. However, these studies still have two shortcomings. First, they simply concatenate the features of the two contrast images together without digging and leveraging the inherent and deep correlation between them. Second, as aliasing artifacts are complicated and non-local, sole image domain reconstruction with local dependencies are far from enough to eliminate these artifacts and achieve faithful reconstruction results. We present a novel Dual-Domain Cross-Attention Fusion (DuDoCAF) scheme with recurrent transformer to comprehensively address these shortcomings. Specifically, the proposed CAF scheme enables deep and effective fusion of features extracted from two modalities. The dual-domain recurrent learning allows our model to restore signals in both k-space and image domains, and hence more comprehensively remove the artifacts. In addition, we tame recurrent transformers to capture long-range dependencies from the fused feature maps to further enhance reconstruction performance. Extensive experiments on public fastMRI and clinical brain datasets demonstrate that the proposed DuDoCAF outperforms the state-of-the-art methods under different under-sampling patterns and acceleration rates."}, {"title": "Dynamic Bank Learning for Semi-supervised Federated Image Diagnosis with Class Imbalance", "abstract": "class distributions among unlabeled clients is still unsolved for real-world use. In this paper, we study a practical yet challenging problem of class imbalanced semi-supervised FL (imFed-Semi), which allows all clients to have only unlabeled data while the server just has a small amount of labeled data. This imFed-Semi problem is addressed by a novel dynamic bank learning scheme, which improves client training by exploiting class proportion information. This scheme consists of two parts, i.e., the dynamic bank construction to distill various class proportions for each local client, and the sub-bank classification to impose the local model to learn different class proportions. We evaluate our approach on two public real-world medical datasets, including the intracranial hemorrhage diagnosis with 25,000 CT slices and skin lesion diagnosis with 10,015 dermoscopy images. The effectiveness of our method has been validated with significant performance improvements (7.61% and 4.69%) compared with the second-best on the accuracy, as well as comprehensive analytical studies. Code is available at https://github.com/med-air/imFedSemi.\n"}, {"title": "EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography", "abstract": "Learning spatiotemporal features is an important task for efficient video understanding especially in medical images such as echocardiograms. Convolutional neural networks (CNNs) and more recent vision transformers (ViTs) are the most commonly used methods with limitations per each. CNNs are good at capturing local context but fail to learn global information across video frames. On the other hand, vision transformers can incorporate global details and long sequences but are computationally expensive and typically require more data to train. In this paper, we propose a method  that addresses the limitations we typically face when training on medical video data such as echocardiographic scans. The algorithm we propose (EchoCoTr) utilizes the strength of vision transformers and CNNs to tackle the problem of estimating the left ventricular ejection fraction (LVEF) on ultrasound videos. We demonstrate how the proposed method outperforms state-of-the-art work to-date on the EchoNet-Dynamic dataset with MAE of 3.95 and R2 of 0.82. These results show noticeable improvement compared to all published research. In addition, we show extensive ablations and comparisons with several algorithms, including ViT and BERT. The code is available\nat https://github.com/BioMedIA-MBZUAI/EchoCoTr\n"}, {"title": "EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks", "abstract": "Ejection fraction (EF) is a key indicator of cardiac function, allowing identification of patients prone to heart dysfunctions such as heart failure. EF is estimated from cardiac ultrasound videos known as echocardiograms (echo) by manually tracing the left ventricle and estimating its volume on certain frames. These estimations exhibit high inter-observer variability due to the manual process and varying video quality. Such sources of inaccuracy and the need for rapid assessment necessitate reliable and explainable machine learning techniques. In this work, we introduce EchoGNN, a model based on graph neural networks (GNNs) to estimate EF from echo videos. Our model first infers a latent echo-graph from the frames of one or multiple echo cine series. It then estimates weights over nodes and edges of this graph, indicating the importance of individual frames that aid EF estimation. A GNN regressor uses this weighted graph to predict EF. We show, qualitatively and quantitatively, that the learned graph weights provide explainability through identification of critical frames for EF estimation, which can be used to determine when human intervention is required. On EchoNet-Dynamic public EF dataset, EchoGNN achieves EF prediction performance that is on par with state of the art and provides explainability, which is crucial given the high inter-observer variability inherent in this task.\n"}, {"title": "Edge-oriented Point-cloud Transformer for 3D Intracranial Aneurysm Segmentation", "abstract": "Point-based 3D intracranial aneurysm segmentation is fundamental for automatic aneurysm diagnosis. Though impressive performances, existing point-based 3D segmentation frameworks still perform poorly around the edge between vessels and aneurysms, which is extremely harmful for the clipping surgery process. To address the issue, we propose an Edge-oriented Point-cloud Transformer Network (EPT-Net) to produce precise segmentation predictions. The framework consists of three paradigms, i.e., dual stream transformer (DST), outer-edge context dissimilation (OCD) and inner-edge hard-sample excavation (IHE). In DST, a dual stream transformer is proposed to jointly optimize the semantics stream and the edge stream, where the latter imposes more supervision around the edge and help the semantics stream produce sharper boundaries. In OCD, aiming to refine features outside the edge, an edge-separation graph is constructed where connections across the edge are prohibited, thereby dissimilating contexts of points belonging to different categories. Upon that, graph convolution is performed to refine the confusing features via information exchange with dissimilated contexts. In IHE, to further refine features inside the edge, triplets (i.e. anchor, positive and negative) are built up around the edge, and contrastive learning is employed. Differently from previous contrastive methods of point clouds, we only select points nearby the edge as hard-negatives, providing informative clues for discriminative feature learning. Extensive experiments on the 3D intracranial aneurysm dataset IntrA demonstrate the superiority of our EPT-Net compared with state-of-the-art methods.\n"}, {"title": "Effective Opportunistic Esophageal Cancer Screening using Noncontrast CT Imaging", "abstract": "Esophageal cancer is the second most deadly cancer. Early detection of resectable/curable esophageal cancers has a great potential to reduce mortality, but no guideline-recommended screening test is available. Although some screening methods have been developed, they are expensive, might be difficult to apply to the general population, and often fail to achieve satisfactory sensitivity for identifying early-stage cancers. In this work, we investigate the feasibility of esophageal tumor detection and classification (cancer or benign) on the noncontrast CT scan, which could potentially be used for opportunistic cancer screening. Global context features of the esophagus have been proven in clinical practice as key signs for cancer detection, especially early-stage ones. To capture such global context, a novel position-sensitive self-attention is proposed to augment nnUNet with non-local interactions. Our model achieves a sensitivity of 93.0% and specificity of 97.5% for the detection of esophageal tumors on a holdout testing set with 180 patients. In comparison, the mean sensitivity and specificity of four doctors are 75.0% and 83.8%, respectively. For the classification task, our model outperforms the mean doctors by absolute margins of 17%, 31%, and 14% for cancer, benign tumor, and normal, respectively. Compared with established state-of-the-art esophageal cancer screening methods, e.g., blood testing and endoscopy AI system, our method has comparable performance and is even more sensitive for early-stage cancer and benign tumor. Our proposed method is a novel, non-invasive, low-cost, and highly accurate tool for opportunistic screening of esophageal cancer.\n"}, {"title": "Efficient Bayesian Uncertainty Estimation for nnU-Net", "abstract": "The self-configuring nnU-Net has achieved leading performance in a large range of medical image segmentation challenges. It is widely considered as the model of choice and a strong baseline for medical image segmentation. However, despite its extraordinary performance, nnU-Net does not supply a measure of uncertainty to indicate its possible failure. This can be problematic for large-scale image segmentation applications, where data are heterogeneous and nnU-Net may fail without notice. In this work, we introduce a novel method to estimate the nnU-Net uncertainty for medical image segmentation. We propose a highly effective scheme for posterior sampling of weight space for Bayesian uncertainty estimation. Different from previous baseline methods such as Monte Carlo Dropout and mean-field Bayesian Neural Networks, our proposed method does not require a variational architecture and keeps the original nnU-Net architecture intact, thereby preserving its excellent performance and ease of use. Additionally, we boost the segmentation performance over the original nnU-Net via ensembling multi-modal posterior models. We applied our method on the public ACDC and M&M datasets of cardiac MRI and demonstrated improved uncertainty estimation over a range of baseline methods. The proposed method further strengthens nnU-Net for medical image segmentation in terms of both segmentation accuracy and quality control.\n"}, {"title": "Efficient Biomedical Instance Segmentation via Knowledge Distillation", "abstract": "Biomedical instance segmentation is vulnerable to complicated instance morphology, resulting in over-merge and over-segmentation. Recent advanced methods apply convolutional neural networks to predict pixel embeddings to overcome this problem. However, these methods suffer from heavy computational burdens and massive storage. In this paper, we present the first knowledge distillation method tailored for biomedical instance segmentation to transfer the knowledge from a cumbersome teacher network to a lightweight student one. Different from existing distillation methods on other tasks, we consider three kinds of essential knowledge of the instance segmentation task, i.e., instance-level features, instance relationships in the feature space and pixel-level instance boundaries. Specifically, we devise two distillation schemes: (i) instance graph distillation that transfers the knowledge of instance-level features and instance relationships by the instance graphs built from embeddings of the teacher-student pair, respectively, and (ii) pixel affinity distillation that converts pixel embeddings into pixel affinities and explicitly transfers the structured knowledge of instance boundaries encoded in affinities. Experimental results on a 3D electron microscopy dataset (CREMI) and a 2D plant phenotype dataset (CVPPP) demonstrate that the student models trained through our distillation method use fewer than 1% parameters and less than 10% inference time while achieving promising performance compared with corresponding teacher models.\n"}, {"title": "Efficient population based hyperparameter scheduling for medical image segmentation", "abstract": "The training hyperparameters (learning rate, augmentation policies, e.t.c) are key factors affecting the performance of deep networks for medical image segmentation. Manual or automatic hyperparameter optimization~(HPO) is used to improve the performance. However, manual tuning is infeasible for a large number of parameters, and existing automatic HPO methods like Bayesian optimization are extremely time consuming. Moreover, they can only find a fixed set of hyperparameters. Population based training (PBT) has shown its ability to find dynamic hyperparameters and has fast search speed by using parallel training processes. However, it is still expensive for large 3D medical image datasets with limited GPUs, and the performance lower bound is unknown. In this paper, we focus on improving the network performance using hyperparameter scheduling via PBT with limited computation cost. The core idea is to train the network with a default setting from prior knowledge, and finetune using PBT based hyperparameter scheduling. Our method can achieve 1% ~ 3% performance improvements over default setting while only taking 3% ~ 10% computation cost of training from scratch using PBT.\n"}, {"title": "Electron Microscope Image Registration using Laplacian Sharpening Transformer U-Net", "abstract": "Image registration is an essential task in electron microscope (EM) image analysis, which aims to accurately warp the moving image to align with the fixed image, to reduce the spatial deformations across serial slices resulted during image acquisition. Existing learning-based registration approaches are primarily based on Convolution Neural Networks (CNNs). However, for the requirements of EM image registration, CNN-based methods lack the capability of learning global and long-term semantic information. In this work, we propose a new framework, Cascaded LST-UNet, which integrates a sharpening skip-connection layer with the Swin Transformer based U-Net structure in a cascaded manner for unsupervised EM image registration. Our experimental results on a public dataset show that our method consistently outperforms the baseline approaches."}, {"title": "Embedding Gradient-based Optimization in Image Registration Networks", "abstract": "Deep learning (DL) image registration methods amortize the costly pair-wise iterative optimization by training deep neural networks to predict the optimal transformation in one fast forward-pass. In this work, we bridge the gap between traditional iterative energy optimization-based registration and network-based registration, and propose Gradient Descent Network for Image Registration (GraDIRN). Our proposed approach trains a DL network that embeds unrolled multi-resolution gradient-based energy optimization in its forward pass, which explicitly enforces image dissimilarity minimization in its update steps. Extensive evaluations were performed on registration tasks using 2D cardiac MR and 3D brain MR images. We demonstrate that our approach achieved state-of-the-art registration performance while using fewer learned parameters, with good data efficiency and domain robustness.\n"}, {"title": "Embedding Human Brain Function via Transformer", "abstract": "BOLD fMRI has been an established tool for studying the human brain\u00e2\u0080\u0099s functional organization. Considering the high dimensionality of fMRI data, various computational techniques have been developed to perform the dimension reduction such as independent component analysis (ICA) or sparse dictionary learning (SDL). These methods decompose the fMRI as compact functional brain networks, and then build the correspondence of those brain networks across individuals by viewing the brain networks as one-hot vectors and performing their matching. However, these one-hot vectors do not encode the regularity and variability of different brains, and thus cannot effectively represent the functional brain activities in different brains and at different time points. To bridge the gaps, in this paper, we propose a novel unsupervised embedding framework based on Transformer to encode the brain function in a compact, stereotyped and comparable latent space where the brain activities are represented as dense embedding vectors. The framework is evaluated on the publicly available Human Connectome Project (HCP) task based fMRI dataset. The experiment on brain state prediction downstream task indicates the effectiveness and generalizability of the learned embeddings. We also explore the interpretability of the embedding vectors and achieve promising result. In general, our approach provides novel insights on representing regularity and variability of human brain function in a general, comparable, and stereotyped latent space.\n"}, {"title": "End-to-End cell recognition by point annotation", "abstract": "Reliable quantitative analysis of immunohistochemical staining images requires accurate and robust cell detection and classification. Recent weakly-supervised methods usually estimate probability density maps for cell recognition. However, in dense cell scenarios, their performance can be limited by pre- and post-processing as it is impossible to find a universal parameter setting. In this paper, we introduce an end-to-end framework that applies direct regression and classification for preset anchor points. Specifically, we propose a pyramidal feature aggregation strategy to combine low-level features and high-level semantics simultaneously, which provides accurate cell recognition for our purely point-based model. In addition, an optimized cost function is designed to adapt our multi-task learning framework by matching ground truth and predicted points. The experimental results demonstrate the superior accuracy and efficiency of the proposed method, which reveals the high potentiality in assisting pathologist assessments.\n"}, {"title": "End-to-End Evidential-Efficient Net for Radiomics Analysis of Brain MRI to Predict Oncogene Expression and Overall Survival", "abstract": "We presented a novel radiomics approach using multimodality MRI to predict the expression of an oncogene (O6-Methylguanine-DNA methyltransferase, MGMT) and overall survival (OS) of glioblastoma (GBM) patients. Specifically, we employed an EffNetV2-T, which was downscaled and modified from EfficientNetV2, as the feature extractor. Besides, we used evidential layers based to control the distribution of prediction outputs. The evidential layers help to classify the high-dimensional radiomics features to predict the methylation status of MGMT and OS. Tests showed that our model achieved an accuracy of 0.844, making it possible to use as a clinic enabling technique in the diagnosing and management of GBM. Comparison results indicated that our method performed better than existing work.\n"}, {"title": "End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology", "abstract": "Current approaches for classification of whole slide images (WSI) in digital pathology predominantly utilize a two-stage learning pipeline. The first stage identifies areas of interest (e.g. tumor tissue), while the second stage processes cropped tiles from these areas in a supervised fashion. During inference, a large number of tiles are combined into a unified prediction for the entire slide. A major drawback of such approaches is the requirement for task-specific auxiliary labels which are not acquired in clinical routine."}, {"title": "End-to-end Multi-Slice-to-Volume Concurrent Registration and Multimodal Generation", "abstract": "For interventional procedures, a real-time mapping between treatment guidance images and planning data is challenging yet essential for successful therapy implementation. Because of time and machine constraints, it involves imaging of different modalities, resolutions and dimensions, along with severe out-of-plane deformations to handle. In this paper, we introduce MSV-RegSyn-Net, a novel, scalable, deep learning-based framework for concurrent slice-to-volume registration and high-resolution modality transfer synthesis. It consists of an end-to-end pipeline made up of (i) a cycle generative adversarial network for multimodal image translation combined with (ii) a multi-slice-to-volume deformable registration network. The concurrent nature of our approach creates mutual benefit for both tasks: image translation is naturally eased by explicit handling of out-of-plane deformations while registration benefits from bringing multimodal signals into the same domain. Our model is fully unsupervised and does not require any ground-truth deformation or segmentation mask. It obtains superior qualitative and quantitative performance for multi-slice MR to 3D CT pelvic imaging compared to state-of-the-art traditional and learning-based methods on both tasks. \n"}, {"title": "End-to-End Segmentation of Medical Images via Patch-wise Polygons Prediction", "abstract": "The leading medical image segmentation methods represent the output map as a pixel grid. We present an alternative in which the object edges are modeled, per image patch, as a polygon with $k$ vertices that is coupled with per-patch label probabilities. The vertices are optimized by employing a differentiable neural renderer to create a raster image. The delineated region is then compared with the ground truth segmentation. Our method obtains multiple state-of-the-art results for the Gland segmentation dataset (Glas), the Nucleus challenges (MoNuSeg), and multiple polyp segmentation datasets, as well as for non-medical benchmarks, including Cityscapes, CUB, and Vaihingen. \nOur code for training and reproducing these results is attached as a supplement.\n"}, {"title": "Enforcing connectivity of 3D linear structures using their 2D projections", "abstract": "Many biological and medical tasks require the delineation of 3D curvilinear structures such as blood vessels and neurites from image volumes. This is typically done using neural networks trained by minimizing voxel-wise loss functions that do not capture the topological properties of these structures. As a result, the connectivity of the recovered structures is often wrong, which lessens their usefulness. In this paper, we propose to improve the 3D connectivity of our results by minimizing a sum of topology-aware losses on their 2D projections. This suffices to increase the accuracy and to reduce the annotation effort required to provide the required annotated training data. \n"}, {"title": "Enhancing model generalization for substantia nigra segmentation using a test-time normalization-based method", "abstract": "Automatic segmentation of substantia nigra (SN), which is Parkinson\u00e2\u0080\u0099s disease-related tissue,  is an important step toward accurate computer-aided diagnosis systems. Conventional methods for SN segmentation depend heavily on such limited modalities of magnetic resonance imaging (MRI) as neuromelanin and quantitative susceptibility mapping, which require longer imaging times and are rare in public datasets. To enable a multi-modal investigation for SN anatomic alterations based on medical bigdata researches, the need for automated SN segmentation arises from commonly investigated T2-weighted MRIs. To improve the performance of the automated SN segmentation from a T2-weighted MRI and enhance the model generalization for cross-center researches, this paper proposes a novel test-time normalization (TTN) method to increase the geometric and intensity similarity between the query data and the model\u00e2\u0080\u0099s trained data. Our proposed method requires no additional training procedure or extra annotation for the unseen data. Our results showed that our proposed TTN achieved a mean Dice score of 71.08% in comparison with the baseline model\u00e2\u0080\u0099s 69.87% score with in-house dataset. Additionally, improved SN segmentation performance was observed from the unseen and unlabeled datasets.\n"}, {"title": "Ensembled Prediction of Rheumatic Heart Disease from Ungated Doppler Echocardiography Acquired in Low-Resource Settings", "abstract": "Rheumatic heart disease (RHD) is a common medical condition in children in which acute rheumatic fever causes permanent damage to the heart valves, thus impairing the heart\u00e2\u0080\u0099s ability to pump blood. Doppler echocardiography is a popular diagnostic tool used in the detection of RHD. However, the execution of this assessment requires the work of skilled physicians, which poses a problem of accessibility, especially in low-income countries with limited access to clinical experts. This paper presents a novel, automated, deep learning-based method to detect RHD using color Doppler echocardiography clips. We first homogenize the analysis of ungated echocardiograms by identifying two acquisition views (parasternal and apical), followed by extracting the left atrium regions during ventricular systole. Then, we apply a model ensemble of multi-view 3D convolutional neural networks and a multi-view Transformer to detect RHD. This model allows our analysis to benefit from the inclusion of spatiotemporal information and uses an attention mechanism to identify the relevant temporal frames for RHD detection, thus improving the ability to accurately detect RHD. The performance of this method was assessed using 2,136 color Doppler echocardiography clips acquired at the point of care of 591 children in low-resource settings, showing an average accuracy of 0.78, sensitivity of 0.81, and specificity of 0.74. These results are similar to RHD detection conducted by expert clinicians and superior to the state-of-the-art approach. Our novel model thus has the potential to improve RHD detection in patients with limited access to clinical experts.\n"}, {"title": "Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores", "abstract": "Machine learning models are typically deployed in a test setting that differs from the training setting, potentially leading to decreased model performance because of domain shift. If we could estimate the performance that a pre-trained model would achieve on data from a specific deployment setting, for example a certain clinic, we could judge whether the model could safely be deployed or if its performance degrades unacceptably on the specific data. Existing approaches estimate this based on the confidence of predictions made on unlabeled test data from the deployment\u00e2\u0080\u0099s domain. We find existing methods struggle with data that present class imbalance, because the methods used to calibrate confidence do not account for bias induced by class imbalance, consequently failing to estimate class-wise accuracy. Here, we introduce class-wise calibration within the framework of performance estimation for imbalanced datasets. Specifically, we derive class-specific modifications of state-of-the-art confidence-based model evaluation methods including temperature scaling (TS), difference of confidences (DoC), and average thresholded confidence (ATC). We also extend the methods to estimate Dice similarity coefficient (DSC) in image segmentation. We conduct experiments on four tasks and find the proposed modifications consistently improve the estimation accuracy for imbalanced datasets. Our methods improve accuracy estimation by 18\\% in classification under natural domain shifts, and double the estimation accuracy on segmentation tasks, when compared with prior methods.\n"}, {"title": "Evidence fusion with contextual discounting for multi-modality medical image segmentation", "abstract": "As information sources are usually imperfect, it is necessary to take into account their reliability in multi-source information fusion tasks. In this paper, we propose a new deep framework allowing us to merge multi-MR image segmentation results using the formalism of Dempster-Shafer theory while taking into account the reliability of different modalities relative to different classes. The framework is composed of an encoder-decoder feature extraction module, an evidential segmentation module that computes a belief function at each voxel for each modality, and a multi-modality evidence fusion module, which assigns a vector of discount rates to each modality evidence and combines the discounted evidence using Dempster\u00e2\u0080\u0099s rule. The whole framework is trained by minimizing a new loss function based on a discounted Dice index to increase segmentation accuracy and reliability. The method was evaluated on the BraTs 2021 database of 1251 patients with brain tumors. Quantitative and qualitative results show that our method outperforms the state of the art, and implements an effective new idea for merging multi-information within deep neural networks.\n"}, {"title": "Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification", "abstract": "The COVID-19 pandemic has threatened global health. Many studies have applied deep convolutional neural networks (CNN) to recognize COVID-19 based on chest 3D computed tomography (CT). Recent works show that no model generalizes well across CT datasets from different countries, and manually designing models for specific datasets requires expertise; thus, neural architecture search (NAS) that aims to search models automatically has become an attractive solution. To reduce the search cost on large 3D CT datasets, most NAS-based works use the weight-sharing (WS) strategy to make all models share weights within a supernet; however, WS inevitably incurs search instability, leading to inaccurate model estimation. In this work, we propose an efficient Evolutionary Multi-objective ARchitecture Search (EMARS) framework. We propose a new objective, namely potential, which can help exploit promising models to indirectly reduce the number of models involved in weights training, thus alleviating search instability. We demonstrate that under objectives of accuracy and potential, EMARS can balance exploitation and exploration, i.e., reducing search time and finding better models. Our searched models are small and perform better than prior works on three public COVID-19 3D CT datasets.\n"}, {"title": "Explainable Contrastive Multiview Graph Representation of Brain, Mind, and Behavior", "abstract": "Understanding the intrinsic patterns of human brain is important to make inferences about the mind and brain-behavior association. Electrophysiological methods(i.e. MEG/EEG) provide direct measures of neural activity without the effect of vascular confounds. The blood oxygenated level-dependent (BOLD) signal of functional MRI (fMRI) reveals the spatial and temporal brain activity across different brain regions. However, it is unclear how to associate the high temporal resolution Electrophysiological measures with high spatial resolution fMRI signals. Here, we present a novel interpretable model for coupling the structure and function activity of brain based on heterogeneous contrastive graph representation.The proposed method is able to link manifest variables of the brain (i.e. MEG, MRI, fMRI and behavior performance) and quantify the intrinsic coupling strength of different modal signals. The proposed method learns the heterogeneous node and graph representations by contrasting the structural and temporal views through the mind to multimodal brain data.  The first experiment with 1200 subjects from Human connectome Project (HCP) shows that the proposed method outperforms the existing approaches in predicting individual gender and enabling the location of the importance of brain regions with sex difference. The second experiment associates the structure and temporal views between the low-level sensory regions and high-level cognitive ones. The experimental results demonstrate that the dependence of structural and temporal views varied spatially through different modal variants. The proposed method enables the heterogeneous biomarkers explanation for different brain measurements. \n"}, {"title": "Explaining Chest X-ray Pathologies in Natural Language", "abstract": "Most deep learning algorithms lack explanations for their predictions, which limits their deployment in clinical practice. Approaches to improve explainability, especially in medical imaging, have often been shown to convey limited information, be overly reassuring, or lack robustness. In this work, we introduce the task of generating natural language explanations (NLEs) to justify predictions made on medical images. NLEs are human-friendly and comprehensive and enable the training of intrinsically explainable models. To this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of various thoracic pathologies and chest X-ray findings. We propose a general approach to solve the task and evaluate several architectures on this dataset, including via clinician assessment.\n"}, {"title": "Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation", "abstract": "Semi-supervised segmentation remains challenging in medical imaging since the amount of annotated medical data is often scarce and there are many blurred pixels near the adhesive edges or in the low-contrast regions. To address the issues, we advocate to firstly constrain the consistency of pixels with and without strong perturbations to apply a sufficient smoothness constraint and further encourage the class-level separation to exploit the low-entropy regularization for the model training. Particularly, in this paper, we propose the SS-Net for semi-supervised medical image segmentation tasks, via exploring the pixel-level smoothness and inter-class separation at the same time. The pixel-level smoothness forces the model to generate invariant results under adversarial perturbations. Meanwhile, the inter-class separation encourages individual class features should approach their corresponding high-quality prototypes, in order to make each class distribution compact and separate different classes. We evaluated our SS-Net against five recent methods on the public LA and ACDC datasets. Extensive experimental results under two semi-supervised settings demonstrate the superiority of our proposed SS-Net model, achieving new state-of-the-art (SOTA) performance on both datasets. The code is available at https://github.com/ycwu1997/SS-Net.\n"}, {"title": "Extended Electrophysiological Source Imaging with Spatial Graph Filters", "abstract": "Electrophysiological Source Imaging (ESI) refers to the process of localizing the brain source activation patterns given measured Electroencephalography (EEG) or Magnetoencephalography (MEG) signal from the scalp. Recent studies have focused on designing sophisticated neurophysiologically plausible regularizations or efficient estimation frameworks to solve the ESI problem, with the underlying assumption that brain source activation has some specific structures. \nEstimation of both source location and its extents is important in clinical applications. However, estimating the high dimensional extended location is challenging due to the highly coherent columns in the leadfield matrix, resulting in a reconstructed spiky spurious sources. In this work, we describe an efficient and accurate framework by exploiting the graph structure defined in the 3D mesh of the brain. Specifically, we decompose the graph signal representation in the source space into low-, medium-, and high-frequency subspaces, and project the source signal into the graph low-frequency subspace.\nWe further introduce a low-rank representation with temporal graph regularization in the projected space to build the ESI framework, which can be efficiently solved.\nExperiments with simulated data and real world EEG data demonstrated the superiority of the proposed paradigm for estimating brain source extents.\n"}, {"title": "FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis", "abstract": "Many works have shown that deep learning-based medical image classification models can exhibit bias toward certain demographic attributes like race, gender, and age. Existing bias mitigation methods primarily focus on learning debiased models, which may not necessarily guarantee all sensitive information can be removed and usually comes with considerable accuracy degradation on both privileged and unprivileged groups. To tackle this issue, we propose a method, FairPrune, that achieves fairness by pruning. Conventionally, pruning is used to reduce the model size for efficient inference. However, we show that pruning can also be a powerful tool to achieve fairness. Our observation is that during pruning, each parameter in the model has different importance for different groups\u00e2\u0080\u0099 accuracy. By pruning the parameters based on this importance difference, we can reduce the accuracy difference between the privileged group and the unprivileged group to improve fairness without a large accuracy drop. To this end, we use the second derivative of the parameters of a pre-trained model to quantify the importance of each parameter with respect to the model accuracy for each group. Experiments on two skin lesion diagnosis datasets over multiple sensitive attributes demonstrate that our method can greatly improve fairness while keeping the average accuracy of both groups as high as possible.\n"}, {"title": "Fast Automatic Liver Tumor Radiofrequency Ablation Planning via Learned Physics Model", "abstract": "Radiofrequency ablation is a minimally-invasive therapy recommended for the treatment of primary and secondary liver cancer in early stages and when resection or transplantation is not feasible. To significantly reduce chances of local recurrences, accurate planning is required, which aims at finding a safe and feasible needle trajectory to an optimal electrode position achieving full coverage of the tumor as well as a safety margin. Computer-assisted algorithms, as an alternative to the time-consuming manual planning performed by the clinicians, commonly neglect the underlying physiology and rely on simplified, spherical or ellipsoidal ablation estimates. To drastically speed up biophysical simulations and enable patient-specific ablation planning, this work investigates the use of non-autoregressive operator learning. The proposed architecture, trained on 1,800 biophysics-based simulations, is able to match the heat distribution computed by a finite-difference solver with a root mean squared error of 0.51+-0.50\u00c2\u00b0C and the estimated ablation zone with a mean dice score of 0.93+-0.05, while being over 100 times faster. When applied to single electrode automatic ablation planning on retrospective clinical data, our method achieves patient-specific results in less than 4 mins and closely matches the finite-difference-based planning, while being at least one order of magnitude faster. Run times are comparable to those of sphere-based planning while accounting for the perfusion of liver tissue and the heat sink effect of large vessels.\n"}, {"title": "Fast FF-to-FFPE Whole Slide Image Translation via Laplacian Pyramid and Contrastive Learning", "abstract": "Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF) are two major types of histopathological Whole Slide Images (WSIs). FFPE provides high-quality images, however the acquisition process usually takes 12 to 48 hours, while FF with relatively low-quality images takes less than 15 minutes to acquire. In this work, we focus on the task of translating FF to FFPE style (FF2FFPE), to synthesize FFPE-style images from FF images. However, WSIs with giga-pixels impose heavy constraints on computation and time resources. To address these issues, we propose the fastFF2FFPE for translating FF into FFPE-style efficiently. Specifically, we decompose FF images into low- and high-frequency components based on the Laplacian Pyramid, wherein the low-frequency component at low resolution is transformed into FFPE-style with low computational cost, and the high-frequency component is used for providing details. We further employ contrastive learning to encourage similarities between original and output patches. We conduct FF2FFPE translation experiments on The Cancer Genome Atlas (TCGA) Glioblastoma Multiforme (GBM) and Lung Squamous Cell Carcinoma (LUSC) datasets, and verify the efficacy of our model on Microsatellite Instability prediction in gastrointestinal cancer. The code and models are released at https://github.com/hellodfan/fastFF2FFPE.\n"}, {"title": "Fast Spherical Mapping of Cortical Surface Meshes using Deep Unsupervised Learning", "abstract": "Spherical mapping of cortical surface meshes provides a more convenient and accurate space for cortical surface registration and analysis and thus has been widely adopted in neuroimaging field. Conventional approaches typically first inflate and project the original cortical surface mesh onto a sphere to generate an initial spherical mesh which contains large distortions. Then they iteratively reshape the spherical mesh to minimize the metric (distance), area or angle distortions. However, these methods suffer from two major issues: 1) the iterative optimization process is computationally expensive, making them not suitable for large-scale data processing; 2) when metric distortion cannot be further minimized, either area or angle distortion is minimized at the expense of the other, which is not flexible to generate application-specific meshes based on both of them. To address these issues, for the first time, we propose a deep learning-based algorithm to learn the mapping between the original cortical surface and spherical surface meshes. Specifically, we take advantage of the Spherical U-Net model to learn the spherical diffeomorphic deformation field for minimizing the distortions between the icosahedron-reparameterized original surface and spherical surface meshes. The end-to-end unsupervised learning scheme is very flexible to incorporate various optimization objectives. We further integrate it into a coarse-to-fine multi-resolution framework for better correcting fine-scaled distortions. We have validated our method on 800+ cortical surfaces, demonstrating reduced distortions than FreeSurfer (the most popularly used tool), while speeding up the process from 20 minutes to 5 seconds.\n"}, {"title": "Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models", "abstract": "Deep generative models have emerged as promising tools for detecting arbitrary anomalies in data, dispensing with the necessity for manual labelling. Recently, autoregressive transformers have achieved state-of-the-art performance for anomaly detection in medical imaging. Nonetheless, these models still have some intrinsic weaknesses, such as requiring images to be modelled as 1D sequences, the accumulation of errors during the sampling process, and the significant inference times associated with transformers. Denoising diffusion probabilistic models are a class of non-autoregressive generative models recently shown to produce excellent samples in computer vision (surpassing Generative Adversarial Networks), and to achieve log-likelihoods that are competitive with transformers while having fast inference times. Diffusion models can be applied to the latent representations learnt by autoencoders, making them easily scalable and great candidates for application to high dimensional data, such as medical images. Here, we propose a method based on diffusion models to detect and segment anomalies in brain imaging. By training the models on healthy data and then exploring its diffusion and reverse steps across its Markov chain, we can identify anomalous areas in the latent space and hence identify anomalies in the pixel space. Our diffusion models achieve competitive performance compared with autoregressive approaches across a series of experiments with 2D CT and MRI data involving synthetic and real pathological lesions with much reduced inference times, making their usage clinically viable.\n"}, {"title": "Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification", "abstract": "Whole slide image (WSI) classification is a fundamental task for the diagnosis and treatment of diseases; but, curation of accurate labels is time-consuming and limits the application of fully-supervised methods. To address this, multiple instance learning (MIL) is a popular method that poses classification as a weakly supervised learning task with slide-level labels only. While current MIL methods apply variants of the attention mechanism to re-weight instance features with stronger models, scant attention is paid to the properties of the data distribution. In this work, we propose to re-calibrate the distribution of a WSI bag (instances) by using the statistics of the max-instance (critical) feature. We assume that in binary MIL, positive bags have larger feature magnitudes than negatives, thus we can enforce the model to maximize the discrepancy between bags with a metric feature loss that models positive bags as out-of-distribution. To achieve this, unlike existing MIL methods that use single-batch training modes, we propose balanced-batch sampling to effectively use the feature loss i.e., (+/-) bags simultaneously. Further, we employ a position encoding module (PEM) to model spatial/morphological information, and perform pooling by multi-head self-attention (PSMA) with a Transformer encoder. Experimental results on existing benchmark datasets show our approach is effective and improves over state-of-the-art MIL methods.\n"}, {"title": "Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer\u00e2\u0080\u0099s disease detection", "abstract": "Convolutional neural networks have enabled significant improvements in medical image-based diagnosis. It is, however, increasingly clear that these models are susceptible to performance degradation when facing spurious correlations and dataset shift, leading, e.g., to underperformance on underrepresented patient groups. In this paper, we compare two classification schemes on the ADNI MRI dataset: a simple logistic regression model using manually selected volumetric features, and a convolutional neural network trained on 3D MRI data. We assess the robustness of the trained models in the face of varying dataset splits, training set sex composition, and stage of disease. In contrast to earlier work in other imaging modalities, we do not observe a clear pattern of improved model performance for the majority group in the training dataset. Instead, while logistic regression is fully robust to dataset composition, we find that CNN performance is generally improved for both male and female subjects when including more female subjects in the training dataset. We hypothesize that this might be due to inherent differences in the pathology of the two sexes. Moreover, in our analysis, the logistic regression model outperforms the 3D CNN, emphasizing the utility of manual feature specification based on prior knowledge, and the need for more robust automatic feature selection.\n"}, {"title": "Federated Medical Image Analysis with Virtual Sample Synthesis", "abstract": "Hospitals and research institutions may not be willing to share their collected medical data due to privacy concerns, transmission cost, and the intrinsic value of the data. Federated medical image analysis is thus explored to obtain a global model without access to the images distributed on isolated clients. However, in real-world applications, the local data from each client are likely non-i.i.d distributed because of the variations in geographic factors, patient demographics, data collection process, and so on. Such heterogeneity in data poses severe challenges to the performance of federated learning. In this paper, we introduce federated medical image analysis with virtual sample synthesis (FedVSS). Our method can improve the generalization ability by adversarially synthesizing virtual training samples with the local models and also learn to align the local models by synthesizing high-confidence samples with regard to the global model. All synthesized data will be further utilized in local model updating. We conduct comprehensive experiments on five medical image datasets retrieved from MedMNIST and Camelyon17, and the experimental results validate the effectiveness of our method.\n"}, {"title": "Federated Stain Normalization for Computational Pathology", "abstract": "Although deep federated learning has received much attention\nin recent years, progress has been made mainly in the context of\nnatural images and barely for computational pathology. However, deep\nfederated learning is an opportunity to create datasets that reflect the\ndata diversity of many laboratories. Further, the effort of dataset construction\ncan be divided among many. Unfortunately, existing algorithms\ncannot be easily applied to computational pathology since previous work\npresupposes that data distributions of laboratories must be similar. This\nis an unlikely assumption, mainly since different laboratories have different\nstaining styles. As a solution, we propose BottleGAN, a generative\nmodel that can computationally align the staining styles of many\nlaboratories and can be trained in a privacy-preserving manner to foster\nfederated learning in computational pathology. We construct a heterogenic\nmulti-institutional dataset based on the PESO segmentation\ndataset and improve the IOU by 42% compared to existing federated\nlearning algorithms. An implementation of BottleGAN is available at\nhttps://github.com/MECLabTUDA/BottleGAN.\n"}, {"title": "FedHarmony: Unlearning Scanner Bias with Distributed Data", "abstract": "The ability to combine data across scanners and studies is vital for neuroimaging, to increase both statistical power and the representation of biological variability.  However, combining datasets across sites leads to two challenges: first, an increase in undesirable non-biological variance due to scanner and acquisition differences - the harmonisation problem - and second, data privacy concerns due to the inherently personal nature of medical imaging data, meaning that sharing them across sites may risk violation of privacy laws. To overcome these restrictions, we propose FedHarmony: a harmonisation framework operating in the federated learning paradigm. We show that to remove the scanner-specific effects, we only need to share the mean and standard deviation of the learned features, helping to protect individual subjects\u00e2\u0080\u0099 privacy. We demonstrate our approach across a range of realistic data scenarios, using real multi-site data from the ABIDE dataset, thus showing the potential utility of our method for MRI harmonisation across studies. Our code is available at https://github.com/nkdinsdale/FedHarmony.\n"}, {"title": "Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning", "abstract": "Clinical adoption of personalized virtual heart simulations faces challenges in model personalization and expensive computation. While an ideal solution is an efficient neural surrogate that at the same time is personalized to an individual subject, the state-of-the-art is either concerned with personalizing an expensive simulation model, or learning an efficient yet generic surrogate. This paper presents a completely new concept to achieve personalized neural surrogates in a single coherent framework of meta-learning (metaPNS). Instead of learning a single neural surrogate, we learn the process of learning a personalized neural surrogate using a small number of context data from a subject, in a novel formulation of few-shot generative modeling underpinned by: 1) a set-conditioned neural surrogate for cardiac simulation that, conditioned on subject-specific context data, learns to generate query simulations not included in the context set, and 2) a meta-model of amortized variational inference that learns to condition the neural surrogate via simple feed-forward embedding of context data. As test time, metaPNS delivers a personalized neural surrogate by fast feed-forward embedding of a small and flexible number of data available from an individual, achieving \u00e2\u0080\u0093 for the first time \u00e2\u0080\u0093 personalization and surrogate construction for expensive simulations in one end-to-end learning framework. Synthetic and real-data experiments demonstrated that metaPNS was able to improve personalization and predictive accuracy in comparison to conventionally-optimized cardiac simulation models, at a fraction of computation.\n"}, {"title": "Few-shot Medical Image Segmentation Regularized with Self-reference and Contrastive Learning", "abstract": "Despite the great progress made by deep convolutional neural networks (CNN) in medical image segmentation, they typically require a large amount of expert-level accurate, densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot learning has thus been proposed to address the challenges by learning to transfer knowledge from a few annotated support examples. In this paper, we propose a new prototype-based few-shot segmentation method. Unlike previous works, where query features are compared with the learned support prototypes to generate segmentation over the query images, we propose a self-reference regularization where we further compare support features with the learned support prototypes to generate segmentation over the support images. By this, we argue for that the learned support prototypes should be representative for each semantic class and meanwhile discriminative for different classes, not only for query images but also for support images. We additionally introduce contrastive learning to impose intra-class cohesion and inter-class separation between support and query features. Results from experiments conducted on two publicly available datasets demonstrated the superior performance of the proposed method over the state-of-the-art (SOTA).\n"}, {"title": "FFCNet: Fourier Transform-Based Frequency Learning and Complex Convolutional Network for Colon Disease Classification", "abstract": "Reliable automatic classification of colonoscopy images is of great significance in assessing the stage of colonic lesions and formulating appropriate treatment plans. However, it is challenging due to uneven brightness, location variability, inter-class similarity, and intra-class dissimilarity, affecting the classification accuracy. To address the above issues, we propose a Fourier-based Frequency Complex Network (FFCNet) for colon disease classification in this study. Specifically, FFCNet is a novel complex network that enables the combination of complex convolutional networks with frequency learning to overcome the loss of phase information caused by real convolution operations. Also, our Fourier transform transfers the average brightness of an image to a point in the spectrum (the DC component), alleviating the effects of uneven brightness by decoupling image content and brightness. Moreover, the image patch scrambling module in FFCNet generates random local spectral blocks, empowering the network to learn long-range and local disease-specific features and improving the discriminative ability of hard samples. We evaluated the proposed FFCNet on an in-house dataset with 2568 colonoscopy images, showing our method achieves high performance outperforming previous state-of-the-art methods with an accuracy of 86.35% and an accuracy of 4.46% higher than the backbone. The project page with code is available at https://github.com/soleilssss/FFCNet.\n"}, {"title": "Fine-grained Correlation Loss for Regression", "abstract": "Regression learning is classic and fundamental for medical image analysis. It provides the continuous mapping for many critical applications, like the attribute estimation, object detection, segmentation and non-rigid registration. However, previous studies mainly took the case-wise criteria, like the mean square errors, as the optimization objectives. They ignored the very important population-wise correlation criterion, which is exactly the final evaluation metric in many tasks. In this work, we propose to revisit the classic regression tasks with novel investigations on directly optimizing the fine-grained correlation losses. We mainly explore two complementary correlation indexes as learnable losses: Pearson linear correlation (PLC) and Spearman rank correlation (SRC). The contributions of this paper are two folds. First, for the PLC on global level, we propose a strategy to make it robust against the outliers and regularize the key distribution factors. These efforts significantly stabilize the learning and magnify the efficacy of PLC. Second, for the SRC on local level, we propose a coarse-to-fine scheme to ease the learning of the exact ranking order among samples. Specifically, we convert the learning for the ranking of samples into the learning of similarity relationships among samples. We extensively validate our method on two typical ultrasound image regression tasks, including the image quality assessment and bio-metric measurement. Experiments prove that, with the fine-grained guidance in directly optimizing the correlation, the regression performances are significantly improved. Our proposed correlation losses are general and can be extended to more important applications.\n"}, {"title": "Flat-aware Cross-stage Distilled Framework for Imbalanced Medical Image Classification", "abstract": "Medical data often follow imbalanced distributions, which poses a long-standing challenge for computer-aided diagnosis systems built upon medical image classification. Most existing efforts are conducted by applying re-balancing methods for the collected training samples, which improves the predictive performance for the minority class but at the cost of decreasing the performance for the majority. To address this paradox, we adopt a flat-aware cross-stage distilled framework (FCD), where we first search for flat local minima of the base training objective function on the original imbalanced dataset, and then continuously finetune this classifier within the flat region on the re-balanced one. To further prevent the performance decreasing for the majority, we propose a cross-stage distillation regularizing term to promote the optimized features to remain in the common optimal subspace. Extensive experiments on two imbalanced medical image datasets demonstrate the effectiveness of our proposed framework and its generality in improving the performance of existing imbalanced methods. The code of this work will be released publicly.\n"}, {"title": "Flexible Sampling for Long-tailed Skin Lesion Classification", "abstract": "Most of the medical tasks naturally exhibit a long-tailed distribution due to the complex patient-level conditions and the existence of rare diseases. Existing long-tailed learning methods usually treat each class equally to re-balance the long-tailed distribution. However, considering that some challenging classes may present diverse intra-class distributions, re-balancing all classes equally may lead to a significant performance drop. \nTo address this, in this paper, we propose a curriculum learning-based framework called Flexible Sampling for the long-tailed skin lesion classification task. \nSpecifically, we initially sample a subset of training data as anchor points based on the individual class prototypes. Then, these anchor points are used to pre-train an inference model to evaluate the per-class learning difficulty. Finally, we use a curriculum sampling module to dynamically query new samples from the rest training samples with the learning difficulty-aware sampling probability. We evaluated our model against several state-of-the-art methods on the ISIC dataset. The results with two long-tailed settings have demonstrated the superiority of our proposed training strategy, which achieves a new benchmark for long-tailed skin lesion classification.\n"}, {"title": "fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits", "abstract": "We obtain a personal signature of a person\u00e2\u0080\u0099s learning progress in a self-neuromodulation\ntask, guided by functional MRI (fMRI). The signature is based on predicting the activity of\nthe Amygdala in a second neurofeedback session, given a similar fMRI-derived brain state\nin the first session. The prediction is made by a deep neural network, which is trained on\nthe entire training cohort of patients. This signal, which is indicative of a person\u00e2\u0080\u0099s progress\nin performing the task of Amygdala modulation, is aggregated across multiple prototypical\nbrain states and then classified by a linear classifier to various personal and clinical indications. The predictive power of the obtained signature is stronger than previous approaches\nfor obtaining a personal signature from fMRI neurofeedback and provides an indication\nthat a person\u00e2\u0080\u0099s learning pattern may be used as a diagnostic tool.\n"}, {"title": "Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions", "abstract": "Self-supervised learning has witnessed great progress in vision and NLP; recently, it also attracted much attention to various medical imaging modalities such as X-ray, CT, and MRI. Existing methods mostly focus on building new pretext self-supervision tasks such as reconstruction, orientation, and masking identification according to the properties of medical images. However, the publicly available self-supervision models are not fully exploited. In this paper, we present a powerful yet efficient self-supervision framework for surgical video understanding. Our key insight is to distill knowledge from publicly available models trained on large generic datasets to facilitate the self-supervised learning of surgical videos.To this end, we first introduce a semantic-preserving training scheme to obtain our teacher model, which not only contains semantics from the publicly available models, but also can produce accurate knowledge for surgical data. Besides training with only contrastive learning, we also introduce a distillation objective to transfer the rich learned information from the teacher model to self-supervised learning on surgical data.Extensive experiments on two surgical phase recognition benchmarks show that our framework can significantly improve the performance of existing self-supervised learning methods. Notably, our framework demonstrates a compelling advantage under a low-data regime."}, {"title": "Frequency-Aware Inverse-Consistent Deep Learning for OCT-Angiogram Super-Resolution", "abstract": "Optical Coherence Tomography Angiography (OCTA) is a novel imaging modality that captures the retinal and choroidal microvasculature in a non-invasive way. So far, 3mm\u00c3\u00973mm and 6mm\u00c3\u00976mm scanning protocols have been the two most widely-used field-of-views. Nevertheless, since both are acquired with the same number of A-scans, resolution of 6mm\u00c3\u00976mm image is inadequately sampled, compared with 3mm\u00c3\u00973mm. Moreover, conventional supervised super-resolution methods for OCTA images are trained with pixel-wise registered data, while clinical data is mostly unpaired. This paper proposes an inverse-consistent generative adversarial network (GAN) for archiving 6mm\u00c3\u00976mm OCTA images with super-resolution. Our method is designed to be trained with unpaired 3mm\u00c3\u00973mm and 6mm\u00c3\u00976mm OCTA image datasets. To further enhance the super-resolution performance, we introduce frequency transformations to refine high-frequency information while retaining low-frequency information. Compared with other state-of-the-art methods, our approach outperforms them on various performance metrics.\n"}, {"title": "From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach", "abstract": "Statistical shape modeling (SSM) directly from 3D medical images is an underutilized tool for detecting pathology, diagnosing disease, and conducting population-level morphology analysis. Deep learning frameworks have increased the feasibility of adopting SSM in medical practice by reducing the expert-driven manual and computational overhead in traditional SSM workflows. However, translating such frameworks to clinical practice requires calibrated uncertainty measures as neural networks can produce over-confident predictions that cannot be trusted in sensitive clinical decision-making. Existing techniques for predicting shape with aleatoric (data-dependent) uncertainty utilize a Principal Component Analysis (PCA) based shape representation computed in isolation of the model training. This constraint restricts the learning task to solely estimating pre-defined shape descriptors from 3D images and imposes a linear relationship between this shape representation and the output (i.e., shape) space. In this paper, we propose a principled framework based on the variational information bottleneck theory to relax these assumptions while predicting probabilistic shapes of anatomy directly from images without supervised encoding of shape descriptors. Here, the latent representation is learned in the context of the learning task, resulting in a more scalable, flexible model that better captures data non-linearity. Additionally, this model is self-regularized and generalizes better given limited training data. Our experiments demonstrate the proposed method provides an accuracy improvement and better calibrated aleatoric uncertainty estimates than current state-of-the-art models.\n"}, {"title": "FSE Compensated Motion Correction for MRI Using Data Driven Methods", "abstract": "Magnetic Resonance Imaging (MRI) is a widely used medical imaging modality boasting great soft tissue contrast without ionizing radiation, but unfortunately suffers from long acquisition times. Long scan times can lead to motion artifacts, for example due to bulk patient motion such as head movement and periodic motion produced by the heart or lungs. Motion artifacts can degrade image quality and in some cases render the scans nondiagnostic. \nTo combat this problem, prospective and retrospective motion correction techniques have been introduced. More recently, data driven methods using deep neural networks have been proposed. \nAs a large number of publicly available MRI datasets are based on Fast Spin Echo (FSE) sequences,  methods that use them for training should incorporate the correct FSE acquisition dynamics.\nUnfortunately, when simulating training data, many approaches fail to generate accurate motion-corrupt images by neglecting the effects of the temporal ordering of the k-space lines as well as neglecting the signal decay throughout the FSE echo train.\nIn this work, we highlight this consequence and demonstrate a training method which correctly simulates the data acquisition process of FSE sequences with higher fidelity by including sample ordering and signal decay dynamics. \nThrough numerical experiments, we show that accounting for the FSE acquisition leads to better motion correction performance during inference."}, {"title": "Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis", "abstract": "In a complex disease such as tuberculosis, the evidence for the disease and its evolution may be present in multiple modalities such as clinical, genomic, or imaging data. Effective patient-tailored outcome prediction and therapeutic guidance will require fusing evidence from these modalities. Such multimodal fusion is difficult since the evidence for the disease may not be uniform across all modalities, not all modality features may be relevant, or not all modalities may be present for all patients. All these nuances make simple methods of early, late, or intermediate fusion of features inadequate for outcome prediction. In this paper, we present a novel fusion framework using multiplexed graphs and derive a new graph neural network for learning from such graphs. Specifically, the framework allows modalities to be represented through their targeted encodings, and models their relationship explicitly via multiplexed graphs derived from salient features in a combined latent space. We present results that show that our proposed method outperforms state-of-the-art methods of fusing modalities for multi-outcome prediction on a large Tuberculosis (TB) dataset. \n"}, {"title": "FUSSNet: Fusing Two Sources of Uncertainty for Semi-Supervised Medical Image Segmentation", "abstract": "In recent years, various semi-supervised learning (SSL) methods have been developed to deal with the scarcity of labeled data in medical image segmentation. Especially, many of them focus on the uncertainty caused by a lack of knowledge (about the best model), i.e. epistemic uncertainty (EU). Besides EU, another type of uncertainty, aleatoric uncertainty (AU), originated from irreducible errors or noise, also commonly exists in medical imaging data. While previous SSL approaches focus on only one of them (mostly EU), this study shows that SSL segmentation models can benefit more by considering both the two sources of uncertainty. The proposed FUSSNet framework is featured by a joint learning scheme, which combines the EU-guided unsupervised learning and AU-guided supervised learning. We assess the method on two benchmark datasets for the segmentation of left atrium and pancreas, respectively. The experimental results show that FUSSNet outperforms the state-of-the-art semi-supervised segmentation methods by large margins.\n"}, {"title": "GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation", "abstract": "Parkinson\u00e2\u0080\u0099s disease (PD) is a neurological disorder that has a variety of observable motor-related symptoms such as slow movement, tremor, muscular rigidity, and impaired posture. PD is typically diagnosed by evaluating the severity of motor impairments according to scoring systems such as the Movement Disorder Society Unified Parkinson\u00e2\u0080\u0099s Disease Rating Scale (MDS-UPDRS). Automated severity prediction using video recordings of individuals provides a promising route for non-intrusive monitoring of motor impairments. However, the limited size of PD gait data hinders model ability and clinical potential. Because of this clinical data scarcity and inspired by the recent advances in self-supervised large-scale language models like GPT-3, we use human motion forecasting as an effective self-supervised pre-training task for the estimation of motor impairment severity. We introduce GaitForeMer, Gait Forecasting and impairment estimation transforMer, which is first pre-trained on public datasets to forecast gait movements and then applied to clinical data to predict MDS-UPDRS gait impairment severity. Our method outperforms previous approaches that rely solely on clinical data by a large margin, achieving an F1 score of 0.76, precision of 0.79, and recall of 0.75. Using GaitForeMer, we show how public human movement data repositories can assist clinical use cases through learning universal motion representations. The code is available at https://github.com/markendo/GaitForeMer.\n"}, {"title": "GazeRadar: A Gaze and Radiomics-guided Disease Localization Framework", "abstract": "We present GazeRadar, a novel radiomics and eye gaze-guided deep learning architecture for disease localization in chest radiographs. GazeRadar combines the representation of radiologists\u00e2\u0080\u0099 visual search patterns with corresponding radiomic signatures into an integrated radiomics-visual attention representation for downstream disease localization and classification tasks. Radiologists generally tend to focus on fine-grained disease features, while the radiomics features provide high-level textural information. Our framework first \u00e2\u0080\u0098fuses\u00e2\u0080\u0099 radiomics features with visual features inside a teacher block. The visual features are learned through a teacher-focal block, while the radiomics features are learned through a teacher-global block. Next, a novel Radiomics-Visual Attention Feature loss is proposed to leverage this joint radiomics-visual representation of the teacher network to the student network. We demonstrate the efficacy of GazeRadar on disease localization and classification tasks in 4 large scale chest radiograph datasets comprising multiple diseases.\n"}, {"title": "Geometric Constraints for Self-supervised Monocular Depth Estimation on Laparoscopic Images with Dual-task Consistency", "abstract": "Depth values are essential information to automate surgical robots and achieve Augmented Reality technology for minimally invasive surgery. Although depth-pose self-supervised monocular depth estimation performs impressively for autonomous driving scenarios, it is more challenging to predict accurate depth values for laparoscopic images due to the following two aspects: (i) the laparoscope\u00e2\u0080\u0099s motions contain many rotations, leading to pose estimation difficulties for the depth-pose learning strategy; (ii) the smooth surface reduces photometric error even if the matching pixels are inaccurate between adjacent frames. This paper proposes a novel self-supervised monocular depth estimation for laparoscopic images with geometric constraints. We predict the scene coordinates as an auxiliary task and construct dual-task consistency between the predicted depth maps and scene coordinates under a unified camera coordinate system to achieve pixel-level geometric constraints. We extend the pose estimation into a Siamese process to provide stronger and more balanced geometric constraints in a depth-pose learning strategy by leveraging the order of the adjacent frames in a video sequence. We also design a weight mask for depth estimation based on our consistency to alleviate the interference from predictions with low confidence. The experimental results showed that the proposed method outperformed the baseline on depth and pose estimation. Our code is available at https://github.com/MoriLabNU/GCDepthL.\n"}, {"title": "Gigapixel Whole-Slide Images Classification using Locally Supervised Learning", "abstract": "Histopathology whole slide images (WSIs)  play a very important role in clinical studies and serve as the gold standard for many cancer diagnoses. However, generating automatic tools for processing WSIs is challenging due to their enormous sizes. Currently, to deal with this issue, conventional methods rely on a multiple instance learning (MIL) strategy to process a WSI at patch level. Although effective, such methods are computationally expensive, because tiling a WSI into patches takes time and does not explore the spatial relations between these tiles.To tackle these limitations, we propose a locally supervised learning framework which processes the entire slide by exploring the entire local and global information that it contains. This framework divides a pre-trained network into several modules and optimizes each module locally using an auxiliary model. We also introduce a random feature reconstruction unit (RFR) to preserve distinguishing features during training and improve the performance of our method by 1% to 3%. Extensive experiments on three publicly available WSI datasets: TCGA-NSCLC, TCGA-RCC and LKS, highlight the superiority of our method on different classification tasks. Our method outperforms the state-of-the-art MIL methods by 2% to 5% in accuracy, while being 7 to 10 times faster. Additionally, when dividing it into eight modules, our method requires as little as 20% of the total gpu memory required by end-to-end training. Our code is available at https://github.com/cvlab-stonybrook/local_learning_wsi.\n"}, {"title": "Global Multi-modal 2D/3D Registration via Local Descriptors Learning", "abstract": "Multi-modal registration is a required step for many image-guided procedures, especially ultrasound-guided interventions that require anatomical context.\nWhile a number of such registration algorithms are already available, they all require a good initialization to succeed due to the challenging appearance of ultrasound images and the arbitrary coordinate system they are acquired in.\nIn this paper, we present a novel approach to solve the problem of registration of an ultrasound sweep to a pre-operative image. We learn dense keypoint descriptors from which we then estimate the registration. We show that our method overcomes the challenges inherent to registration tasks with freehand ultrasound sweeps, namely, the multi-modality and multi-dimensionality of the data in addition to lack of precise ground truth and low amounts of training examples. We derive a registration method that is fast, generic, fully automatic, does not require any initialization and can naturally generate visualizations aiding interpretability and explainability.  Our approach is evaluated on a clinical dataset of paired MR volumes and ultrasound sequences.\n"}, {"title": "GradMix for nuclei segmentation and classification in imbalanced pathology image datasets", "abstract": "An automated segmentation and classification of nuclei is an essential task in digital pathology. The current deep learning-based approaches require a vast amount of annotated datasets by pathologists. However, the existing datasets are imbalanced among different types of nuclei in general, leading to a substantial performance degradation. In this paper, we propose a simple but effective data augmentation technique, termed GradMix, that is specifically designed for nuclei segmentation and classification. GradMix takes a pair of a major-class nucleus and a rare-class nucleus, creates a customized mixing mask, and combines them using the mask to generate a new rare-class nucleus. As it combines two nuclei, GradMix considers both nuclei and the neighboring environment by using the customized mixing mask. This allows us to generate realistic rare-class nuclei with varying environments. We employed two datasets to evaluate the effectiveness of GradMix. The experimental results suggest that GradMix is able to improve the performance of nuclei segmentation and classification in imbalanced pathology image datasets.\n"}, {"title": "Graph convolutional network with probabilistic spatial regression: application to craniofacial landmark detection from 3D photogrammetry", "abstract": "Quantitative evaluation of pediatric craniofacial anomalies relies on the accurate identification of anatomical landmarks and structures. While segmentation and landmark detection methods in standard clinical images are available in the literature, image-based methods are not directly applicable to 3D photogrammetry because of its unstructured nature consisting in variable numbers of vertices and polygons. In this work, we propose a graph-based convolutional neural network based on Chebyshev polynomials that exploits vertex coordinates, polygonal connectivity, and surface normal vectors to extract multi-resolution spatial features from the 3D photographs. We then aggregate them using a novel weighting scheme that accounts for local spatial resolution variability in the data. We also propose a new trainable regression scheme based on the probabilistic distances between each original vertex and the anatomical landmarks to calculate coordinates from the aggregated spatial features. This approach allows calculating accurate landmark coordinates without assuming correspondences with specific vertices in the original mesh. Our method achieved state-of-the-art landmark detection errors.\n"}, {"title": "Graph Emotion Decoding from Visually Evoked Neural Responses", "abstract": "Brain signal-based affective computing has recently drawn considerable attention due to its potential widespread applications. Most existing efforts exploit emotion similarities or brain region similarities to learn emotion representations. However, the relationships between emotions and brain regions are not explicitly incorporated into the representation learning process. Consequently, the learned representations may not be informative enough to benefit downstream tasks, e.g., emotion decoding. In this work, we propose a novel neural decoding framework, Graph Emotion Decoding (GED), which integrates the relationships between emotions and brain regions via a bipartite graph structure into the neural decoding process. Further analysis shows that exploiting such relationships helps learn better representations, verifying the rationality and effectiveness of GED. Comprehensive experiments on visually evoked emotion datasets demonstrate the superiority of our model.\n"}, {"title": "Graph-based Compression of Incomplete 3D Photoacoustic Data", "abstract": "Photoacoustic imaging (PAI) is a newly emerging bimodal imaging technology based on the photoacoustic effect; specifically, it uses sound waves caused by light absorption in a material to obtain 3D structure data noninvasively. PAI has attracted attention as a promising measurement technology for comprehensive clinical application and medical diagnosis. Because it requires exhaustively scanning an entire object and recording ultrasonic waves from various locations, it encounters two problems: a long imaging time and a huge data size. To reduce the imaging time, a common solution is to apply compressive sensing (CS) theory. CS can effectively accelerate the imaging process by reducing the number of measurements, but the data size is still large, and efficient compression of such incomplete data remains a problem. In this paper, we present the first attempt at direct compression of incomplete 3D PA observations, which simultaneously reduces the data acquisition time and alleviates the data storage issue. Specifically, we first use a graph model to represent the incomplete observations. Then, we propose three coding modes and a reliability-aware rate-distortion optimization (RDO) to adaptively compress the data into sparse coefficients. Finally, we obtain a coded bit stream through entropy coding. We demonstrate the effectiveness of our proposed framework through both objective evaluation and subjective visual checking of real medical PA data captured from patients.\n"}, {"title": "Greedy Optimization of Electrode Arrangement for Epiretinal Prostheses", "abstract": "Visual neuroprostheses are the only FDA-approved technology for the treatment of retinal degenerative blindness. Although recent work has demonstrated a systematic relationship between electrode location and the shape of the elicited visual percept, this knowledge has yet to be incorporated into retinal prosthesis design, where electrodes are typically arranged on either a rectangular or hexagonal grid. Here we optimize the intraocular placement of epiretinal electrodes using dictionary learning. Importantly, the optimization process is informed by a previously established and psychophysically validated model of simulated prosthetic vision. We systematically evaluate three different electrode placement strategies across a wide range of possible phosphene shapes and recommend electrode arrangements that maximize visual subfield coverage. In the near future, our work may guide the prototyping of next-generation neuroprostheses.\n"}, {"title": "Hand Hygiene Quality Assessment using Image-to-Image Translation", "abstract": "Hand hygiene can reduce the transmission of pathogens and prevent healthcare-associated infections. Ultraviolet (UV) test is an effective tool for evaluating and visualizing hand hygiene quality during medical training. However, due to various hand shapes, sizes, and positions, systematic documentation of the UV test results to summarize frequently untreated areas and validate hand hygiene technique effectiveness is challenging. Previous studies often summarize errors within predefined hand regions, but this only provides low-resolution estimations of hand hygiene quality. Alternatively, previous studies manually translate errors to hand templates, but this lacks standardized observational practices. In this paper, we propose a novel automatic image-to-image translation framework to evaluate hand hygiene quality and document the results in a standardized manner. The framework consists of two models, including an Attention U-Net model to segment hands from the background and simultaneously classify skin surfaces covered with hand disinfectants, and a U-Net-based generator to translate the segmented hands to hand templates. Moreover, due to the lack of publicly available datasets, we conducted a lab study to collect 1218 valid UV test images containing different skin coverage with hand disinfectants. The proposed framework was then evaluated on the collected dataset through five-fold cross-validation. Experimental results show that the proposed framework can accurately assess hand hygiene quality and document UV test results in a standardized manner. The benefit of our work is that it enables systematic documentation of hand hygiene practices, which in turn enables clearer communication and comparisons.\n"}, {"title": "Harnessing Deep Bladder Tumor Segmentation with Logical Clinical Knowledge", "abstract": "Segmentation of bladder tumors from Magnetic Resonance (MR) images is important for early detection and auxiliary diagnosis of bladder cancer. Deep Convolutional Neural Networks (DCNNs) have been widely used for bladder tumor segmentation but the DCNN-based tumor segmentation over-depends on data training and neglects the clinical knowledge. From a clinical point of view, a bladder tumor must rely on the bladder wall to survive and grow, and the domain prior is very helpful for bladder tumor localization. Aiming at the problem, we propose a novel bladder tumor segmentation method in which the clinical logic rules of bladder tumor and wall are incorporated into DCNNs and make the segmentation of DCNN harnessed by the clinical rules. The logic rules provide a semantic and friendly knowledge representation for human clinicians, which are easy to set and understand. Moreover, fusing the logic rules of clinical knowledge facilitates to reduce the data dependency of the segmentation network and achieve precise segmentation results even with limited labeled training images. Experiments on the bladder MR images from the cooperative hospital validate the effectiveness of the proposed tumor segmentation method.\n"}, {"title": "Hierarchical Brain Networks Decomposition via Prior Knowledge Guided Deep Belief Network", "abstract": "Task-based functional magnetic resonance imaging (fMRI) has been widely used for functional brain network identification. Recently, deep belief network (DBN) has shown great advantages in modeling the hierarchical and complex task functional brain networks (FBNs). However, due to the unsupervised nature, traditional DBN algorithms may be limited in fully utilizing the prior knowledge from the task design. In addition, the FBNs extracted from different DBN layers do not have correspondences, which makes the hierarchical analysis of FBNs a challenging problem. In this paper, we propose a novel prior knowledge guided DBN (PKG-DBN) to overcome the above limitations when conducting hierarchical task FBNs analysis. Specifically, we enforce part of the time courses learnt from DBN to be task-related (in either positive or negative way) and the rest to be linear combinations of task-related components. By incorporating such constraints in the learning process, our method can simultaneously leverage the advantages of data-driven approaches and the prior knowledge of task design. Our experiment results on HCP task fMRI data showed that the proposed PKG-DBN can not only successfully identify meaningful hierarchical task FBNs with correspondence comparing to traditional DBN models, but also converge significantly faster than traditional DBN models.\n"}, {"title": "Histogram-based unsupervised domain adaptation for medical image classification", "abstract": "Domain shift is a common problem in machine learning and\nmedical imaging. Currently one of the most popular domain adaptation\napproaches is the domain-invariant mapping method using generative\nadversarial networks (GANs). These methods deploy some variation of\na GAN to learn target domain distributions which work on pixel level.\nHowever, they often produce too complicated or unnecessary transformations. This paper is based on the hypothesis that most domain shifts\nin medical images are variations of global intensity changes which can\nbe captured by transforming histograms along with individual pixel intensities. We propose a histogram-based GAN methodology for domain\nadaptation that outperforms standard pixel-based GAN methods in classifying chest x-rays from various heterogeneous target domains.\n"}, {"title": "How Much to Aggregate: Learning Adaptive Node-wise Scales on Graphs for Brain Networks", "abstract": "Brain connectomes are heavily studied to characterize early symptoms of various neurodegenerative diseases such as Alzheimer\u00e2\u0080\u0099s Disease (AD). As the connectomes over different brain regions are naturally represented as a graph, variants of Graph Neural Networks (GNNs) have been developed to identify topological patterns for disease early diagnosis. However, existing GNNs heavily rely on the fixed local structure given by an initial graph as they aggregate information from a direct neighborhood of each node. Such an approach overlooks useful information from further nodes, and multiple layers for node aggregations have to be stacked across the entire graph which leads to an over-smoothing issue. In this regard, we propose a flexible model that learns adaptive scales of neighborhood for individual nodes of a graph to incorporate broader information from appropriate range. Leveraging an adaptive diffusion kernel, the proposed model identifies desirable scales for each node for feature aggregation, which leads to better prediction of diagnostic labels of brain networks. Empirical results show that our method outperforms well-structured baselines on Alzheimer\u00e2\u0080\u0099s Disease Neuroimaging Initiative (ADNI) study for classifying various stages towards AD based on the brain connectome and relevant node-wise features from neuroimages.\n"}, {"title": "Hybrid Graph Transformer for Tissue Microstructure Estimation with Undersampled Diffusion MRI Data", "abstract": "Advanced contemporary di\u00ef\u00ac\u0080usion models for tissue microstructure often require di\u00ef\u00ac\u0080usion MRI (DMRI) data with su\u00ef\u00ac\u0083ciently dense sampling in the di\u00ef\u00ac\u0080usion wavevector space for reliable model \u00ef\u00ac\u0081tting, which might not always be feasible in practice. A potential remedy to this problem is by using deep learning techniques to predict high-quality di\u00ef\u00ac\u0080usion microstructural indices from sparsely sampled data. However, existing methods are either agnostic to the data geometry in the di\u00ef\u00ac\u0080usion wavevector space (q-space) or limited to leveraging information from only local neighborhoods in the physical coordinate space (x-space). Here, we propose a hybrid graph transformer (HGT) to explicitly consider the q-space geometric structure with a graph neural network (GNN) and make full use of spatial information with a novel residual dense transformer (RDT). The RDT consists of multiple densely connected transformer layers and a residual connection to facilitate model training. Extensive experiments on the data from the Human Connectome Project (HCP) demonstrate that our method signi\u00ef\u00ac\u0081cantly improves the quality of microstructural estimations over existing state-of-the-art methods.\n"}, {"title": "Hybrid Spatio-Temporal Transformer Network for Predicting Ischemic Stroke Lesion Outcomes from 4D CT Perfusion Imaging", "abstract": "Predicting the follow-up infarct lesion from baseline spatio-temporal (4D) Computed Tomography Perfusion (CTP) imaging is essential for the diagnosis and management of acute ischemic stroke (AIS) patients. However, due to their noisy appearance and high dimensionality, it has been technically challenging to directly use 4D CTP images for this task. Thus, CTP datasets are usually post-processed to generate parameter maps that describe the perfusion situation. Existing deep learning-based methods mainly utilize these maps to make lesion outcome predictions, which may only provide a limited understanding of the spatio-temporal details available in the raw 4D CTP. While a few efforts have been made to incorporate raw 4D CTP data, a more effective spatio-temporal integration strategy is still needed. Inspired by the success of Transformer models in medical image analysis, this paper presents a novel hybrid CNN-Transformer framework that directly maps 4D CTP datasets to stroke lesion outcome predictions. This hybrid prediction strategy enables an efficient modeling of spatio-temporal information, eliminating the need for post-processing steps and hence increasing the robustness of the method. Experiments on a multicenter CTP dataset of 45 AIS patients demonstrate the superiority of the proposed method over the state-of-the-art. Code is available on GitHub.\n"}, {"title": "Ideal Midsagittal Plane Detection using Deep Hough Plane Network for Brain Surgical Planning", "abstract": "The ideal midsagittal plane (MSP) approximately bisects the human brain into two cerebral hemispheres, and its projection on the cranial surface serves as an important guideline for surgical navigation, which lays a foundation for its significant role in assisting neurosurgeons in planning surgical incisions during preoperative planning. However, the existing plane detection algorithms are generally based on iteration procedure, which have the disadvantages of low efficiency, poor accuracy, and unable to extract the non-local plane features. In this study, we propose an end-to-end deep Hough plane network (DHPN) for ideal MSP detection, which has four highlights. First, we introduce differentiable deep Hough transform (DHT) and inverse deep Hough transform (IDHT) to achieve the mutual transformation between semantic features and Hough features, which converts and simplifies the plane detection problem in the image space into a keypoint detection problem in the Hough space. Second, we design a sparse DHT strategy to increase the sparsity of features, improving inference speed and greatly reducing calculation cost in the voting process. Third, we propose a Hough pyramid attention network (HPAN) to further extract non-local features by aggregating Hough attention modules (HAM). Fourth, we introduce dual space supervision (DSS) mechanism to integrate training loss from both image and Hough spaces. Through extensive validations on a large in-house dataset, our method outperforms state-of-the-art methods on the ideal MSP detection task.\n"}, {"title": "Identification of vascular cognitive impairment in adult moyamoya disease via integrated graph convolutional network", "abstract": "As one of the common complications, vascular cognitive impairment (VCI) comprises a range of cognitive disorders related to cerebral vessel diseases like moyamoya disease (MMD), and it is reversible by surgical revascularization in its early stage. However, diagnosis of VCI is time-consuming and less accurate if it solely relies on neuropsychological examination. Even if some existing research connected VCI with medical image, most of them were solely statistical methods with single modality. Therefore, we propose a graph-based framework to integrate both dual-modal imaging information (rs-fMRI and DTI) and non-imaging information to identify VCI in adult MMDs. Unlike some previous studies based on node-level classification, the proposed graph-level model can fully utilize imaging information and improve interpretability of results. Specifically, we firstly design two different graphs for each subject based on characteristics of different modalities and feed them to a dual-modal graph convolution network to extract complementary imaging features and select important brain biomarkers for each subject. Node-based normalization and constraint item are further devised to weakening influence of over-smoothing and natural difference caused by non-imaging information. Experiments on a real dataset not only achieve accuracy of 80.0%, but also highlight some salient brain regions related to VCI in adult MMDs, demonstrating the effectiveness and clinical interpretability of our proposed method.\n"}, {"title": "Identify Consistent Imaging Genomic Biomarkers for Characterizing the Survival-associated Interactions between Tumor-infiltrating Lymphocytes and Tumors", "abstract": "The tumor-infiltrating lymphocytes(TILs) and its correlation with tumors play a critical role in the development and progression of breast cancer. Existing studies have demonstrated that the combination of the whole-slide pathological images (WSIs) and genomic data can better characterize the immunological mechanisms of TILs and assess the prognostic outcome in breast cancer. However, it is still very challenging to characterize the intersections between TILs and tumors in WSIs because of their large size and heterogeneity patterns, and the high dimensional genomic data also brings difficulty for the integrative analysis with WSIs data. To address the above challenges, in this paper, we propose an interpretable multi-modal fusion framework, IMGFN, that can fuse the interaction information between TILs and tumors with the genomic data via an attention mechanism for prognosis predictions of breast cancer. Specifically, for WSIs data, we use the graph attention network (i.e., GAT)  to describe the spatial interactions of TILs and tumor regions across WSIs. As to genomic data, we use co-expression network analysis algorithms to cluster genes into co-expressed modules followed by applying the Concrete Autoencoders to select survival-associated modules. Finally, a self-attention layer is adopted to combine both the imaging and genomic features for the prognosis prediction of breast cancer. The experimental results on The Cancer Genome Atlas(TCGA) dataset suggest that the proposed IMGFN can not only achieve better prognosis results than the comparing methods but also identify consistent survival-associated imaging and genomic biomarkers correlated strongly with the interaction between TILs and Tumors. \n"}, {"title": "Identifying and Combating Bias in Segmentation Networks by leveraging multiple resolutions", "abstract": "Exploration of bias has significant impact on the transparency and applicability of deep learning pipelines in medical settings, yet is so far woefully understudied.\nIn this paper, we consider two separate groups for which training data is only available at differing image resolutions. For group H, available images and labels are at the preferred high resolution while for group L only deprecated lower resolution data exist. We analyse how this resolution-bias in the data distribution propagates to systematically biased predictions for group L at higher resolutions. Our results demonstrate that single-resolution training settings result in significant loss of volumetric group differences that translate to erroneous segmentations as measured by DSC and subsequent classification failures on the low resolution group. We further explore how training data across resolutions can be used to combat this systematic bias. Specifically, we investigate the effect of image resampling, scale augmentation and resolution independence and demonstrate that biases can effectively be reduced with multi-resolution approaches.\n"}, {"title": "Identifying Phenotypic Concepts Discriminating Molecular Breast Cancer Sub-Types", "abstract": "Molecular breast cancer sub-types derived from core-biopsy are central for individual outcome prediction and treatment decisions. Determining sub-types by non-invasive imaging procedures would benefit early assessment. Furthermore, identifying phenotypic traits of sub-types may inform our understanding of disease processes as we become able to monitor them longitudinally. We propose a model to learn phenotypic appearance concepts of four molecular sub-types of breast cancer. A deep neural network classification model predicts sub-types from multi-modal, multi-parametric imaging data. Intermediate representations of the visual information are clustered, and clusters are scored based on testing with concept activation vectors to assess their contribution to correctly discriminating sub-types. The proposed model can predict sub-types with competitive accuracy from simultaneous ${}^{18}$F-FDG PET/MRI, and identifies visual traits in the form of shared and discriminative phenotypic concepts associated with the sub-types. \n"}, {"title": "Implicit Neural Representations for Generative Modeling of Living Cell Shapes", "abstract": "Methods allowing the synthesis of realistic cell shapes could help generate training data sets to improve cell tracking and segmentation in biomedical images. Deep generative models for cell shape synthesis require a light-weight and flexible representation of the cell shape. However, commonly used voxel-based representations are unsuitable for high-resolution shape synthesis, and polygon meshes have limitations when modeling topology changes such as cell growth or mitosis. In this work, we propose to use level sets of signed distance functions (SDFs) to represent cell shapes. We optimize a neural network as an implicit neural representation of the SDF value at any point in a 3D+time domain. The model is conditioned on a latent code, thus allowing the synthesis of new and unseen shape sequences. We validate our approach quantitatively and qualitatively on C. elegans cells that grow and divide, and lung cancer cells with growing complex filopodial protrusions. Our results show that shape descriptors of synthetic cells resemble those of real cells, and that our model is able to generate topologically plausible sequences of complex cell shapes in 3D+time. \n"}, {"title": "Implicit Neural Representations for Medical Imaging Segmentation", "abstract": "3D signals in medical imaging, such as CT scans, are usually parameterized as a discrete grid of voxels. For instance, existing state-of-the-art organ segmentation methods learn discrete segmentation maps. Unfortunately, the memory requirements of such methods grow cubically with increasing spatial resolution, which makes them unsuitable for processing high resolution scans. To overcome this, we design an Implicit Organ Segmentation Network (IOSNet) that utilizes continuous Implicit Neural Representations and has several useful properties. Firstly, the IOSNet decoder memory is roughly constant and independent of the spatial resolution since it parameterizes the segmentation map as a continuous function. Secondly, IOSNet converges much faster than discrete voxel based methods due to its ability to accurately segment organs irrespective of organ sizes, thereby alleviating size imbalance issues without requiring any auxiliary tricks. Thirdly, IOSNet naturally supports super-resolution (i.e. sampling at arbitrary resolutions during inference) due to its continuous learnt representations. Moreover, despite using a simple lightweight decoder, IOSNet consistently outperforms the discrete specialized segmentation architecture UNet. Hence, our approach demonstrates that Implicit Neural Representations are well-suited for medical imaging applications, especially for processing high-resolution 3D medical scans. As open science, we make our code publicly available to aid future research in this area: https://github.com/osamakhaan/iosnet.\n"}, {"title": "Improved Domain Generalization for Cell Detection in Histopathology Images via Test-Time Stain Augmentation", "abstract": "Automated cell detection in histopathology images can provide a valuable tool for cancer diagnosis and prognosis, and cell detectors based on deep learning have achieved promising detection performance. However, the stain color variation of histopathology images acquired at different sites can deteriorate the performance of cell detection, where a cell detector trained on a source dataset may not perform well on a different target dataset. Existing methods that address this domain generalization problem perform stain normalization or augmentation during network training. However, such stain transformation performed during network training may still not be optimally representative of the test images from the target domain. Therefore, in this work, given a cell detector that may be trained with or without consideration of domain generalization, we seek to improve domain generalization for cell detection in histopathology images via test-time stain augmentation. Specifically, a histopathology image can be decomposed into the stain color matrix and stain density map, and we transform the test images by mixing their stain color with that of the source domain, so that the mixed images may better resemble the source images or their stain-transformed versions used for training. Since it is difficult to determine the optimal amount of the mixing, we choose to generate a number of transformed test images where the stain color mixing varies. The generated images are fed into the given detector, and the outputs are fused with a robust strategy that suppresses improper stain color mixing. The proposed method was validated on a publicly available dataset that comprises histopathology images acquired at different sites, and the results show that our method can effectively improve the generalization of cell detectors to new domains.\n"}, {"title": "Improving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets", "abstract": "The regulatory approval and broad clinical deployment of medical AI have been hampered by the perception that deep learning models fail in unpredictable and possibly catastrophic ways. A lack of statistically rigorous uncertainty quantification is a significant factor undermining trust in AI results. Recent developments in distribution-free uncertainty quantification present practical solutions for these issues by providing reliability guarantees for black-box models on arbitrary data distributions as formally valid finite-sample prediction intervals. Our work applies these new uncertainty quantification methods \u00e2\u0080\u0094 specifically conformal prediction \u00e2\u0080\u0094 to a deep-learning model for grading the severity of spinal stenosis in lumbar spine MRI. We demonstrate a technique for forming ordinal prediction sets that are guaranteed to contain the correct stenosis severity within a user-defined probability (confidence interval). On a dataset of 409 MRI exams processed by the deep-learning model, the conformal method provides tight coverage with small prediction set sizes. Furthermore, we explore the potential clinical applicability of flagging cases with high uncertainty predictions (large prediction sets) by quantifying an increase in the prevalence of significant imaging abnormalities (e.g. motion artifacts, metallic artifacts, and tumors) that could degrade confidence in predictive performance when compared to a random sample of cases.\n"}, {"title": "Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling", "abstract": "Supervised  learning  tasks  such  as  survival  prediction  that involve large whole slide images (WSIs) are a critical challenge in computational pathology that require modeling complex features of the tumor microenvironment. These learning tasks are often solved with deep multi-instance learning (MIL) models that do not explicitly capture intratumoral heterogeneity. We develop a novel variance pooling architecture that enables a MIL model to incorporate intratumoral heterogeneity into its predictions. Two interpretability tools based on \u00e2\u0080\u009crepresentative patches\u00e2\u0080\u009d are illustrated to probe the biological signals captured by these models. An empirical study with 4,479 gigapixel WSIs from the Cancer Genome Atlas shows that adding variance pooling into existing MIL frameworks improves survival prediction performance for five cancer types.\n"}, {"title": "INSightR-Net: Interpretable Neural Network for Regression using Similarity-based Comparisons to Prototypical Examples", "abstract": "Convolutional neural networks (CNNs) have shown exceptional performance for a range of medical imaging tasks. However, conventional CNNs are not able to explain their reasoning process, therefore limiting their adoption in clinical practice. In this work, we propose an inherently interpretable CNN for regression using similarity-based comparisons (INSightR-Net) and demonstrate our methods on the task of diabetic retinopathy grading. A prototype layer incorporated into the architecture enables visualization of the areas in the image that are most similar to learned prototypes. The final prediction is then intuitively modeled as a mean of prototype labels, weighted by the similarities. We achieved competitive prediction performance with our INSightR-Net compared to a ResNet baseline, showing that it is not necessary to compromise performance for interpretability. Furthermore, we quantified the quality of our explanations using sparsity and diversity, two concepts considered important for a good explanation, and demonstrated the effect of several parameters on the latent space embeddings\n"}, {"title": "InsMix: Towards Realistic Generative Data Augmentation for Nuclei Instance Segmentation", "abstract": "Nuclei Segmentation within histology images is a fundamental prerequisite in digital pathology workflow. However, deep-learning-based nuclei segmentation methods often suffer from limited annotations. This paper aims at a realistic data augmentation for nuclei segmentation, named InsMix, that follows a Copy-Smooth-Paste principle and performs morphology-constrained generative instance augmentation. Specifically, we propose morphology constraints that enable the augmented images to acquire luxuriant information about nuclei while maintaining their morphology characteristics (e.g., geometry, location). To fully exploit the pixel redundancy of the background, we further propose a background perturbation method, which randomly shuffles the background patches without disordering the original nuclei distribution. To achieve contextual consistency between original and template instances, a smooth-GAN is designed with a foreground similarity encoder (FSE) and a triplet loss. We validated the proposed method on two datasets, i.e., Kumar and CPS dataset. Comprehensive experimental results demonstrate the effectiveness of each component and the superior performance achieved by our method to the state-of-the-art methods. Codes will be made publicly available upon acceptance.\n"}, {"title": "Instrument-tissue Interaction Quintuple Detection in Surgery Videos", "abstract": "Instrument-tissue interaction detection in surgical videos is a fundamental problem for surgical scene understanding which is of great significance to computer-assisted surgical systems. However, few works focus on this fine-grained surgical activity representation. In this paper, we propose to represent instrument-tissue interaction as \u00e2\u009f\u00a8instrument bounding box,  tissue bounding box,  instrument class,  tissue class,  action class\u00e2\u009f\u00a9 quintuples. We present a novel quintuple detection network (QDNet) to address the instrument-tissue interaction quintuple detection task in cataract surgery videos. Specifically, a spatiotemporal attention layer (STAL) is proposed to aggregate spatial and temporal information of the regions of interest between adjacent frames. We also propose a graph-based quintuple prediction layer (GQPL) to reason the relationship between instruments and tissues. Our method achieves an mAP of 42.24% on a cataract surgery video dataset, significantly outperforming other methods.\n"}, {"title": "Interaction-Oriented Feature Decomposition for Medical Image Lesion Detection", "abstract": "Common lesion detection networks typically use lesion features for classification and localization. However, many lesions are classified only by lesion features without considering the relation with global context features, which raises the misclassification problem. In this paper, we propose an Interaction-Oriented Feature Decomposition (IOFD) network to improve the detection performance on context-dependent lesions. Specifically, we decompose features output from a backbone into global context features and lesion features that are optimized independently. Then, we design two novel modules to improve the lesion classification accuracy. A Global Context Embedding (GCE) module is designed to extract global context features. A Global Context Cross Attention (GCCA) module without additional parameters is designed to model the interaction between global context features and lesion features. Besides, considering the different features required by classification and localization tasks, we further adopt a task decoupling strategy. IOFD is easy to train and end-to-end in terms of training and inference. The experimental results for datasets in two modalities outperform state-of-the-art algorithms, which demonstrates the effectiveness and generality of IOFD. The source code is available at https://github.com/mklz-sjy/IOFD\n"}, {"title": "Interpretable differential diagnosis for Alzheimer\u00e2\u0080\u0099s disease and Frontotemporal dementia", "abstract": "Alzheimer\u00e2\u0080\u0099s disease and Frontotemporal dementia are two major types of dementia. Their accurate diagnosis and differentiation is crucial for determining specific intervention and treatment. However, differential diagnosis of these two types of dementia remains difficult at the early stage of disease due to similar patterns of clinical symptoms. Therefore, the automatic classification of multiple types of dementia has an important clinical value. So far, this challenge has not been actively explored. Recent development of deep learning in the field of medical image has demonstrated high performance for various classification tasks. In this paper, we propose to take advantage of two types of biomarkers: structure grading and structure atrophy. To this end, we propose first to train a large ensemble of 3D U-Nets to locally discriminate healthy versus dementia anatomical patterns. The result of these models is an interpretable 3D grading map capable of indicating abnormal brain regions. This map can also be exploited in various classification tasks using graph convolutional neural network. Finally, we propose to combine deep grading and atrophy-based classifications to improve dementia type discrimination. The proposed framework showed competitive performance compared to state-of-the-art methods for different tasks of disease detection and differential diagnosis.\n"}, {"title": "Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis", "abstract": "Human brains lie at the core of complex neurobiological systems, where the neurons, circuits, and subsystems interact in enigmatic ways. Understanding the structural and functional mechanisms of the brain has long been an intriguing pursuit for neuroscience research and clinical disorder therapy. Mapping the connections of the human brain as a network is one of the most pervasive paradigms in neuroscience. Graph Neural Networks (GNNs) have recently emerged as a potential method for modeling complex network data. Deep models, on the other hand, have low interpretability, which prevents their usage in decision-critical contexts like healthcare. To bridge this gap, we propose an interpretable framework to analyze disorder-specific Regions of Interest (ROIs) and prominent connections. The proposed framework consists of two modules: a brain-network-oriented backbone model for disease prediction and a globally shared explanation generator that highlights disorder-specific biomarkers including salient ROIs and important connections. We conduct experiments on three real-world datasets of brain disorders. The results verify that our framework can obtain outstanding performance and also identify meaningful biomarkers. All code for this work is available at https://github.com/HennyJie/IBGNN.git.\n"}, {"title": "Interpretable Modeling and Reduction of Unknown Errors in Mechanistic Operators", "abstract": "Prior knowledge about the imaging physics provides a mechanistic forward operator that plays an important role in image reconstruction, although myriad sources of possible errors in the operator could negatively impact the reconstruction solutions. In this work, we propose to embed the traditional  mechanistic forward operator inside a neural function, and focus on modeling and correcting its unknown errors in an interpretable manner. This is achieved by a conditional generative model that transforms a given mechanistic operator \nwith unknown errors, arising from a latent space of self-organizing clusters \nof potential sources of error generation. Once learned, the generative model can be used in place of a fixed forward operator in any traditional optimization-based reconstruction process where, together with the inverse solution, \nthe error in prior mechanistic forward operator can be minimized and the potential source of error uncovered. We apply the presented method to \nthe reconstruction of heart electrical potential from body surface potential. \nIn controlled simulation experiments and in-vivo real data experiments, we demonstrate that the presented method allowed reduction of errors in the physics-based forward operator and thereby delivered inverse reconstruction of heart-surface potential with increased accuracy.\nkeywords: Inverse imaging, Forward modeling, Physics-based.\n"}, {"title": "Interpretable signature of consciousness in resting-state functional network brain activity", "abstract": "A major challenge in medicine is the rehabilitation of brain-injured patients with poor neurological outcome who experience chronic impairment of consciousness, termed minimally conscious state or vegetative state. Resting-state functional Magnetic Resonance Imaging (rs-fMRI) holds the promise of easy-to-acquire and wide-spectrum biomarkers. Previous rs-fMRI studies in monkeys and humans have highlighted that different consciousness levels are characterized by the relative prevalence of different functional connectivity patterns - also referred to as brain states - which conform closely the underlying structural connectivity. Results suggest that changes in consciousness lead to changes in connectivity patterns, not just the co-activation strength between regions, but also at the level of entire networks.\nIn this work, a four-stage framework is proposed to identify interpretable spatial signature of consciousness, by i) defining brain regions of interest (ROIs) from atlases, ii) filtering and extracting  the time series associated with these ROIs, iii) recovering disjoint networks and associated connectivities, and iv) performing pairwise non-parametric tests between network activities grouped by acquisition conditions.\nOur approach yields tailored networks, spatially consistent and symmetric. They will be helpful to study spontaneous recovery from disorders of consciousness known to be accompanied by a functional restoration of several networks.\n"}, {"title": "Intervention & Interaction Federated Abnormality Detection with Noisy Clients", "abstract": "Federated learning (FL), which trains a shared global model by collaboration between distributed clients (e.g. medical institutions) and preserves the privacy of local data, has been widely deployed in the medical field to benefit abnormality diagnosis. However, it is inevitable that local data contains noise across clients, resulting in notably performance deterioration in the global model. To this end, a practical yet challenging FL problem is studied in this paper, namely Federated abnormality detection with noisy clients (FADN). We represent the first effort to reason the FADN task as a structural causal model, and identify the main issue that leads to the performance deterioration, namely \\textit{recognition bias}. To tackle the problem, an Intervention \\& Interaction FL framework (FedInI) is proposed, comprising two key strategies: (1) Intervention: considering the data distribution heterogeneity caused by different noisy levels within each client, we use the global model to intervene the training of local models, by shuffling and mixing features extracted from different models and suppress the noise gradually; (2) Interaction: we devise an adaptive sample-wise weighting strategy that jointly considers the local training statuses and global noisy levels with a shared interactive layer. Extensive experiments on class-conditional noise and instance-dependant noise settings are conducted, FedInI outperforms state-of-the-arts by a remarkable margin. \n"}, {"title": "Intra-class Contrastive Learning Improves Computer Aided Diagnosis of Breast Cancer in Mammography", "abstract": "Radiologists consider fine-grained characteristics of mammograms as well as patient-specific information before making the final diagnosis.\nRecent literature suggests that a similar strategy works for Computer Aided Diagnosis (CAD) models; multi-task learning with radiological and patient features as auxiliary classification tasks improves the model performance in breast cancer detection. \nUnfortunately, the additional labels that these learning paradigms require, such as patient age, breast density, and lesion type, are often unavailable due to privacy restrictions and annotation costs.\nIn this paper, we introduce a contrastive learning framework comprising a Lesion Contrastive Loss (LCL) and a Normal Contrastive Loss (NCL), which jointly encourage models to learn subtle variations beyond class labels in a self-supervised manner.\nThe proposed loss functions effectively utilize the multi-view property of mammograms to sample contrastive image pairs.\nUnlike previous multi-task learning approaches, our method improves cancer detection performance without additional annotations.\nExperimental results further demonstrate that the proposed losses produce discriminative intra-class features and reduce false positive rates in challenging cases.\n"}, {"title": "Invertible Sharpening Network for MRI Reconstruction Enhancement", "abstract": "High-quality MRI reconstruction plays a critical role in clinical applications. Deep learning-based methods have achieved promising results on MRI reconstruction. However, most state-of-the-art methods were designed to optimize the evaluation metrics commonly used for natural images, such as PSNR and SSIM, whereas the visual quality is not primarily pursued. Compared to the fully-sampled images, the reconstructed images are often blurry, where high-frequency features might not be sharp enough for confident clinical diagnosis. To this end, we propose an invertible sharpening network (InvSharpNet) to improve the visual quality of MRI reconstructions. During training, unlike the traditional methods that learn to map the input data to the ground truth, InvSharpNet adapts a backward training strategy that learns a blurring transform from the ground truth (fully-sampled image) to the input data (blurry reconstruction). During inference, the learned blurring transform can be inverted to a sharpening transform leveraging the network\u00e2\u0080\u0099s invertibility. The experiments on various MRI datasets demonstrate that InvSharpNet can improve reconstruction sharpness with few artifacts. The results were also evaluated by radiologists, indicating better visual quality and diagnostic confidence of our proposed method. \n"}, {"title": "Is a PET all you need? A multi-modal study for Alzheimer\u00e2\u0080\u0099s disease using 3D CNNs", "abstract": "Alzheimer\u00e2\u0080\u0099s Disease (AD) is the most common form of dementia and often difficult to diagnose due to the multifactorial etiology of dementia. Recent works on neuroimaging-based computer-aided diagnosis with deep neural networks (DNNs) showed that fusing structural magnetic resonance images (sMRI) and fluorodeoxyglucose positron emission tomography (FDG-PET) leads to improved accuracy in a study population of healthy controls and subjects with AD. However, this result conflicts with the established clinical knowledge that FDG-PET better captures AD-specific pathologies than sMRI. Therefore, we propose a framework for the systematic evaluation of multi-modal DNNs and critically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI for binary healthy vs. AD, and three-way healthy/mild cognitive impaired/AD classification. Our experiments demonstrate that a single-modality network using FDG-PET performs better than MRI (accuracy 0.91 vs 0.87) and does not show improvement when combined. This conforms with the established clinical knowledge on AD biomarkers, but raises questions about the true benefit of multi-modal DNNs. We argue that future work on multi-modal fusion should systematically assess the contribution of individual modalities following our proposed evaluation framework. Finally, we encourage the community to go beyond healthy vs.AD classification and focus on differential diagnosis of dementia, where fusing multi-modal image information conforms with a clinical need.\n"}, {"title": "iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images", "abstract": "Interactive image segmentation has been widely applied to obtain high-quality voxel-level labels for medical images.\nThe recent success of Transformers on various vision tasks has paved the road for developing Transformer-based interactive image segmentation approaches.\nHowever, these approaches remain unexplored and, in particular, have not been developed for 3D medical image segmentation. To fill this research gap, we investigate Transformer-based interactive image segmentation and its application to 3D medical images.\nThis is a nontrivial task due to two main challenges: 1) limited memory for computationally inefficient Transformers and 2) limited labels for 3D medical images.\nTo tackle the first challenge, we propose iSegFormer, a memory-efficient Transformer that combines a Swin Transformer with a lightweight multilayer perceptron (MLP) decoder.\nTo address the second challenge, we pretrain iSegFormer on large amount of unlabeled datasets and then finetune it with only a limited number of segmented 2D slices.\nWe further propagate the 2D segmentations obtained by iSegFormer to unsegmented slices in 3D images using a pre-existing segmentation propagation model pretrained on videos. We evaluate iSegFormer on the public OAI-ZIB dataset for interactive knee cartilage segmentation. Evaluation results show that iSegFormer outperforms its convolutional neural network (CNN) counterparts on interactive 2D knee cartilage segmentation, with competitive computational efficiency. When propagating the 2D interactive segmentations \nof 5 slices to other unprocessed slices within the same 3D volume, we achieve 82.2% Dice score for 3D knee cartilage segmentation. Code is available at\nhttps://github.com/uncbiag/iSegFormer.\n"}, {"title": "Joint Class-Affinity Loss Correction for Robust Medical Image Segmentation with Noisy Labels", "abstract": "Noisy labels collected with limited annotation cost prevent medical image segmentation algorithms from learning precise semantic correlations. Previous segmentation arts of learning with noisy labels merely perform a pixel-wise manner to preserve semantics, such as pixel-wise label correction, but neglect the pair-wise manner. In fact, we observe that the pair-wise manner capturing affinity relations between pixels can greatly reduce the label noise rate. Motivated by this observation, we present a novel perspective for noisy mitigation by incorporating both pixel-wise and pair-wise manners, where supervisions are derived from noisy class and affinity labels, respectively. Unifying the pixel-wise and pair-wise manners, we propose a robust Joint Class-Affinity Segmentation (JCAS) framework to combat label noise issues in medical image segmentation. Considering the affinity in pair-wise manner incorporates contextual dependencies, a differentiated affinity reasoning (DAR) module is devised to rectify the pixel-wise segmentation prediction by reasoning about intra-class and inter-class affinity relations. To further enhance the noise resistance, a class-affinity loss correction (CALC) strategy is designed to correct supervision signals via the modeled noise label distributions in class and affinity labels. Meanwhile, CALC strategy interacts the pixel-wise and pair-wise manners through the theoretically derived consistency regularization. Extensive experiments under both synthetic and real-world noisy labels corroborate the efficacy of the proposed JCAS framework with a minimum gap towards the upper bound performance. The source code is available at https://github.com/CityU-AIM-Group/JCAS.\n"}, {"title": "Joint Graph Convolution for Analyzing Brain Structural and Functional Connectome", "abstract": "The white-matter (micro-)structural architecture of the brain promotes synchrony among neuronal populations, giving rise to richly patterned functional connections. A fundamental question for systems neuroscience is determining the best way to relate structural and functional networks quantified by diffusion tensor imaging and resting-state functional MRI. As one of the state-of-the-art approaches for network analysis, graph convolutional networks (GCN) has been separately used to analyze functional and structural networks, but separate analysis is not able to explore inter-network relationships. In this work, we propose to couple the two networks of an individual by adding inter-network edges between corresponding brain regions, so that the joint structure-function graph can be directly analyzed by a single GCN. The weights of inter-network edges are learnable, reflecting the possibility that the structure-function coupling strength is not uniform across the brain. We applied our approach to predict the age and sex of participants on the public dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA) based on their functional and structural networks. Our results support that the proposed Joint-GCN outperforms existing multi-modal graph learning approaches for analyzing structural and functional networks.\n"}, {"title": "Joint Modeling of Image and Label Statistics for Enhancing Model Generalizability of Medical Image Segmentation", "abstract": "Although supervised deep-learning has achieved promising performance in medical image segmentation, many methods cannot generalize well on unseen data, limiting their real-world applicability. To address this problem, we propose a deep learning-based Bayesian framework, which jointly models image and label statistics, utilizing the domain-irrelevant contour of a medical image for segmentation.  Specifically, we first decompose an image into components of contour and basis. Then, we model the expected label as a variable only related to the contour. Finally, we develop a variational Bayesian framework to infer the posterior distributions of these variables, including the contour, the basis, and the label. The framework is implemented with neural networks, thus is referred to as deep Bayesian segmentation. Results on the task of cross-sequence cardiac MRI segmentation show that our method set a new state of the art for model generalizability. Particularly, the BayeSeg model trained with LGE MRI generalized well on T2 images and outperformed other models with great margins, \\textit{i.e.}, over 0.47 in terms of average Dice. Our code is available at https://zmiclab.github.io/projects.html.\n"}, {"title": "Joint Prediction of Meningioma Grade and Brain Invasion via Task-Aware Contrastive Learning", "abstract": "Preoperative and noninvasive prediction of the meningioma grade is important in clinical practice, as it directly influences the clinical decision making. What\u00e2\u0080\u0099s more, brain invasion in meningiomas (i.e., the presence of tumor tissue within the adjacent brain tissue) is an independent criterion for the grading of meningiomas and influences the treatment strategy. Although efforts have been reported to address these two tasks, most of them rely on hand-crafted features and there is no attempt to exploit the two prediction tasks simultaneously. In this paper, we propose a novel task-aware contrastive learning algorithm to jointly predict meningioma grade and brain invasion from multi-modal MRIs. Based on the basic multi-task learning framework, our key idea is to adopt contrastive learning strategy to disentangle the image features into task-specific features and task-common features, and explicitly leverage their inherent connections to improve feature representation for the two prediction tasks. In this retrospective study, an MRI dataset was collected, for which 800 patients (containing 148 high-grade, 62 invasion) were diagnosed with meningiomas by pathological analysis. Experimental results show that the proposed algorithm outperforms alternative multi-task learning methods, achieving AUCs of 0.8870 and 0.9787 for the prediction of meningioma grade and brain invasion, respectively. The code is available at https://github.com/IsDling/predictTCL.\n"}, {"title": "Joint Region-Attention and Multi-Scale Transformer for Microsatellite Instability Detection from Whole Slide Images in Gastrointestinal Cancer", "abstract": "Microsatellite instability (MSI) is a crucial biomarker to clinical immunotherapy in gastrointestinal cancer, while additional immunohistochemical or genetic tests for MSI are generally missing due to lack of medical resources. Deep learning has achieved promising performance in detecting MSI from hematoxylin and eosin (H&E) stained histopathology slides. However, these methods are primarily based on patch-supervised slide-label models and then aggregate patch-level results into the slideslevel result, resulting unstable prediction due to noisy patches and aggregation ways. \nIn this paper, we propose a joint region-attention and multi-scale transformer (RAMST) network for microsatellite instability detection from whole slide images in gastrointestinal cancer. Specifically, we present a region-attention mechanism and a feature weight uniform sampling (FWUS) method to learn a representative subset of image patches from whole slide images. Moreover, we introduce the transformer architecture to fuse the multi-scale histopathology features consisting of patch-level features with region-level features to characterize the whole slide images for slide-level MSI detection. Compared to the existing MSI detection methods, the proposed RAMST shows the best performances on the colorectal and stomach cancer dataset from The Cancer Genome Atlas (TCGA) and provides an effective features representation learning method for WSI-label tasks.\n"}, {"title": "Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification", "abstract": "Transformer has been widely used in histopathology whole slide image (WSI) classification for the purpose of tumor grading, prognosis analysis, etc. However, the design of token-wise self-attention and positional embedding strategy in the common Transformer limits the effectiveness and efficiency in the application to gigapixel histopathology images. In this paper, we propose a kernel attention Transformer (KAT) for histopathology WSI classification. The information transmission of the tokens is achieved by cross-attention between the tokens and a set of kernels related to the spatial relationship of the tokens on the WSI. Compared to the common Transformer structure, the proposed KAT can describe the hierarchical context information of the local regions of the WSI and thereby is more effective in histopathology WSI analysis. Meanwhile, the kernel-based cross-attention paradigm sharply reduces the computational amount. The proposed method was evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset with 2560 WSIs, and was compared with 5 state-of-the-art methods. The experimental results have demonstrated the proposed KAT is effective and efficient in the task of histopathology WSI classification and is superior to the state-of-the-art methods.\n"}, {"title": "Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos", "abstract": "Ultrasound examination is widely used in the clinical diagnosis of thyroid nodules (benign/malignant). However, the accuracy relies heavily on radiologist experience. Although deep learning techniques have been investigated for thyroid nodules recognition. Current solutions are mainly based on static ultrasound images, with limited temporal information used and inconsistent with clinical diagnosis. This paper proposes a novel method for the automated recognition of thyroid nodules through an exhaustive exploration of ultrasound videos and key-frames. We first propose a detection-localization framework to automatically identify the clinical key-frame with a typical nodule in each ultrasound video. Based on the localized key-frame, we develop a key-frame guided video classification model for thyroid nodule recognition. Besides, we introduce a motion attention module to help the network focus on significant frames in an ultrasound video, which is consistent with clinical diagnosis. The proposed thyroid nodule recognition framework is validated on clinically collected ultrasound videos, demonstrating superior performance compared with other state-of-the-art methods. \n"}, {"title": "Knowledge Distillation to Ensemble Global and Interpretable Prototype-based Mammogram Classification Models", "abstract": "State-of-the-art (SOTA) deep learning mammogram classifiers, trained with weakly-labelled images, often rely on global models that produce predictions with limited interpretability, which is a key barrier to their successful translation into clinical practice. On the other hand, prototype-based models improve interpretability by associating predictions with training image prototypes, but they are less accurate than global models and their prototypes tend to have poor diversity. We address these two issues with the proposal of ProtoPNet++, which adds interpretability to a global model by ensembling it with a prototype-based model. ProtoPNet++ distills the knowledge of the global model when training the prototype-based model with the goal of increasing the classification accuracy of the ensemble. Moreover, we propose an approach to increase prototype diversity by guaranteeing that all prototypes are  associated with different training images. Experiments on weakly-labelled private and public datasets show that ProtoPNet++ has higher classification accuracy than SOTA global and prototype-based models. Using lesion localisation to assess model interpretability, we show ProtoPNet++ is more effective than other prototype-based models and post-hoc explanation of global models. Finally, we show that the diversity of the prototypes learned by ProtoPNet++ is superior to SOTA prototype-based approaches.\n"}, {"title": "Landmark-free Statistical Shape Modeling via Neural Flow Deformations", "abstract": "Statistical shape modeling aims at capturing shape variations of an anatomical structure that occur within a given population. Shape models are employed in many tasks, such as shape reconstruction and image segmentation, but also shape generation and classification. Existing shape priors either require dense correspondence between training examples or lack robustness and topological guarantees. We present FlowSSM, a novel shape modeling approach that learns shape variability without requiring dense correspondence between training instances. It relies on a hierarchy of continuous deformation flows, which are parametrized by a neural network. Our model outperforms state-of-the-art methods in providing an expressive and robust shape prior for distal femur and liver. We show that the emerging latent representation is discriminative by separating healthy from pathological shapes. Ultimately, we demonstrate its effectiveness on two shape reconstruction tasks from partial data. Our source code is publicly available.\n"}, {"title": "Layer Ensembles: A Single-Pass Uncertainty Estimation in Deep Learning for Segmentation", "abstract": "Uncertainty estimation in deep learning has become a leading research field in medical image analysis due to the need for safe utilisation of AI algorithms in clinical practice. Most approaches for uncertainty estimation require sampling the network weights multiple times during testing or training multiple networks. This leads to higher training and testing costs in terms of time and computational resources. In this paper, we propose Layer Ensembles, a novel uncertainty estimation method that uses a single network and requires only a single pass to estimate epistemic uncertainty of a network. Moreover, we introduce an image-level uncertainty metric, which is more beneficial for segmentation tasks compared to the commonly used pixel-wise metrics such as entropy and variance. We evaluate our approach on 2D and 3D, binary and multi-class medical image segmentation tasks. Our method shows competitive results with state-of-the-art Deep Ensembles, requiring only a single network and a single pass.\n"}, {"title": "Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis", "abstract": "The limited availability of large image datasets, mainly due to data privacy and differences in acquisition protocols or hardware, is a significant issue in the development of accurate and generalizable machine learning methods in medicine. This is especially the case for Magnetic Resonance (MR) images, where different MR scanners introduce a bias that limits the performance of a machine learning model. We present a novel method that learns to ignore the scanner-related features present in MR images, by introducing specific additional constraints on the latent space. We focus on a real-world classification scenario, where only a small dataset provides images of all classes. Our method Learn to Ignore (L2I) outperforms state-of-the-art domain adaptation methods on a multi-site MR dataset for a classification task between multiple sclerosis patients and healthy controls.\n"}, {"title": "Learning Incrementally to Segment Multiple Organs in a CT Image", "abstract": "There exists a large number of datasets for organ segmentation, which are partially annotated and sequentially constructed. A typical dataset is constructed at a certain time by curating medical images and annotating the organs of interest. In other words, new datasets with annotations of new organ categories are built over time. To unleash the potential behind these partially labeled, sequentially-constructed datasets, we propose to incrementally learn a multi-organ segmentation model. In each incremental learning (IL) stage, we lose the access to previous data and annotations, whose knowledge is assumingly captured by the current model, and gain the access to a new dataset with annotations of new organ categories, from which we learn to update the organ segmentation model to include the new organs. While IL is notorious for its \u00e2\u0080\u0098catastrophic forgetting\u00e2\u0080\u0099 weakness in the context of natural image analysis, we experimentally discover that such a weakness mostly disappears for CT multi-organ segmentation. To further stabilize the model performance across the IL stages, we introduce a light memory module and some loss functions to restrain the representation of different categories in feature space, aggregating feature representation of the same class and separating feature representation of different classes. Extensive experiments on five open-sourced datasets are conducted to illustrate the effectiveness of our method.\n"}, {"title": "Learning iterative optimisation for deformable image registration of lung CT with recurrent convolutional networks", "abstract": "Deep learning-based methods for deformable image registration have continually been increasing in accuracy. However, conventional methods using optimisation remain ubiquitous, as they often outperform deep learning-based methods regarding accuracy on test data. Recent learning-based methods for lung registration tasks prevalently employ instance optimisation on test data to achieve state-of-the-art performance. We propose a fully deep learning-based approach, that aims to emulate the structure of gradient-based optimisation as used in con- ventional registration and thus learns how to optimise. Our architecture consists of recurrent updates on a convolutional network with deep supervision. It uses a dynamic sampling of the cost function, hidden states to imitate information flow during optimisation and incremental displacements for multiple iterations. Our code is publicly available at https://github.com/multimodallearning/Learn2Optimise/.\n"}, {"title": "Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement", "abstract": "Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes of permanent blindness worldwide. Designing an automatic grading system with good generalization ability for DR and DME is vital in clinical practice. However, prior works either grade DR or DME independently, without considering internal correlations between them, or grade them jointly by shared feature representation, yet ignoring potential generalization issues caused by difficult samples and data bias. Aiming to address these problems, we propose a framework for joint grading with the dynamic difficulty-aware weighted loss (DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired by curriculum learning, DAW learns from simple samples to difficult samples dynamically via measuring difficulty adaptively. DETACH separates features of grading tasks to avoid potential emphasis on the bias. With the addition of DAW and DETACH, the model learns robust disentangled feature representations to explore internal correlations between DR and DME and achieve better grading performance. Experiments on three benchmarks show the effectiveness and robustness of our framework under both the intra-dataset and cross-dataset tests. "}, {"title": "Learning self-calibrated optic disc and cup segmentation from multi-rater annotations", "abstract": "The segmentation of optic disc (OD) and optic cup (OC) from fundus images is an important fundamental task for glaucoma diagnosis. In the clinical practice, it is often necessary to collect opinions from multiple experts to obtain the final OD/OC annotation. This clinical routine helps to mitigate the individual bias. But when data is multiply annotated, standard deep learning models will be inapplicable. In this paper, we propose a novel neural network framework to learn OD/OC segmentation from multi-rater annotations. The segmentation results are self-calibrated through the iterative optimization of multi-rater expertness estimation and calibrated OD/OC segmentation. In this way, the proposed method can realize a mutual improvement of both tasks and finally obtain a refined segmentation result. Specifically, we propose Diverging Model (DivM) and Converging Model (ConM) to process the two tasks respectively. ConM segments the raw image based on the multi-rater expertness map provided by DivM. DivM generates multi-rater expertness map from the segmentation mask provided by ConM. The experiment results show that by recurrently running ConM and DivM, the results can be self-calibrated so as to outperform a range of state-of-the-art (SOTA) multi-rater segmentation methods.\n"}, {"title": "Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection", "abstract": "We propose a scalable and data-driven approach to learn shape distributions from large databases of healthy organs. To do so, volumetric segmentation masks are embedded into a common probabilistic shape space that is learned with a variational auto-encoding network. The resulting latent shape representations are leveraged to derive a zero-shot and few-shot methods for abnormal shape detection. The proposed distribution learning approach is illustrated on a large database of 1200 healthy pancreas shapes. Downstream qualitative and quantitative experiments are conducted on a separate test set of 224 pancreas from patients with mixed conditions. The abnormal pancreas detection AUC reached up to 65.41% in the zero-shot configuration, and 78.97% in the few-shot configuration with as few as 15 abnormal examples, outperforming a baseline approach based on the sole volume. \n"}, {"title": "Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites", "abstract": "In clinical practice, a segmentation network is often required to continually learn on a sequential data stream from multiple sites rather than a consolidated set, due to the storage cost and privacy restriction. However, during the continual learning process, existing methods are usually restricted in either network memorizability on previous sites or generalizability on unseen sites. This paper aims to tackle the challenging problem of Synchronous Memorizability and Generalizability (SMG) and to simultaneously improve performance on both previous and unseen sites, with a novel proposed SMG-learning framework. First, we propose a Synchronous Gradient Alignment (SGA) objective, which not only promotes the network memorizability by enforcing coordinated optimization for a small exemplar set from previous sites (called replay buffer), but also enhances the generalizability by facilitating site-invariance under simulated domain shift. Second, to simplify the optimization of SGA objective, we design a Dual-Meta algorithm that approximates the SGA objective as dual meta-objectives for optimization without expensive computation overhead. Third, for efficient rehearsal, we configure the replay buffer comprehensively considering additional inter-site diversity to reduce redundancy. Experiments on prostate MRI data sequentially acquired from six institutes demonstrate that our method can simultaneously achieve higher memorizability and generalizability over state-of-the-art methods. Code is available at https://github.com/jingyzhang/SMG-Learning.\n"}, {"title": "Learning Tumor-Induced Deformations to Improve Tumor-Bearing Brain MR Segmentation", "abstract": "We propose a novel framework that applies atlas-based whole-brain segmentation methods to tumor-bearing MR images. Given a patient brain MR image where the tumor is initially segmented, we use a point-cloud deep learning method to predict a displacement field, which is meant to be the deformation (inverse) caused by the growth (mass-effect and cell-infiltration) of the tumor. It\u00e2\u0080\u0099s then used to warp and modify the brain atlas to represent the change so that existing atlas-based healthy-brain segmentation methods could be applied to these pathological images. To show the practicality of our method, we implement a pipeline with nnU-Net MRI tumor initial segmentation and SAMSEG, an atlas-based whole-brain segmentation method. To train and validate the deformation network, we synthesize pathological ground truth by simulating artificial tumors in healthy images with TumorSim. This method is evaluated with both real and synthesized data. These experiments show that segmentation accuracy can be improved by learning tumor-induced deformation before applying standard full brain segmentation. Our code is available at https://github.com/jiameng1010/Brain_MRI_Tumor.\n"}, {"title": "Learning Underrepresented Classes from Decentralized Partially Labeled Medical Images", "abstract": "Using decentralized data for federated training is one promising emerging research direction for alleviating data scarcity in the medical domain. However, in contrast to large-scale fully labeled data commonly seen in general object recognition tasks, the local medical datasets are more likely to only have images annotated for a subset of classes of interest due to high annotation costs. In this paper, we consider a practical yet under-explored problem, where underrepresented classes only have few labeled instances available and only exist in a few clients of the federated system. We show that standard federated learning approaches fail to learn robust multi-label classifiers with extreme class imbalance and address it by proposing a novel federated learning framework, FedFew. FedFew consists of three stages, where the first stage leverages federated self-supervised learning to learn class-agnostic representations. In the second stage, the decentralized partially labeled data are exploited to learn an energy-based multi-label classifier for the common classes. Finally, the underrepresented classes are detected with the learned energy and a prototype-based nearest-neighbor model is proposed for few-shot matching. We evaluate FedFew on multi-label thoracic disease classification tasks and demonstrate that it outperforms the federated baselines by a large margin.\n"}, {"title": "Learning with Context Encoding for Single-Stage Cranial Bone Labeling and Landmark Localization", "abstract": "Automatic anatomical segmentation and landmark localization in medical images are important tasks during craniofacial analysis. While deep neural networks have been recently applied to segment cranial bones and identify cranial landmarks from computed tomography (CT) or magnetic resonance (MR) images, existing methods often provide suboptimal and sometimes unrealistic results because they do not incorporate contextual image information. Additionally, most state-of-the-art deep learning methods for cranial bone segmentation and landmark detection rely on multi-stage data processing pipelines, which are inefficient and prone to errors. In this paper, we propose a novel context encoding-constrained neural network for single-stage cranial bone labeling and landmark localization. Specifically, we design and incorporate a novel context encoding module into a U-Net-like architecture. We explicitly enforce the network to capture context-related features for representation learning so pixel-wise predictions are not isolated from the image context. In addition, we introduce a new auxiliary task to model the relative spatial configuration of different anatomical landmarks, which serves as an additional regularization that further refines network predictions. The proposed method is end-to-end trainable for single-stage cranial bone labeling and landmark localization. The method was evaluated on a highly diverse pediatric 3D CT image dataset with 274 subjects. Our experiments demonstrate superior performance of our method compared to state-of-the-art approaches.\n"}, {"title": "Learning-based and unrolled motion-compensated reconstruction for cardiac MR CINE imaging", "abstract": "Motion-compensated MR reconstruction (MCMR) is a powerful concept with considerable potential, consisting of two coupled sub-problems: Motion estimation, assuming a known image, and image reconstruction, assuming  known motion. In this work, we propose a learning-based self-supervised framework for MCMR, to efficiently deal with non-rigid motion corruption in cardiac MR imaging. Contrary to conventional MCMR methods in which the motion is estimated prior to reconstruction and remains unchanged during the iterative optimization process, we introduce a dynamic motion estimation process and embed it into the unrolled optimization. We establish a cardiac motion estimation network that leverages temporal information via a group-wise registration approach, and carry out a joint optimization between the motion estimation and reconstruction. Experiments on 40 acquired 2D cardiac MR CINE datasets demonstrate that the proposed unrolled MCMR framework can reconstruct high quality MR images at high acceleration rates where other state-of-the-art methods fail. We also show that the joint optimization mechanism is mutually beneficial for both sub-tasks, i.e., motion estimation and image reconstruction, especially when the MR image is highly undersampled.\n"}, {"title": "Learning-based US-MR Liver Image Registration with Spatial Priors", "abstract": "Registration of multi-modality images is necessary for the assessment of liver disease. In this work, we present an image registration workflow which is designed to achieve reliable alignment for subject-specific magnetic resonance (MR) and intercostal 3D ultrasound (US) images of the liver. Spatial priors modeled from the right rib segmentation are utilized to generate the initial alignment between the MR and US scans without the need of any additional tracking information. For rigid image alignment, tissue segmentation models are extracted from the MR and US data with a learning-based approach to apply surface point cloud registration. Local alignment accuracy is further improved via the LC2 image similarity metrics-based non-rigid registration technique. This workflow was validated with in-vivo liver image data for 18 subjects. The best average TRE of rigid and non-rigid registration obtained with our dataset was at 6.27 \u00c2\u00b1 2.82 mm and 3.63 \u00c2\u00b1 1.87 mm, respectively.\n"}, {"title": "Lesion Guided Explainable Few Weak-shot Medical Report Generation", "abstract": "Medical images are widely used in clinical practice for diagnosis. Automatically generating interpretable medical reports can reduce radiologists\u00e2\u0080\u0099 burden and facilitate timely care. However, most existing approaches to automatic report generation require sufficient labeled data for training. In addition, the learned model can only generate reports for the training classes, lacking the ability to adapt to previously unseen novel diseases. To this end, we proposed a lesion guided explainable few weak-shot medical report generation framework that learns correlation between seen and novel classes through visual and semantic feature alignment, aiming to generate medical reports for diseases not observed in training. It integrates a lesion-centric feature extractor and a Transformer-based report generation module. Concretely, the lesion-centric feature extractor detects the abnormal regions and learns correlations between seen and novel classes with multi-view (visual and lexical) embeddings. Then, the detected regions and corresponding embeddings were concatenated as input to the report generation module for explainable report generation including text descriptions and corresponding abnormal regions detected in the images. We conduct experiments on FFA-IR, a dataset providing explainable annotations, showing that our framework outperforms others on report generation for novel diseases.\n"}, {"title": "Lesion-Aware Contrastive Representation Learning for Histopathology Whole Slide Images Analysis", "abstract": "Image representation learning has been a key challenge to promote the performance of the histopathological whole slide images analysis. The previous representation learning methods followed the supervised learning paradigm. However, manual annotation for large-scale WSIs is time-consuming and labor-intensive. Hence, the self-supervised contrastive learning has recently attracted intensive attention. In this paper, we proposed a novel contrastive representation learning framework named Lesion-Aware Contrastive Learning (LACL) for histopathology whole slide image analysis. We built a lesion queue based on the memory bank structure to store the representations of different classes of WSIs, which allowed the contrastive model to selectively define the negative pairs during the training. Moreover, We designed a queue refinement strategy to purify the representations stored in the lesion queue. The experimental results demonstrate that LACL achieves the best performance in histopathology image representation learning on different datasets, and outperforms state-of-the-art methods under different WSI classification benchmarks. \n"}, {"title": "Lesion-aware Dynamic Kernel for Polyp Segmentation", "abstract": "Automatic and accurate polyp segmentation plays an essential role in early colorectal cancer diagnosis. However, it has always been a challenging task due to 1) the diverse shape, size, brightness and other appearance characteristics of polyps, 2) the tiny contrast between concealed polyps and their surrounding regions. To address these problems, we propose a lesion-aware dynamic network (LDNet) for polyp segmentation, which is a traditional u-shape encoder-decoder structure incorporated with a dynamic kernel generation and updating scheme. Specifically, the designed segmentation head is conditioned on the global context features of the input image and iteratively updated by the extracted lesion features according to polyp segmentation predictions. This simple but effective scheme endows our model with powerful segmentation performance and generalization capability. Besides, we utilize the extracted lesion representation to enhance the feature contrast between the polyp and background regions by a tailored lesion-aware cross-attention module (LCA), and design an efficient self-attention module (ESA) to capture long-range context relations, further improving the segmentation accuracy. Extensive experiments on four public polyp benchmarks and our collected large-scale polyp dataset demonstrate the superior performance of our method compared with other state-of-the-art approaches. The source code is available at https://github.com/ReaFly/LDNet.\n"}, {"title": "Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis", "abstract": "Thyroid nodule classification aims at determining whether the nodule is benign or malignant based on a given ultrasound image. However, the label obtained by the cytological biopsy which is the golden standard in clinical medicine is not always consistent with the ultrasound imaging TI-RADS criteria. The information difference between the two causes the existing deep learning-based classification methods to be indecisive. To solve the Inconsistent Label problem, we propose an Adaptive Curriculum Learning (ACL) framework, which adaptively discovers and discards the samples with inconsistent labels. Specifically, ACL takes both hard sample and model certainty into account, and could accurately determine the threshold to distinguish the samples with Inconsistent Label. Moreover, we contribute TNCD: a Thyroid Nodule Classification Dataset to facilitate future related research on thyroid nodules. Extensive experimental results on TNCD based on three different backbone networks not only demonstrate the superiority of our method but also prove that the less-is-more principle which strategically discards the samples with Inconsistent Label could yield performance gains. Source code and data are available at https://github.com/chenghui-666/ACL/.\n"}, {"title": "Leveraging Labeling Representations in Uncertainty-based Semi-supervised Segmentation", "abstract": "Semi-supervised segmentation tackles the scarcity of annotations by leveraging unlabeled data with a small amount of labeled data. A prominent way to utilize the unlabeled data is by consistency training which commonly uses a teacher-student network, where a teacher guides a student segmentation. The predictions of unlabeled data are not reliable, therefore, uncertainty-aware methods have been proposed to gradually learn from meaningful and reliable predictions. Uncertainty estimation, however, relies on multiple inferences from model predictions that need to be computed for each training step, which is computationally expensive. This work proposes a novel method to estimate the pixel-level uncertainty by leveraging the labeling representation of segmentation masks. On the one hand, a labeling representation is learnt to represent the available segmentation masks. The learnt labeling representation is used to map the prediction of the segmentation into a set of plausible masks. Such a reconstructed segmentation mask aids in estimating the pixel-level uncertainty guiding the segmentation network. The proposed method estimates the uncertainty with a single inference from the labeling representation, thereby reducing the total computation. We evaluate our method on the 3D segmentation of left atrium in MRI, and we show that our uncertainty estimates from our labeling representation improves the segmentation accuracy over state-of-the-art methods. Code is released at https://github.com/adigasu/Labeling_Representations.\n"}, {"title": "LIDP: A Lung Image Dataset with Pathological Information for Lung Cancer Screening", "abstract": "Lung cancer has been one of the greatest lethal cancers worldwide. Computed Tomograph (CT) makes it possible to diagnose lung cancer at an early stage, which can significantly reduce its mortality. In recent years, deep neural networks (DNN) have been widely used to improve the accuracy of benign and malignant pulmonary nodules classification. But the limitation of DNN approach is that AI model\u00e2\u0080\u0099s performance and generalization highly depend on the size and quality of the training data. With our best knowledge, almost all existing public lung nodule datasets, e.g., LIDC-IDRI, obtain the crucial benign and malignant labels by radiographic analysis, instead of pathological examination. In this paper, we argue that, without pathology report and hence lack of labels\u00e2\u0080\u0099 authenticity, LIDC-IDRI based machine-learning (ML) models are short of generalization. To prove our hypothesis, we introduce a new lung CT image dataset with pathological information (LIDP), for lung cancer screening. LIDP contains 990 samples, including 783 malignant samples and 207 benign samples. More critically, the labels of all samples have been all examined by pathological biopsy. We evaluate various of existing LIDC-based state-of-the-art (SOTA) models on LIDP. Our experimental results show the extreme poor generalization ability of existing SOTA models that are trained on LIDC-IDRI dataset. Our scientific conclusion is striking: the distributions of these datasets are significantly different. We claim that the LIDP dataset is a very valuable addition to the existing datasets like LIDC-IDRI. LIDP can be well used for independent testing or for training new ML models for lung cancer early detection.\n"}, {"title": "LifeLonger: A Benchmark for Continual Disease Classification", "abstract": "Deep learning models have shown a great effectiveness in recognition of findings in medical images. However, they cannot handle the ever-changing clinical environment, bringing newly annotated medical data from different sources. To exploit the incoming streams of data, these models would benefit largely from sequentially learning from new samples, without forgetting the previously obtained knowledge. In this paper we introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST collection, by applying existing state-of-the-art continual learning methods. In particular, we consider three continual learning scenarios, namely, task and class incremental learning and the newly defined cross-domain incremental learning. Task and class incremental learning of diseases address the issue of classifying new samples without re-training the models from scratch, while cross-domain incremental learning addresses the issue of dealing with datasets originating from different institutions while retaining the previously obtained knowledge. We perform a thorough analysis of the performance and examine how the well-known challenges of continual learning, such as the catastrophic forgetting exhibit themselves in this setting. The encouraging results demonstrate that continual learning has a major potential to advance disease classification and to produce a more robust and efficient learning framework for clinical settings. The code repository, data partitions and baseline results for the complete benchmark are publicly available.\n"}, {"title": "LiftReg: Limited Angle 2D/3D Deformable Registration", "abstract": "In this work we propose LiftReg, a 2D/3D deformable reg-\nistration approach. LiftReg is a deep registration framework which is\ntrained using sets of digitally reconstructed radiographs (DRR) and com-\nputed tomography (CT) image pairs. By using simulated training data,\nLiftReg can use high-quality CT-CT image similarity measure, which\nhelps the network to learn a high-quality deformation space. To further\nimprove registration quality and to address the inherent depth ambi-\nguities of very limited angle acquisitions, we propose to use the lifted\nfeatures extracted from the backprojected volume and a statistical de-\nformation model. We test our approach on the DirLab lung registration\ndataset and show that it outperforms the existing learning-based pair-\nwise registration approach.\n"}, {"title": "Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound", "abstract": "Accurate and consistent predictions of echocardiography parameters are important for cardiovascular diagnosis and treatment. In particular, segmentations of the left ventricle can be used to derive ventricular volume, ejection fraction (EF) and other relevant measurements. In this paper we propose a new automated method called EchoGraphs for predicting ejection fraction and segmenting the left ventricle by detecting anatomical keypoints. Models for direct coordinate regression based on Graph Convolutional Networks (GCNs) are used to detect the keypoints. GCNs can learn to represent the cardiac shape based on local appearance of each keypoint, as well as global spatial and temporal structures of all keypoints combined. We evaluate our EchoGraphs model on the EchoNet benchmark dataset. Compared to semantic segmentation, GCNs show accurate segmentation and improvements in robustness and inference run-time. EF is computed simultaneously to segmentations and our method also obtains state-of-the-art ejection fraction estimation. Source code is available online: https://github.com/guybenyosef/EchoGraphs\n"}, {"title": "Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction", "abstract": "Classical multiple instance learning (MIL) methods are often based on the identical and independent distributed assumption between instances, hence neglecting the potentially rich contextual information beyond individual entities. On the other hand, Transformers with global self-attention modules have been proposed to model the interdependencies among all instances. However, in this paper, we question: Is global relation modeling using self-attention necessary, or can we appropriately restrict self-attention calculations to local regimes in large-scale whole slide images (WSIs)? We propose a general-purpose local attention graph-based Transformer for MIL (LA-MIL), introducing an inductive bias by explicitly contextualizing instances in adaptive local regimes of arbitrary size. Additionally, an efficiently adapted loss function enables our approach to learn expressive WSI embeddings for the joint analysis of multiple biomarkers. We demonstrate that LA-MIL achieves state-of-the-art results in mutation prediction for gastrointestinal cancer, outperforming existing models on important biomarkers such as microsatellite instability for colorectal cancer. Our findings suggest that local self-attention sufficiently models dependencies on par with global modules. Our implementation will be published.\n"}, {"title": "Local Graph Fusion of Multi-View MR Images for Knee Osteoarthritis Diagnosis", "abstract": "Magnetic resonance imaging (MRI) has become necessary in clinical diagnosis for knee osteoarthritis (OA), while deep neural networks can contribute to the computer-assisted diagnosis. Recent works prove that instead of only using a single-view MR image (e.g., sagittal), integrating multi-view MR images can boost the performance of the deep network. However, existing multi-view networks typically encode each MRI view to a feature vector, fuse the feature vectors of all views, and then derive the final output using a set of shallow computations. Such a global fusion scheme happens at a coarse granularity, which may not effectively localize the often tiny abnormality related to the onset of OA. Therefore, this paper proposes a Local Graph Fusion Network (LGF-Net), which implements graph-based representation of knee MR images and multi-view fusion for OA diagnosis. We first model the multi-view MR images to a unified knee graph. Then, the patches of the same location yet from different views are encoded to one-dimensional features and are exchanged mutually during fusing. The local fusion of the features further propagates following edges by Graph Transformer Network in the LGF-Net, which finally yields the grade of OA. The experimental results show that the proposed framework outperforms state-of-the-art methods, demonstrating the effectiveness of local graph fusion in OA diagnosis.\n"}, {"title": "Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework", "abstract": "Tumor infiltration of the recurrent laryngeal nerve (RLN) is a contraindication for robotic thyroidectomy and can be difficult to detect via standard laryngoscopy. Ultrasound (US) is a viable alternative for RLN detection due to its safety and ability to provide real-time feedback. However, the tininess of the RLN, with a diameter typically less than 3 mm, poses significant challenges to the accurate localization of the RLN. In this work, we propose a knowledge-driven framework for RLN localization, mimicking the standard approach surgeons take to identify the RLN according to its surrounding organs. We construct a prior anatomical model based on the inherent relative spatial relationships between organs. Through Bayesian shape alignment (BSA), we obtain the candidate coordinates of the center of a region of interest (ROI) that encloses the RLN. The ROI allows a decreased field of view for determining the refined centroid of the RLN using a dual-path identification network, based on multi-scale semantic information. Experimental results indicate that the proposed method achieves superior hit rates and substantially smaller distance errors compared with state-of-the-art methods.\n"}, {"title": "Local-Region and Cross-Dataset Contrastive Learning for Retinal Vessel Segmentation", "abstract": "Retinal vessel segmentation is an essential preprocessing step for computer-aided diagnosis of ophthalmic diseases. Many efforts have been made to improve vessel segmentation by designing complex deep networks. However, due to some features related to detailed structures are not discriminative enough, it is still required to further improve the segmentation performance. Without adding complex network structures, we propose a local-region and cross-dataset contrastive learning method to enhance the feature embedding ability of a U-Net. Our method includes a local-region contrastive learning strategy and a cross-dataset contrastive learning strategy. The former aims to more effectively separate the features of pixels that are easily confused with their neighbors inside local regions. The latter utilizes a memory bank scheme that further enhances the features by fully exploiting the global contextual information of the whole dataset. We conducted extensive experiments on two public datasets (DRIVE and CHASE_DB1). The experimental results verify the effectiveness of the proposed method that has achieved the state-of-the-art performances.\n"}, {"title": "Longitudinal Infant Functional Connectivity Prediction via Conditional Intensive Triplet Network", "abstract": "Longitudinal infant brain functional connectivity (FC) constructed\nfrom resting-state functional MRI (rs-fMRI) has increasingly become a pivotal\ntool in studying the dynamics of early brain development. However, due to various reasons including high acquisition cost, strong motion artifact, and subject\ndropout, there has been an extreme shortage of usable longitudinal infant rs-fMRI\nscans to construct longitudinal FCs, which hinders comprehensive understanding\nand modeling of brain functional development at early ages. To address this issue, in this paper, we propose a novel conditional intensive triplet network\n(CITN) for longitudinal prediction of the dynamic development of infant FC,\nwhich can traverse FCs within a long duration and predict the target FC at any\nspecific age during infancy. Targeting at accurately modeling of the progression\npattern of FC, while maintaining the individual functional uniqueness, our model\neffectively disentangles the intrinsically mixed age-related and identity-related\ninformation from the source FC and predicts the target FC by fusing well-disentangled identity-related information with the specific age-related information.\nSpecifically, we introduce an intensive triplet auto-encoder for effective disentanglement of age-related and identity-related information and an identity conditional module to mix identity-related information with designated age-related information. We train the proposed model in a self-supervised way and design\ndownstream tasks to help robustly disentangle age-related and identity-related\nfeatures. Experiments on 464 longitudinal infant fMRI scans show the superior\nperformance of the proposed method in longitudinal FC prediction in comparison\nwith state-of-the-art approaches.\n"}, {"title": "Long-tailed Multi-label Retinal Diseases Recognition via Relational Learning and Knowledge Distillation", "abstract": "More and more people are suffering from ocular diseases, which may cause blindness if not treated promptly. However, it is not easy to diagnose these diseases for the barely visible clinical symptoms. Even though some computer-aided approaches have been developed to help ophthalmologists make an accurate diagnosis, there still exist some challenges to be solved. For example, one patient may suffer from more than one retinal disease and these diseases often exhibit a long-tailed distribution, making it difficult to be automatically classified. In this work, we propose a novel framework that utilizes the correlations among these diseases in a knowledge distillation manner. Specifically, we apply the correlations from three main aspects (i.e., multi-task relation, feature relation, and pathological region relation) to recognize diseases more exactly. Firstly, we take diabetic retinopathy (DR) lesion segmentation and severity grading as the downstream tasks to train the network backbone for the findings that segmentation may improve the classification. Secondly, the long-tailed dataset is divided into several subsets to train multiple teacher networks according to semantic feature relation, which can help reduce the label co-occurrence and class imbalance. Thirdly, an improved attention mechanism is adopted to explore relations among pathological regions. Finally, a class-balanced distillation loss is introduced to distill the multiple teacher models into a student model. Extensive experiments are conducted to validate the superiority of our proposed method. The results have demonstrated that we achieve state-of-the-art performance on the publicly available datasets. ode will be available at: https://github.com/liyiersan/RLKD.\n"}, {"title": "Low-Dose CT Reconstruction via Dual-Domain Learning and Controllable Modulation", "abstract": "Existing CNN-based low-dose CT reconstruction methods focus on restoring the degraded CT images by  processing on the image domain or the raw data (sinogram) domain independently, or leveraging both domains by connecting them through some simple domain transform operators or matrices. However, both domains and their mutual benefits are not fully exploited, which impedes the performance to go step further. In addition, considering the subjective perceptual quality of the restored image, it is more necessary for doctors to adaptively control the denoising level for  different regions or organs according to diagnosis convenience, which cannot be done using existing deterministic networks.\nTo tackle these difficulties, this paper breaks away the shackles of general paradigms and proposes a novel low-dose CT reconstruction framework via dual-domain learning and controllable modulation. Specifically, we propose a dual-domain base network to fully address the mutual dependencies between the image domain and sinogram domain. Upon this, we integrate a controllable modulation module to adjust the latent features of the base network, which allows to finely-grained control the reconstruction by considering the trade-off between artifacts reduction and detail preservation to assist doctors in diagnosis. Experiments results on Mayo clinic dataset and Osaka dataset demonstrate that our method achieves superior performance.\n"}, {"title": "Low-Resource Adversarial Domain Adaptation for Cross-Modality Nucleus Detection", "abstract": "Due to domain shifts, deep cell/nucleus detection models trained on one microscopy image dataset might not be applicable to other datasets acquired with different imaging modalities. Unsupervised domain adaptation (UDA) based on generative adversarial networks (GANs) has recently been exploited to close domain gaps and has achieved excellent nucleus detection performance. However, current GAN-based UDA model training often requires a large amount of unannotated target data, which may be prohibitively expensive to obtain in real practice. Additionally, these methods have significant performance degradation when using limited target training data. In this paper, we study a more realistic yet challenging UDA scenario, where (unannotated) target training data is very scarce, a low-resource case rarely explored for nucleus detection in previous work. Specifically, we augment a dual GAN network by leveraging a task-specific model to supplement the target-domain discriminator and facilitate generator learning with limited data. The task model is constrained by cross-domain prediction consistency to encourage semantic content preservation for image-to-image translation. Next, we incorporate a stochastic, differentiable data augmentation module into the task-augmented GAN network to further improve model training by alleviating discriminator overfitting. This data augmentation module is a plug-and-play component, requiring no modification of network architectures or loss functions. We evaluate the proposed low-resource UDA method for nucleus detection on multiple public cross-modality microscopy image datasets. With a single training image in the target domain, our method significantly outperforms recent state-of-the-art UDA approaches and delivers very competitive or superior performance over fully supervised models trained with real labeled target data.\n"}, {"title": "LSSANet: A Long Short Slice-Aware Network for Pulmonary Nodule Detection", "abstract": "Convolutional neural networks (CNNs) have been demonstrated to be highly effective in the field of pulmonary nodule detection. However, existing CNN based pulmonary nodule detection methods lack the ability to capture long-range dependencies, which is vital for global information extraction. In computer vision tasks, non-local operations have been widely utilized, but the computational cost could be very high for 3D computed tomography (CT) images. To address this issue, we propose a long short slice-aware network (LSSANet) for the detection of pulmonary nodules. In particular, we develop a new non-local mechanism termed long short slice grouping (LSSG), which splits the compact non-local embeddings into a short-distance slice grouped one and a long-distance slice grouped counterpart. This not only reduces the computational burden, but also keeps long-range dependencies among any elements across slices and in the whole feature map. The proposed LSSG is easy-to-use and can be plugged into many pulmonary nodule detection networks. To verify the performance of LSSANet, we compare with several recently proposed and competitive detection approaches based on 2D/3D CNN. Promising evaluation results on the large-scale PN9 dataset demonstrate the effectiveness of our method. Code is at https://github.com/Ruixxxx/LSSANet.\n"}, {"title": "MAL: Multi-modal attention learning for tumor diagnosis based on bipartite graph and multiple branches", "abstract": "The multi-modal fusion of medical images has been widely used in recent years. Most methods focus on images with a single plane, such as the axial plane with different sequences (T1, T2) or different modalities (CT, MRI), rather than multiple planes with or without cross modalities. Further, most methods focus on segmentation or classification at the image or sequence level rather than the patient level. This paper proposes a general and scalable framework named MAL for the classification of benign and malignant tu-mors at the patient level based on multi-modal attention learning. A bipartite graph is used to model the correlations between different modalities, and then modal fusion is carried out in feature space by attention learning and multi-branch networks. Thereafter, multi-instance learning is adopted to ob-tain patient-level diagnostic results by considering different modal pairs of patient images to be bags and the edges in the bipartite graph to be instances. The modal and intra-type similarity losses at the patient level are calculated using the feature similarity matrix to encourage the model to extract high-level semantic features with high correlation. The experimental results con-firm the effectiveness of MAL on three datasets with respect to different multi-modal fusion tasks, including axial and sagittal MRI, axial CT and sag-ittal MRI, and T1 and T2 MRI sequences. And the application of MAL can also significantly improve the diagnostic accuracy and efficiency of doctors. Code is available at https://github.com/research-med/MAL.\n"}, {"title": "MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation", "abstract": "In this work, we propose a mutual information (MI) based unsupervised domain adaptation (UDA) method for the cross-domain nuclei segmentation. Nuclei vary substantially in structure and appearances across different cancer types, leading to a drop in performance of deep learning models when trained on one cancer type and tested on another. This domain shift becomes even more critical as accurate segmentation and quantification of nuclei is an essential histopathology task for the diagnosis/ prognosis of patients and annotating nuclei at the pixel level for new cancer types demands extensive effort by medical experts. To address this problem, we maximize the MI between labeled source cancer type data and unlabeled target cancer type data for transferring nuclei segmentation knowledge across domains. We use the Jensen-Shanon divergence bound, requiring only one negative pair per positive pair for MI maximization. We evaluate our set-up for multiple modeling frameworks and on different datasets comprising of over 20 cancer-type domain shifts and demonstrate competitive performance. All the recently proposed approaches consist of multiple components for improving the domain adaptation, whereas our proposed module is light and can be easily incorporated into other methods. \n"}, {"title": "Mapping in Cycles: Dual-Domain PET-CT Synthesis Framework with Cycle-Consistent Constraints", "abstract": "Positron emission tomography (PET) is an important medical imaging technique, especially for brain and cancer disease diagnosis. Modern PET scanner is usually combined with computed tomography (CT), where CT image is used for anatomical localization, PET attenuation correction, and radiotherapy treatment planning. Considering radiation dose of CT image as well as increasing spatial resolution of PET image, there is growing demand to synthesize CT image from PET image (without scanning CT) to reduce risk of radiation exposure. However, existing works perform learning-based image synthesis to construct cross-modality mapping only in the image domain, without considering of the projection domain, leading to potential physical inconsistency. To address this problem, we propose a novel PET-CT synthesis framework by exploiting dual-domain information (i.e., image domain and projection domain). Specifically, we design both image domain network and projection domain network to jointly learn high-dimensional mapping from PET-to-CT. The image domain and the projection domain can be connected together with a forward projection (FP) and a filtered back projection (FBP). To further help the PET-to-CT synthesis task, we also design a secondary CT-to-PET synthesis task with the same network structure, and combine the two tasks into a bidirectional mapping framework with several closed cycles. More importantly, these cycles can serve as cycle-consistent losses to further help network training for better synthesis performance. Extensive validations on the clinical PET-CT data demonstrate the proposed PET-CT synthesis framework outperforms the state-of-the-art (SOTA) medical image synthesis methods with significant improvements.\n"}, {"title": "Mask Rearranging Data Augmentation for 3D Mitochondria Segmentation", "abstract": "3D mitochondria segmentation in electron microscopy (EM) images has achieved significant progress. However, existing learning-based methods with high performance typically rely on extensive training data with high-quality manual annotations, which is time-consuming and labor-intensive. To address this challenge, we propose a novel data augmentation method tailored for 3D mitochondria segmentation. First, we train a Mask2EM network for learning the mapping from the ground-truth instance masks to real 3D EM images in an adversarial manner. Based on the Mask2EM network, we can obtain synthetic 3D EM images from arbitrary instance masks to form a sufficient amount of paired training data for segmentation. Second, we design a 3D mask layout generator to generate diverse instance layouts by rearranging volumetric instance masks according to mitochondrial distance distribution. Experiments demonstrate that, as a plug-and-play module, the proposed method boosts existing 3D mitochondria segmentation networks to achieve state-of-the-art performance. Especially, the proposed method brings significant improvements when training data is extremely limited. Code will be available at: https://github.com/qic999/MRDA_MitoSeg.\n"}, {"title": "MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation", "abstract": "Convolutional neural networks (CNNs) have achieved remarkable segmentation accuracy on benchmark datasets where training and test sets are from the same domain, yet their performance can degrade significantly on unseen domains, which hinders the deployment of CNNs in many clinical scenarios. Most existing works improve model out-of-domain (OOD) robustness by collecting multi-domain datasets for training, which is expensive and may not always be feasible due to privacy and logistical issues. In this work, we focus on improving model robustness using a single-domain dataset only. We propose a novel data augmentation framework called MaxStyle, which maximizes the effectiveness of style augmentation for model OOD performance. It attaches an auxiliary style-augmented image decoder to a segmentation network for robust feature learning and data augmentation. Importantly, MaxStyle augments data with improved image style diversity and hardness, by expanding the style space with noise and searching for the worst-case style composition of latent features via adversarial training. With extensive experiments on multiple public cardiac and prostate MR datasets, we demonstrate that MaxStyle leads to significantly improved out-of-distribution robustness against unseen corruptions as well as common distribution shifts across multiple, different, unseen sites and unknown image sequences under both low- and high-training data settings. The code will be available at https://github.com/cherise215/MaxStyle.\n"}, {"title": "MCP-Net: Inter-frame Motion Correction with Patlak Regularization for Whole-body Dynamic PET", "abstract": "Inter-frame patient motion introduces spatial misalignment and degrades parametric imaging in whole-body dynamic positron emission tomography (PET). Most current deep learning inter-frame motion correction works consider only the image registration problem, ignoring tracer kinetics. We propose an inter-frame Motion Correction framework with Patlak regularization (MCP-Net) to directly optimize the Patlak fitting error and further improve model performance. The MCP-Net contains three modules: a motion estimation module consisting of a multiple-frame 3-D U-Net with a convolutional long short-term memory layer combined at the bottleneck; an image warping module that performs spatial transformation; and an analytical Patlak module that estimates Patlak fitting with the motion-corrected frames and the individual input function. A Patlak loss penalization term using mean squared percentage fitting error is introduced to the loss function in addition to image similarity measurement and displacement gradient loss. Following motion correction, the parametric images were generated by standard Patlak analysis. Compared with both traditional and deep learning benchmarks, our network further corrected the residual spatial mismatch in the dynamic frames, improved the spatial alignment of Patlak Ki/Vb images, and reduced normalized fitting error. With the utilization of tracer dynamics and enhanced network performance, MCP-Net has the potential for further improving the quantitative accuracy of dynamic PET. Our code is released at https://github.com/gxq1998/MCP-Net.\n"}, {"title": "Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction", "abstract": "We propose a novel and unified method, measurement-conditioned denoising diffusion probabilistic model (MC-DDPM), for under-sampled medical image reconstruction based on DDPM. Different from previous works, MC-DDPM is defined in measurement domain (e.g. k-space in MRI reconstruction) and conditioned on under-sampling mask. We apply this method to accelerate MRI reconstruction and the experimental results show excellent performance, outperforming full supervision baseline and the state-of-the-art score-based reconstruction method. Due to its generative nature, MC-DDPM can also quantify the uncertainty of reconstruction. Our code is available on github.\n"}, {"title": "Mesh-based 3D Motion Tracking in Cardiac MRI using Deep Learning", "abstract": "3D motion estimation from cine cardiac magnetic resonance (CMR) images is important for the assessment of cardiac function and diagnosis of cardiovascular diseases. Most of the previous methods focus on estimating pixel-/voxel-wise motion fields in the full image space, which ignore the fact that motion estimation is mainly relevant and useful within the object of interest, e.g., the heart. In this work, we model the heart as a 3D geometric mesh and propose a novel deep learning-based method that can estimate 3D motion of the heart mesh from 2D short- and long-axis CMR images. By developing a differentiable mesh-to-image rasterizer, the method is able to leverage the anatomical shape information from 2D multi-view CMR images for 3D motion estimation. The differentiability of the rasterizer enables us to train the method end-to-end. One advantage of the proposed method is that by tracking the motion of each vertex, it is able to keep the vertex correspondence of 3D meshes between time frames, which is important for quantitative assessment of the cardiac function on the mesh. We evaluate the proposed method on CMR images acquired from the UK Biobank study. Experimental results show that the proposed method quantitatively and qualitatively outperforms both conventional and learning-based cardiac motion tracking methods.\n"}, {"title": "Meta-hallucinator: Towards few-shot cross-modality cardiac image segmentation", "abstract": "Domain shift and label scarcity heavily limit deep learning applications to various medical image analysis tasks. Unsupervised domain adaptation (UDA) techniques have recently achieved promising cross-modality medical image segmentation by transferring knowledge from a label-rich source domain to an unlabeled target domain. However, it is also difficult to collect annotations from the source domain in many clinical applications, rendering most prior works suboptimal with the label-scarce source domain, particularly for few-shot scenarios, where only a few source labels are accessible. To achieve efficient few-shot cross-modality segmentation, we propose a novel transformation-consistent meta-hallucination framework, meta hallucinator, with the goal of learning to diversify data distributions and generate useful examples for enhancing cross-modality performance. In our framework, hallucination and segmentation models are jointly trained with the gradient-based meta-learning strategy to synthesize examples that lead to good segmentation performance on the target domain. To further facilitate data hallucination and cross-domain knowledge transfer, we develop a self-ensembling model with a hallucination-consistent property. Our meta-hallucinator can seamlessly collaborate with the meta-segmenter for learning to hallucinate with mutual benefits from a combined view of meta-learning and self-ensembling learning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiac segmentation demonstrate that our method performs favorably against various approaches by a lot in the few-shot UDA scenario.\n"}, {"title": "MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer", "abstract": "Robust self-training (RST) can augment the adversarial robustness of image classification models without significantly sacrificing models\u00e2\u0080\u0099 generalizability. However, RST and other state-of-the-art defense approaches failed to preserve the generalizability and reproduce their good adversarial robustness on small medical image sets. In this work, we propose the Multi-instance RST with drop-max layer, namely MIRST-DM, which involves a sequence of iteratively generated adversarial instances during training to learn smoother decision boundaries on small datasets. The proposed drop-max layer eliminates unstable features and helps learn representations that are robust to image perturbations. The proposed approach was validated using a small breast ultrasound dataset with 1,190 images. The results demonstrate that the proposed approach achieves state-of-the-art adversarial robustness against three prevalent attacks.\n"}, {"title": "Mixed Reality and Deep Learning for External Ventricular Drainage Placement: a Fast and Automatic Workflow for Emergency Treatments", "abstract": "The treatment of hydrocephalus is based on anatomical landmarks to guide the insertion of an External Ventricular Drain (EVD). This procedure can benefit from the adoption of Mixed Reality (MR) technology. \nIn this study, we assess the feasibility of a fully automatic MR and deep learning-based workflow to support emergency EVD placement, for which CT images are available and a fast and automatic workflow is needed. The proposed study provides a tool to automatically i) segment the skull, face skin, ventricles and Foramen of Monro from CT scans; ii) import the segmented model in the MR application; iii) register holograms on the patient\u00e2\u0080\u0099s head via a marker-less approach.\nAn ad-hoc evaluation approach including 3D-printed anatomical structures was developed to quantitatively assess the accuracy and usability of the registration workflow.\n"}, {"title": "mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation", "abstract": "Accurate brain tumor segmentation from Magnetic Resonance Imaging (MRI) requires joint learning of multimodal images. However, in clinical practice, it is not always possible to acquire a complete set of MRIs, and the problem of missing modalities causes severe performance degradation in existing multimodal segmentation methods. In this work, we present the first attempt to exploit the Transformer for multimodal brain tumor segmentation that is robust to any combinatorial subset of available modalities. Concretely, we propose a novel multimodal Medical Transformer (mmFormer) for incomplete multimodal learning with three main components: hybrid modality-specific encoders that bridge a convolutional encoder and an intra-modal Transformer for both local and global context modeling within each modality, an inter-modal Transformer to build and align the long-range correlations across modalities for modality-invariant features with global semantics corresponding to tumor region, and a decoder that performs a progressive up-sampling and fusion with the modality-invariant features to generate robust segmentation. Besides, auxiliary regularizers are introduced in both encoder and decoder to further enhance the model\u00e2\u0080\u0099s robustness to incomplete modalities. We conduct extensive experiments on the public BraTS 2018 dataset for brain tumor segmentation. The results demonstrate that the proposed mmFormer outperforms the state-of-the-art methods for incomplete multimodal brain tumor segmentation on almost all subsets of incomplete modalities, especially by an average 19.07% improvement of Dice on tumor segmentation with only one available modality. The source code will be publicly available after the blind review.\n"}, {"title": "Modality-adaptive Feature Interaction for Brain Tumor Segmentation with Missing Modalities", "abstract": "Multi-modal Magnetic Resonance Imaging (MRI) plays a crucial role in brain tumor segmentation. However, missing modality is a common phenomenon in clinical practice, leading to performance degradation in tumor segmentation. Considering that there exist complementary information among modalities, feature interaction among modalities is important for tumor segmentation. In this work, we propose Modality- adaptive Feature Interaction (MFI) with multi-modal code to adaptively interact features among modalities in different modality missing situations. MFI is a simple yet effective unit, based on graph structure and attention mechanism, to learn and interact complementary features be- tween graph nodes (modalities). Meanwhile, the proposed multi-modal code, indicating whether each modality is missing or not, guides MFI to learn adaptive complementary information between nodes in differ- ent missing situations. Applying MFI with multi-modal code in different stages of a U-shaped architecture, we design a novel network U-Net-MFI to interact multi-modal features hierarchically and adaptively for brain tumor segmentation with missing modality(ies). Experiments show that our model outperforms the current state-of-the-art methods for brain tumor segmentation with missing modalities.\n"}, {"title": "ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities", "abstract": "Multiple Sclerosis (MS) is a chronic neuroinflammatory disease and multi-modality MRIs are routinely used to monitor MS lesions. Many automatic MS lesion segmentation models have been developed and have reached human-level performance. However, most established methods assume the MRI modalities used during training are also available during testing, which is not guaranteed in  clinical practice. Previously, a training strategy termed Modality Dropout (ModDrop) has been applied to MS lesion segmentation to achieve the state-of-the-art performance with missing modality. In this paper, we present a novel method dubbed ModDrop++ to train a unified network adaptive to an arbitrary number of input MRI sequences. ModDrop++ upgrades the main idea of ModDrop in two key ways. First, we devise a plug-and-play dynamic head and adopt a filter scaling strategy to improve the expressiveness of the network. Second, we design a co-training strategy to leverage the intra-subject relation between full modality and missing modality. Specifically, the intra-subject co-training strategy aims to guide the dynamic head to generate similar feature representations between the full- and missing-modality data from the same subject. We use two public MS datasets to show the superiority of ModDrop++. Source code and trained models are available at https://github.com/han-liu/ModDropPlusPlus.\n"}, {"title": "Modelling Cycles in Brain Networks with the Hodge Laplacian", "abstract": "Cycles or loops in a network embed higher-order interactions beyond pairwise relations. The cycles are essential for the parallel processing of information and enable feedback loops. Despite the fundamental importance of cycles in understanding the higher-order connectivity, identifying and extracting them are computationally prohibitive. This paper proposes a novel persistent homology-based framework for extracting and modelling cycles in brain networks using the Hodge Laplacian. The method is applied in discriminating the functional brain networks of males and females. The code for modeling cycles through the Hodge Laplacian is provided in \\url{https://github.com/laplcebeltrami/hodge}.\n"}, {"title": "Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation", "abstract": "Contrastive learning (CL) aims to learn useful representation without relying on expert annotations in the context of medical image segmentation. Existing approaches mainly contrast a single positive vector (i.e., an augmentation of the same image) against a set of negatives within the entire remainder of the batch by simply mapping all input features into the same constant vector. Despite the impressive empirical performance, those methods have the following shortcomings: (1) it remains a formidable challenge to prevent the collapsing problems to trivial solutions; and (2) we argue that not all voxels within the same image are equally positive since there exist the dissimilar anatomical structures with the same image. In this work, we present a novel Contrastive Voxel-wise Representation Learning (CVRL) method to effectively learn low-level and high-level features by capturing 3D spatial context and rich anatomical information along both the feature and the batch dimensions. Specifically, we first introduce a novel CL strategy to ensure feature diversity promotion among the 3D representation dimensions. We train the framework through bi-level contrastive optimization (i.e., low-level and high-level) on 3D images. Experiments on two benchmark datasets and different labeled settings demonstrate the superiority of our proposed framework. More importantly, we also prove that our method inherits the benefit of hardness-aware property from the standard CL approaches."}, {"title": "Morphology-Aware Interactive Keypoint Estimation", "abstract": "Diagnosis based on medical images, such as X-ray images, often involves manual annotation of anatomical keypoints. However, this process involves significant human efforts and can thus be a bottleneck in the diagnostic process. To fully automate this procedure, deep-learning-based methods have been widely proposed and have achieved high performance in detecting keypoints in medical images. However, these methods still have clinical limitations: accuracy cannot be guaranteed for all cases, and it is necessary for doctors to double-check all predictions of models. In response, we propose a novel deep neural network that, given an X-ray image, automatically detects and refines the anatomical keypoints through a user-interactive system in which doctors can fix mispredicted keypoints with fewer clicks than needed during manual revision. Using our own collected data and the publicly available AASCE dataset, we demonstrate the effectiveness of the proposed method in reducing the annotation costs via extensive quantitative and qualitative results.\n"}, {"title": "Moving from 2D to 3D: volumetric medical image classification for rectal cancer staging", "abstract": "Volumetric images from Magnetic Resonance Imaging (MRI) provide invaluable information in preoperative staging of rectal cancer. Above all, accurate preoperative discrimination between T2 and T3 stages is arguably both the most challenging and clinically significant task for rectal cancer treatment, as chemo-radiotherapy is usually recommended for patients with T3 (or greater) stage cancer. In this study, we present a volumetric convolutional neural network to accurately discriminate T2 from T3 stage rectal cancer with rectal MR volumes. Specifically, we propose 1) a custom ResNet-based volume encoder that models the inter-slice relationship with late fusion (i.e., 3D convolution at the last layer), 2) a bilinear computation that aggregates the resulting features from the encoder to create a volume-wise feature, and 3) a joint minimization of triplet loss and focal loss. With MR volumes of pathologically confirmed T2/T3 rectal cancer, we perform extensive experiments to compare various designs within the framework of residual learning. As a result, our network achieves an AUC of 0.831, which is higher than the reported accuracy of the professional radiologist groups. We believe this method can be extended to other volume analysis tasks.\n"}, {"title": "MRI Reconstruction by Completing Under-sampled K-space Data with Learnable Fourier Interpolation", "abstract": "Magnetic resonance imaging (MRI) acceleration is usually achieved by data undersampling, while reconstruction from undersampled data is a challenging ill-posed problem for data-missing and noisy measurements introduce various artifacts. In recent years, deep learning methods have been extensively studied for MRI reconstruction, and most of work treat the reconstruction problem as a denoising problem or replace the regularization subproblem with a deep neural network (DNN) in an optimization unrolling scheme. In this work, we proposed to directly complete the missing and corrupted k-space data by a specially designed interpolation deep neural networks combined with some convolution layers in both frequency and spatial domains. Specifically, for every missing and corrupted frequency, we use a K\u00e2\u0088\u0092 nearest neighbors estimation with learnable weights. Then, two convolution neural networks (CNNs) are applied to regularize the data in both k-space and image space. The proposed DNN structures have clear interpretability for solving this undersampling problem. Extensive experiments on MRI reconstruction with diverse sampling patterns and ratios, under noiseless and noise settings demonstrate the accuracy of the proposed method compared to other learning based algorithms, while being computationally efficient for both training and reconstruction processes.\n"}, {"title": "mulEEG: A Multi-View Representation Learning on EEG Signals", "abstract": "Modeling effective representations using multiple views that\npositively influence each other is challenging, and the existing\nmethods perform poorly on Electroencephalogram (EEG) signals for sleep-staging tasks. In this paper, we propose a novel multi-view self-supervised method (mulEEG) for unsupervised EEG representation learning. Our method attempts to effectively utilize the complementary information available in multiple views to learn better representations. We introduce diverse loss that further encourages complementary information across multiple views. Our method with no access to labels, beats the supervised training while outperforming multi-view baseline methods on transfer learning experiments carried out on sleep-staging tasks. We posit that our method was able to learn better representations by using complementary multi-views.\n"}, {"title": "Multidimensional Hypergraph on Delineated Retinal Features for Pathological Myopia Task.", "abstract": "Vision-threatening pathological myopia presents several lesions affecting various retinal anatomical structures. Detection approaches, however, either focus on one anatomical feature or are not intentional. This study uses hypergraph learning to modulate delineated retinal anatomical features from fundus images and capitalize on hidden associations between these features. Experiments are conducted to assess prediction performance when targeting a particular anatomical trait versus using a mixture of select anatomical features, and in comparison to a ResNet34-based convolutional neural network classifier. Results indicate better prediction with hypergraph learning on a mix of the delineated features (F1 score 89.75%, AUC score 95.39%). A choroid tessellation segmentation method is also included. \n"}, {"title": "Multi-head Attention-based Masked Sequence Model for Mapping Functional Brain Networks", "abstract": "It has been of great interest in the neuroimaging community to discover brain functional networks (FBNs) based on task functional magnetic resonance imag-ing (tfMRI). A variety of methods have been used to model tfMRI sequences so far, such as recurrent neural network (RNN) and Autoencoder. However, these models are not designed to incorporate the characteristics of tfMRI sequences, and the same signal values at different time points in a fMRI time series may rep-resent different states and meanings. Inspired by cloze learning methods and the human ability to judge polysemous words based on context, we proposed a self-supervised a Multi-head Attention-based Masked Sequence Model (MAMSM), as BERT model uses (Masked Language Modeling) MLM and multi-head atten-tion to learn the different meanings of the same word in different sentences. MAMSM masks and encodes tfMRI time series, uses multi-head attention to cal-culate different meanings corresponding to the same signal value in fMRI se-quence, and obtains context information through MSM pre-training. Furthermore this work redefined a new loss function to extract FBNs according to the task de-sign information of tfMRI data. The model has been applied to the Human Con-nectome Project (HCP) task fMRI dataset and achieves state-of-the-art perfor-mance in brain temporal dynamics, the Pearson correlation coefficient between learning features and task design curves was more than 0.95, and the model can extract more meaningful network besides the known task related brain networks.\n"}, {"title": "Multi-institutional Investigation of Model Generalizability for Virtual Contrast-enhanced MRI Synthesis", "abstract": "The purpose of this study is to investigate the model generalizability using multi-institutional data for virtual contrast-enhanced MRI (VCE-MRI) synthesis. This study presented a retrospective analysis of contrast-free T1-weighted (T1w), T2-weighted (T2w), and gadolinium-based contrast-enhanced T1w MRI (CE-MRI) images of 231 NPC patients enrolled from four institutions. Data from three participating institutions were employed to generate a training and an internal testing set, while data from the remaining institution was employed as an independent external testing set. The multi-institutional data were trained separately (single-institutional model) and jointly (joint-institutional model) and tested using the internal and external sets. The synthetic VCE-MRI was quantitatively evaluated using MAE and SSIM. In addition, a visual qualitative evaluation was performed to assess the quality of synthetic VCE-MRI compared to the ground-truth CE-MRI. Quantitative analyses showed that the joint-institutional model outperformed single-institutional models in both internal and external testing sets, and demonstrated high model generalizability, yielding top-ranked MAE, and SSIM of 71.69 \u00c2\u00b1 21.09 and 0.81 \u00c2\u00b1 0.04 respectively on the external testing set. The qualitative evaluation indicated that the joint-institutional model gave a closer visual approximation between the synthetic VCE-MRI and ground-truth CE-MRI on the external testing set, compared with single-institutional models. The model generalizability for VCE-MRI synthesis was enhanced, both quantitatively and qualitatively, when data from more institutions were involved during model development.\n"}, {"title": "Multimodal Brain Tumor Segmentation Using Contrastive Learning based Feature Comparison with Monomodal Normal Brain Images", "abstract": "Many deep learning (DL) based methods for brain tumor segmentation have been proposed. Most of them put emphasis on elaborating deep network\u00e2\u0080\u0099s internal structure to enhance the capacity of learning tumor-related features, while other valuable related information, such as normal brain appearance, is often ignored. Inspired by the fact that radiologists are often trained to compare with normal tissues when identifying tumor regions, in this paper, we propose a novel brain tumor segmentation framework by adopting normal brain images as reference to compare with tumor brain images in the learned feature space. In this way, tumor-related features can be highlighted and enhanced for accurate tumor segmentation. Considering that the routine tumor brain images are multimodal while the normal brain images are often monomodal, a new contrastive learning based feature comparison module is proposed to solve incomparable issue between features learned from multimodal and monomodal images. In the experiments, both in-house and public (BraTS2019) multimodal tumor brain image datasets are used to evaluate our proposed framework, demonstrating better performance compared to the state-of-the-art methods in terms of Dice score, sensitivity, and Hausdorff distance. Code: https://github.com/hbliu98/CLFC-Brain-Tumor-Segmentation\n"}, {"title": "Multimodal Contrastive Learning for Prospective Personalized Estimation of CT Organ Dose", "abstract": "The increasing frequency of computed tomography (CT) examinations has sparked development of dose reduction techniques to reduce the radiation dose to patients. Optimal dose while maintaining image quality can be achieved through accurate and realistic dose estimates. Unfortunately, existing dosimetric measures are either prohibitively slow or heavily reliant on absorbed dose within a cylindrical phantom, thereby ignoring the impact of patient anatomy and organ radiosensitivity on effective dose. We propose a novel deep learning-based patient-specific CT organ dose estimation method namely, multimodal contrastive learning with Scout images (Scout-MCL). Our proposed Scout-MCL gives accurate and realistic dose estimates in real-time and prospectively, by learning from multi-modal information leveraging image (lateral and frontal scouts) and profile (patient body size). Additionally, the incorporation of an accurately modeled tube current modulation (TCM) enables Scout-MCL to learn realistic dose variations. We evaluate our proposed method on a scout-CT paired scan dataset and show its effectiveness on predicting diverse TCM doses. \n"}, {"title": "Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification", "abstract": "The automatic early diagnosis of prodromal stages of Alzhei-mer\u00e2\u0080\u0099s disease is of great relevance for patient treatment to improve quality of life.  We address this problem as a multi-modal classification task. Multi-modal data provides richer and complementary information. However, existing techniques only consider lower order relations between the data and single/multi-modal imaging data. In this work, we introduce a novel semi-supervised hypergraph learning framework for Alzheimer\u00e2\u0080\u0099s disease diagnosis. Our framework allows for higher-order relations among multi-modal imaging and non-imaging data whilst requiring a tiny labelled set. Firstly, we introduce a dual embedding strategy  for constructing a robust hypergraph that preserve the data semantics. We achieve this by enforcing perturbation invariance at the image and graph levels using a contrastive based mechanism. Secondly, we present a dynamically adjusted hypergraph diffusion model, via a semi-explicit flow, to improve the predictive uncertainty. We demonstrate, through our experiments, that our framework is able to outperform current techniques for Alzheimer\u00e2\u0080\u0099s disease diagnosis.\n"}, {"title": "Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training", "abstract": "Medical vision-and-language pre-training provides a feasible solution to extract effective vision-and-language representations from medical images and texts. However, few studies have been dedicated to this field to facilitate medical vision-and-language understanding. In this paper, we propose a self-supervised learning paradigm with multi-modal masked autoencoders (M$^3$AE), which learn cross-modal domain knowledge by reconstructing missing pixels and tokens from randomly masked images and texts. There are three key designs to make this simple approach work. First, considering the different information densities of vision and language, we adopt different masking ratios for the input image and text, where a considerably larger masking ratio is used for images. Second, we use visual and textual features from different layers to perform the reconstruction to deal with different levels of abstraction in visual and language. Third, we develop different designs for vision and language decoders (i.e., a Transformer for vision and a multi-layer perceptron for language). To perform a comprehensive evaluation and facilitate further research, we construct a medical vision-and-language benchmark including three tasks. Experimental results demonstrate the effectiveness of our approach, where state-of-the-art results are achieved on all downstream tasks. Besides, we conduct further analysis to better verify the effectiveness of different components of our approach and various settings of pre-training. The source code is available at~\\url{https://github.com/zhjohnchan/M3AE}.\n"}, {"title": "Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network", "abstract": "In ophthalmological imaging, multiple imaging systems, such as color fundus, infrared, fluorescein angiography, optical coherence tomography (OCT) or OCT angiography, are often involved to make a diagnosis of retinal disease. Multi-modal retinal registration techniques can assist ophthalmologists by providing a pixel-based comparison of aligned vessel structures in images from different modalities or acquisition times. To this end, we propose an end-to-end trainable deep learning method for multi-modal retinal image registration. Our method extracts convolutional features from the vessel structure for keypoint detection and description and uses a graph neural network for feature matching. The keypoint detection and description network and graph neural network are jointly trained in a self-supervised manner using synthetic multi-modal image pairs and are guided by synthetically sampled ground truth homographies. Our method demonstrates higher registration accuracy as competing methods for our synthetic retinal dataset and generalizes well for our real macula dataset and a public fundus dataset.\n"}, {"title": "Multi-Modal Unsupervised Pre-Training for Surgical Operating Room Workflow Analysis", "abstract": "Data-driven approaches to assist operating room (OR) workflow analysis depend on large curated datasets that are time consuming and expensive to collect. On the other hand, we see a recent paradigm shift from supervised learning to self-supervised and/or unsupervised learning approaches that can learn representations from unlabeled datasets. In this paper, we leverage the unlabeled data captured in robotic surgery ORs and propose a novel way to fuse  the multi-modal data for a single video frame or image. Instead of producing different augmentations (or \u00e2\u0080\u009cviews\u00e2\u0080\u009d) of the same image or video frame which is a common practice in self-supervised learning, we treat the multi-modal data as  different views to train the model in an unsupervised manner via clustering. We compared our method with other state of the art methods and results show the superior performance of our approach on surgical video activity recognition and semantic segmentation. \n"}, {"title": "Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning", "abstract": "Eye trackers can provide visual guidance to sonographers during ultrasound (US) scanning. Such guidance is potentially valuable for less experienced operators to improve their scanning skills on how to manipulate the probe to achieve the desired plane. In this paper, a multimodal guidance approach (Multimodal-GuideNet) is proposed to capture the stepwise dependency between a real-world US video signal, synchronized gaze, and probe motion within a unified framework. To understand the causal relationship between gaze movement and probe motion, our model exploits multitask learning to jointly learn two related tasks: predicting gaze movements and probe signals that an experienced sonographer would perform in routine obstetric scanning. The two tasks are associated by a modality-aware spatial graph to detect the co-occurrence among the multi-modality inputs and share useful cross-modal information. Instead of a deterministic scanning path, Multimodal-GuideNet allows for scanning diversity by estimating the probability distribution of real scans. Experiments performed with three typical obstetric scanning examinations show that the new approach outperforms single-task learning for both probe motion guidance and gaze movement prediction. The prediction can also provide a visual guidance signal with an error rate of less than 10 pixels for a 224x288 US image.\n"}, {"title": "Multiple Instance Learning with Mixed Supervision in Gleason Grading", "abstract": "With the development of computational pathology, deep learning methods for Gleason grading through whole slide images (WSIs) have excellent prospects. Since the size of WSIs is extremely large, the image label usually contains only slide-level label or limited pixel-level labels. The current mainstream approach adopts multi-instance learning to predict Gleason grades. However, some methods only considering the slide-level label ignore the limited pixel-level labels containing rich local information. Furthermore, the method of additionally considering the pixel-level labels ignores the inaccuracy of pixel-level labels. To address these problems, we propose a mixed supervision Transformer based on the multiple instance learning framework. The model utilizes both slide-level label and instance-level labels to achieve more accurate Gleason grading at the slide level. The impact of inaccurate instance-level labels is further reduced by introducing an efficient random masking strategy in the mixed supervision training process. We achieve the state-of-the-art performance on the SICAPv2 dataset, and the visual analysis shows the accurate prediction results of instance level. The source code is available at https://github.com/bianhao123/Mixed_supervision.\n"}, {"title": "Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness", "abstract": "Magnetic Resonance Spectroscopic Imaging (MRSI) is a valuable tool for studying metabolic activities in the human body, but the current applications are limited to low spatial resolutions. The existing deep learning-based MRSI super-resolution methods require training a separate network for each upscaling factor, which is time-consuming and memory inefficient. We tackle this multi-scale super-resolution problem using a Filter Scaling strategy that modulates the convolution filters based on the upscaling factor, such that a single network can be used for various upscaling factors. Observing that each metabolite has distinct spatial characteristics, we also modulate the network based on the specific metabolite. Furthermore, our network is conditioned on the weight of adversarial loss so that the perceptual sharpness of the super-resolved metabolic maps can be adjusted within a single network. We incorporate these network conditionings using a novel Multi-Conditional Module. The experiments were carried out on a 1H-MRSI dataset from 15 high-grade glioma patients. Results indicate that the proposed network achieves the best performance among several multi-scale super-resolution methods and can provide super-resolved metabolic maps with adjustable sharpness. \n"}, {"title": "Multiscale Unsupervised Retinal Edema Area Segmentation in OCT Images", "abstract": "Retinal edema area, which can be observed in the non-invasive optical coherence tomography image, is essential for the diagnosis and treatment of many retinal diseases. Due to the demand of professional knowledge for its annotation, acquiring sufficient labeled data for the usual data-driven learning-based approaches is time-consuming and laborious. To alleviate the intensive workload for manual labeling, unsupervised learning technique has been widely explored and adopted in different applications. However, the corresponding research in medical image segmentation is still limited and the performance is unsatisfactory. In this paper, we propose a novel unsupervised segmentation framework, which consists of two stages: the image-level clustering to group images into different categories and the pixel-level segmentation which leverages the guidance of the clustering network. Based on the observation that smaller lesions are more obvious on large scale images with detail texture information and larger lesions are easier to capture on small scale images for the large field-of-view, we introduce multiscale information into both stages through a scale-invariant regularization and a multiscale Class Activation Map (CAM) fusing strategy, respectively. Experiments on the public retinal dataset show that the proposed framework achieves a 76.28% Dice score without any supervision, which outperforms state-of-the-art unsupervised approaches by a large margin (more than 20% improvement in Dice score).\n"}, {"title": "Multi-site Normative Modeling of Diffusion Tensor Imaging Metrics Using Hierarchical Bayesian Regression", "abstract": "Multi-site imaging studies can increase statistical power and improve the reproducibility and generalizability of findings, yet data often need to be harmonized. One alternative to data harmonization in the normative model-ing setting is Hierarchical Bayesian Regression (HBR), which overcomes some of the weaknesses of data harmonization. Here, we test the utility of three model types, i.e., linear, polynomial and b-spline - within the norma-tive modeling HBR framework - for multi-site normative modeling of diffu-sion tensor imaging (DTI) metrics of the brain\u00e2\u0080\u0099s white matter microstruc-ture, across the lifespan. These models of age dependencies were fitted to cross-sectional data from over 1,300 healthy subjects (age range: 2-80 years), scanned at eight sites in diverse geographic locations. We found that the polynomial and b-spline fits were better suited for modeling relation-ships of DTI metrics to age, compared to the linear fit. To illustrate the method, we also apply it to detect microstructural brain differences in carri-ers of rare genetic copy number variants, noting how model complexity can impact findings.\n"}, {"title": "Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network", "abstract": "Lung nodules can be an alarming precursor to potential lung cancer. Missed nodule detections during chest radiograph analysis remains a common challenge among thoracic radiologists. In this work, we present a multi-task lung nodule detection algorithm for chest radiograph analysis. Unlike past approaches, our algorithm predicts a global-level label indicating nodule presence along with local-level labels predicting nodule locations using a Dual Head Network (DHN). We demonstrate the favorable nodule detection performance that our multi-task formulation yields in comparison to conventional methods. In addition, we introduce a novel Dual Head Augmentation (DHA) strategy tailored for DHN, and we demonstrate its significance in further enhancing global and local nodule predictions.\n"}, {"title": "Multi-task video enhancement for dental interventions", "abstract": "A microcamera firmly attached to a dental handpiece allows dentists to continuously monitor the progress of conservative dental procedures. Video enhancement in video-assisted dental interventions alleviates low-light, noise, blur, and camera handshakes that collectively degrade visual comfort. To this end, we introduce a novel deep network for multi-task video enhancement that enables macro-visualization of dental scenes. In particular, the proposed network jointly leverages video restoration and temporal alignment in a multi-scale manner for effective video enhancement. Our experiments on videos of natural teeth in phantom scenes demonstrate that the proposed network achieves state-of-the-art results in multiple tasks with near real-time processing. We release Vident-lab at https://doi.org/10.34808/1jby-ay90, the first dataset of dental videos with multi-task labels to facilitate further research in relevant video processing applications.\n"}, {"title": "Multi-TransSP: Multimodal Transformer for Survival Prediction of Nasopharyngeal Carcinoma Patients", "abstract": "Nasopharyngeal carcinoma (NPC) is a malignant tumor that often occurs in Southeast Asia and southern China. Since there is a need for a more precise personalized therapy plan that depends on accurate prognosis prediction, it may be helpful to predict patients\u00e2\u0080\u0099 overall survival (OS) based on clinical data. However, most of the current deep learning (DL) based methods which use a single modality fail to effectively utilize amount of multimodal data of patients, causing inaccurate survival prediction. In view of this, we propose a Multimodal Transformer for Survival Prediction (Multi-TransSP) of NPC patients that uses tabular data and computed tomography (CT) images jointly. Taking advantage of both convolutional neural network and Transformer, the architecture of our network is comprised of a multimodal CNN-Based Encoder and a Transformer-Based Encoder. Particularly, the CNN-Based Encoder can learn rich information from specific modalities and the Transformer-Based Encoder is able to fuse multimodal feature. Our model automatically gives the final prediction of OS with a concordance index (CI) of 0.6941 on our in-house dataset, and our model significantly outperforms other methods using any single source of data or previous multimodal frameworks. Code is available at https://github.com/gluglurice/Multi-TransSP.\n"}, {"title": "Multi-view Local Co-occurrence and Global Consistency Learning Improve Mammogram Classification Generalisation", "abstract": "When analysing screening mammograms, radiologists can naturally process information across two ipsilateral views of each breast, namely the cranio-caudal (CC) and mediolateral-oblique (MLO) views. These multiple related images provide complementary diagnostic information and can improve the radiologist\u00e2\u0080\u0099s classification accuracy. Unfortunately, most existing deep learning systems, trained with globally-labelled images, lack the ability to jointly analyse and integrate global and local information from these multiple views. By ignoring the potentially valuable information present in multiple images of a screening episode, one limits the potential accuracy of these systems. Here, we propose a new multi-view global-local analysis method that mimics the radiologist\u00e2\u0080\u0099s reading procedure, based on a global consistency learning and local co-occurrence learning of ipsilateral views in mammograms. Extensive experiments show that our model outperforms competing methods, in terms of classification accuracy and generalisation, on a large-scale private dataset and two publicly available datasets, where models are exclusively trained and tested with global labels. \n"}, {"title": "MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts", "abstract": "While self-supervised learning (SSL) algorithms have been widely used to pre-train deep models, few efforts have been done to improve representation learning of X-ray image analysis with SSL pre-trained models. In this work, we study a novel self-supervised pre-training pipeline, namely Multi-task Self-supervised Continual Learning (MUSCLE), for multiple medical imaging tasks, such as classification and segmentation, using X-ray images collected from multiple body parts, including heads, lungs, and bones. Specifically, MUSCLE aggregates X-rays collected from multiple body parts for MoCo-based representation learning, and adopts a well-designed continual learning (CL) procedure to further pre-train the backbone subject various X-ray analysis tasks jointly. Certain strategies for image pre-processing, learning schedules, and regularization have been used to solve data heterogeneity, over-fitting, and catastrophic forgetting problems for multi-task/dataset learning in MUSCLE. We evaluate MUSCLE using 9 real-world X-ray datasets with various tasks, including pneumonia classification, skeletal abnormality classification, lung segmentation, and tuberculosis (TB) detection. Comparisons against other pre-trained models confirm the proof-of-concept that self-supervised multi-task/dataset continual pre-training could boost the performance of X-ray image analysis.\n"}, {"title": "NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction", "abstract": "This paper proposes a novel and fast self-supervised solution for sparse-view  CBCT reconstruction (Cone Beam Computed Tomography) that requires no external training data. Specifically, the desired attenuation coefficients are represented as a continuous function of 3D spatial coordinates, parameterized by a fully-connected deep neural network. We synthesize projections discretely and train the network by minimizing the error between real and synthesized projections. A learning-based encoder entailing hash coding is adopted to help the network capture high-frequency details. This encoder outperforms the commonly used frequency-domain encoder in terms of having higher performance and efficiency, because it exploits the smoothness and sparsity of human organs. Experiments have been conducted on both human organ and phantom datasets. The proposed method achieves state-of-the-art accuracy and spends reasonably short computation time. \n"}, {"title": "NerveFormer: A Cross-Sample Aggregation Network for Corneal Nerve Segmentation", "abstract": "The segmentation of corneal nerves in corneal confocal microscopy (CCM) is of great importance to the quantification of clinical parameters in the diagnosis of eye-related diseases and systematic diseases.\nExisting works mainly use convolutional neural networks to improve the segmentation accuracy, while further improvement is needed to mitigate the nerve discontinuity and noise interference.\nIn this paper, we propose a novel corneal nerve segmentation network, named NerveFormer, to resolve the above-mentioned limitations. The proposed NerveFormer includes a Deformable and External Attention Module (DEAM), which exploits the Transformer-based Deformable Attention (TDA) and  External Attention (TEA) mechanisms. TDA is introduced to explore the local internal nerve features in a single CCM, while TEA is proposed to model global external nerve features across different CCM \nimages. Specifically, to efficiently fuse the internal and external nerve features, TDA obtains the \\textit{query} set required by TEA, thereby strengthening the characterization ability of TEA.\nTherefore, the proposed model aggregates the learned features from both single-sample and cross-sample, allowing for better extraction of corneal nerve features across the whole dataset. Experimental results on two public CCM datasets show that our proposed method achieves state-of-the-art performance, especially in terms of segmentation continuity and noise discrimination.\n"}, {"title": "NestedFormer: Nested Modality-Aware Transformer for Brain Tumor Segmentation", "abstract": "Multi-modal MR imaging is routinely used in clinical practice to diagnose and investigate brain tumors by providing rich complementary information. Previous multi-modal MRI segmentation methods usually perform modal fusion by concatenating multi-modal MRIs at an early/middle stage of the network, which hardly explores non-linear dependencies between modalities. In this work, we propose a novel Nested Modality-Aware Transformer (NestedFormer) to explicitly explore the intra-modality and inter-modality relationships of multi-modal MRIs for brain tumor segmentation. Built on the transformer-based multi-encoder and single-decoder structure, we perform nested multi-modal fusion for high-level representations of different modalities and apply modality-sensitive gating (MSG) at lower scales for more effective skip connections. Specifically, the multi-modal fusion is conducted in our proposed Nested Modality-aware Feature Aggregation (NMaFA) module, which enhances long-term dependencies within individual modalities via a tri-orientated spatial-attention transformer, and further complements key contextual information among modalities via a cross-modality attention transformer. Extensive experiments on BraTS2020 benchmark and a private meningiomas segmentation (MeniSeg) dataset show that the Nest- edFormer clearly outperforms the state-of-the-arts. The code is available at https://github.com/920232796/NestedFormer.\n"}, {"title": "Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis", "abstract": "The human annotations are imperfect, especially when produced by junior practitioners. Multi-expert consensus is usually regarded as golden standard, while this annotation protocol is too expensive to implement in many real-world projects. In this study, we propose a method to refine human annotation, named Neural Annotation Refinement (NeAR). It is based on a learnable implicit function, which decodes a latent vector into represented shape. By integrating the appearance as an input of implicit functions, the appearance-aware NeAR fixes the annotation artefacts. Our method is demonstrated on the application of adrenal gland analysis. We first show that the NeAR can repair distorted golden standards on a public adrenal gland segmentation dataset. Besides, we develop a new Adrenal gLand ANalysis (ALAN) dataset with the proposed NeAR, where each case consists of a 3D shape of adrenal gland and its diagnosis label (normal vs. abnormal) assigned by experts. We show that models trained on the shapes repaired by the NeAR can diagnose adrenal glands better than the original ones. The ALAN dataset will be open-source, with 1,584 shapes for adrenal gland diagnosis, which serves as a new benchmark for medical shape analysis. Code and dataset are available at https://github.com/M3DV/NeAR.\n"}, {"title": "Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery", "abstract": "Reconstruction of the soft tissues in robotic surgery from endoscopic stereo videos is important for many applications such as intra-operative navigation and image-guided robotic surgery automation. Previous works on this task mainly rely on SLAM-based approaches, which struggle to handle complex surgical scenes. Inspired by recent progress in neural rendering, we present a novel framework for deformable tissue reconstruction from binocular captures in robotic surgery under the single-viewpoint setting. Our framework adopts dynamic neural radiance fields to represent deformable surgical scenes in MLPs and optimize shapes and deformations in a learning-based manner. In addition to non-rigid deformations, tool occlusion and poor 3D clues from a single viewpoint are also particular challenges in soft tissue reconstruction. To overcome these difficulties, we present a series of strategies of tool mask-guided ray casting, stereo depth-cueing ray marching and stereo depth-supervised optimization. With experiments on DaVinci robotic surgery videos, our method significantly outperforms the current state-of-the-art reconstruction method for handling various complex non-rigid deformations. To our best knowledge, this is the first work leveraging neural rendering for surgical scene 3D reconstruction with remarkable potential demonstrated.\n"}, {"title": "Neuro-RDM: An Explainable Neural Network Landscape of Reaction-Diffusion Model for Cognitive Task Recognition", "abstract": "The functional neural imaging technology sheds new light on characterizing the neural activity at each brain region and the information exchange from region to region. However, it is challenging to reverse-engineer the working mechanism of brain function and behavior through the evolving functional neuroimages. In this work, we conceptualize that the ensemble of evolving neuronal synapses forms a dynamic system of functional connectivity, where the system behavior (aka. brain state) of such a reaction-diffusion process can be formulated by a set of trainable PDEs (partial differential equations). To that end, we first introduce a PDE-based reaction-diffusion model (RDM) to jointly characterize the propagation of brain states throughout the functional brain network as well as the non-linear interac-tion between brain state and neural activity manifested in the BOLD (blood-oxygen-level-dependent) signals. Next, we translate the diffusion and reaction processes formulated in the PDE into a graph neural network, where the driving force is to establish the mapping from the evolution of brain states to the known cognitive tasks. By doing so, the layer-by-layer learning scenario allows us to not only fine-tune the RDM model for predicting cognitive tasks but also explain how brain functions support cognitive status with a great neuroscience insight. We have evaluated our proof-of-concept approach on both simulated data and functional neuroimages from HCP (Human Connectome Project) database. Since our neuro-RDM combines the power of deep learning and insight of dynamic systems, our method has significantly improved recognition performance in terms of accuracy and robustness to the non-neuronal noise, compared with the conventional deep learning models.\n"}, {"title": "Noise transfer for unsupervised domain adaptation of retinal OCT images", "abstract": "Optical coherence tomography (OCT) imaging from different camera devices causes challenging domain shifts and can cause a severe drop in accuracy for machine learning models. In this work, we introduce a minimal noise adaptation method based on a singular value decomposition (SVDNA) to overcome the domain gap between target domains from three different device manufacturers in retinal OCT imaging. Our method utilizes the difference in noise structure to successfully bridge the domain gap between different OCT devices and transfer the style from unlabeled target domain images to source images for which manual annotations are available. We demonstrate how this method, despite its simplicity, compares or even outperforms state-of-the-art unsupervised domain adaptation methods for semantic segmentation on a public OCT dataset. SVDNA can be integrated with just a few lines of code into the augmentation pipeline of any network which is in contrast\nto many state-of-the-art domain adaptation methods which often need to change the underlying model architecture or train a separate style transfer model. The full code implementation for SVDNA will be made available at https://github.com/ValentinKoch/SVDNA.\n"}, {"title": "Noise2SR: Learning to Denoise from Super-Resolved Single Noisy Fluorescence Image", "abstract": "Fluorescence microscopy is a key driver to promote discoveries of biomedical research. However, with the limitation of microscope hardware and characteristics of the observed samples, the fluorescence microscopy images are susceptible to noise. Recently, a few self-supervised deep learning (DL) denoising methods have been proposed. However, the training efficiency and denoising performance of existing methods are relatively low in real scene noise removal. To address this issue, this paper proposed self-supervised image denoising method Noise2SR (N2SR) to train a simple and effective image denoising model based on single noisy observation. Our Noise2SR denoising model is designed for training with\npaired noisy images of different dimensions. Benefiting from this training strategy, Noise2SR is more efficiently self-supervised and able to restore more image details from a single noisy observation. Experimental results of simulated noise and real microscopy noise removal show that Noise2SR outperforms two blind-spot based self-supervised deep learning image denoising methods.We envision that Noise2SR has the potential to improve more other kind of scientific imaging quality.\n"}, {"title": "Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning", "abstract": "Deformable image registration is a crucial step in medical image analysis for finding a non-linear spatial transformation between a pair of fixed and moving images. Deep registration methods based on Convolutional Neural Networks (CNNs) have been widely used as they can perform image registration in a fast and end-to-end manner. However, these methods usually have limited performance for image pairs with large deformations. Recently, iterative deep registration methods have been used to alleviate this limitation, where the transformations are iteratively learned in a coarse-to-fine manner. However, iterative methods inevitably prolong the registration runtime, and tend to learn separate image features for each iteration, which hinders the features from being leveraged to facilitate the registration at later iterations. In this study, we propose a Non-Iterative Coarse-to-finE registration Network (NICE-Net) for deformable image registration. In the NICE-Net, we propose: (i) a Single-pass Deep Cumulative Learning (SDCL) decoder that can cumulatively learn coarse-to-fine transformations within a single pass (iteration) of the network, and (ii) a Selectively-propagated Feature Learning (SFL) encoder that can learn common image features for the whole coarse-to-fine registration process and selectively propagate the features as needed. Extensive experiments on six public datasets of 3D brain Magnetic Resonance Imaging (MRI) show that our proposed NICE-Net can outperform state-of-the-art iterative deep registration methods while only requiring similar runtime to non-iterative methods.\n"}, {"title": "Nonlinear Conditional Time-varying Granger Causality of Task fMRI via Deep Stacking Networks and Adaptive Convolutional Kernels", "abstract": "Time-varying Granger causality refers to patterns of causal relation-ships that vary over time between brain functional time series at distinct source and target regions. It provides rich information about the spatiotemporal structure of brain activity that underlies behavior. Current methods for this problem fail to quantify nonlinear relationships in source-target relationships, and require ad hoc setting of relationship time lags. This paper proposes deep stacking networks (DSNs), with adaptive convolutional kernels (ACKs) as component parts, to ad-dress these challenges. The DSNs use convolutional neural networks to estimate nonlinear source-target relationships, ACKs allow these relationships to vary over time, and time lags are estimated by analysis of ACKs coefficients. When applied to synthetic data and data simulated by the STANCE fMRI simulator, the method identified ground-truth time-varying causal relationships and time lags more robustly than competing methods. The method also identified more biologically-plausible causal relationships in a real-world task fMRI dataset than a competing method. Our method is promising for modeling complex functional relationships within brain networks.\n"}, {"title": "Nonlinear Regression of Remaining Surgical Duration via Bayesian LSTM-based Deep Negative Correlation Learning", "abstract": "In this paper, we address the problem of estimating remaining surgical duration (RSD) from surgical video frames. we propose a Bayesian long short-term memory (LSTM) network-based Deep Negative Correlation Learning approach called BD-Net for accurate regression of RSD prediction as well as estimating prediction uncertainty. Our method aims to extract discriminative visual features from surgical video frames and modeling the temporal dependencies among frames to improve the RSD prediction accuracy. To this end, we propose to ensemble a group of Bayesian LSTMs on top of a backbone network by the way of deep negative correlation learning (DNCL). More specifically, we deeply learn a pool of decorrelated Bayesian regressors with sound generalization capabilities through managing their intrinsic diversities. BD-Net is simple and efficient. After training, it can produce both RSD prediction and uncertainty estimation in a single inference run. We demonstrate the efficacy of BD-Net on a public video dataset containing 101 cataract surgeries. The experimental results show that the proposed BD-Net achieves better results than the state-of-the-art (SOTA) methods.\n"}, {"title": "NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification", "abstract": "Real-world large-scale medical image analysis (MIA) datasets have three challenges: 1) they contain noisy-labelled samples that affect training convergence and generalisation, 2) they usually have an imbalanced distribution of samples per class, and 3) they normally comprise a multi-label problem, where samples can have multiple diagnoses. Current approaches are commonly trained to solve a subset of those problems, but we are unaware of methods that address the three problems simultaneously. In this paper, we propose a new training module called Non-Volatile Unbiased Memory (NVUM), which non-volatility stores running average of model logits for a new regularization loss on noisy multi-label problem. We further unbias the classification prediction in NVUM update for imbalanced learning problem. We run extensive experiments to evaluate NVUM on new benchmarks proposed by this paper, where training is performed on noisy multi-label imbalanced chest X-ray (CXR) training sets, formed by Chest-Xray14 and CheXpert, and the testing is performed on the clean multi-label CXR datasets OpenI and PadChest. Our method outperforms previous state-of-the-art CXR classifiers and previous methods that can deal with noisy labels on all evaluations. Our code is available at this https://github.com/FBLADL/NVUM\n"}, {"title": "On Surgical Planning of Percutaneous Nephrolithotomy with Patient-Specific CTRs", "abstract": "Percutaneous nephrolithotomy (PCNL) is considered a first-choice minimally invasive procedure for treating kidney stones larger than 2 cm. This treatment modality yields higher stone-free rates than other minimally invasive techniques and is employed when extracorporeal shock wave lithotripsy or uteroscopy, are unsuccessful or infeasible. Using this technique, surgeons create a tract through which a scope is inserted for gaining access to the stones. Traditional PCNL tools, however, present limited maneuverability, may require multiple punctures and often lead to excessive torquing of the instruments which can damage the kidney parenchyma and thus increase the risk of hemorrhage. We approach this problem by proposing a nested optimization-driven scheme for determining a single tract surgical plan along which a patient-specific concentric-tube robot (CTR) is deployed so as to enhance manipulability along the most dominant directions of the stone presentations. The approach is illustrated with eight sets of clinical data from patients who underwent PCNL. The simulated results may set the stage for achieving higher stone-free rates through single tract PCNL interventions while decreasing blood loss.\n"}, {"title": "On the Dataset Quality Control for Image Registration Evaluation", "abstract": "Current registration evaluations typically compute target registration error using manually annotated datasets. As a result, the quality of landmark annotations is crucial for unbiased comparisons. Even though some data providers claim to have mitigated the inter-observer variability by having multiple raters, quality control such as a third-party screening can still be reassuring for intended users. Examining the landmark quality for neurosurgical datasets (RESECT and BITE) poses specific challenges. In this study, we applied the variogram, which is a tool extensively used in geostatistics, to convert 3D landmark distributions into an intuitive 2D representation. This allowed us to identify potential problematic cases efficiently so that they could be examined by experienced radiologists. In both the RESECT and BITE datasets, we identified and confirmed a small number of landmarks with potential localization errors and found that, in some cases, the landmark distribution was not ideal for an unbiased assessment of non-rigid registration errors. Under Discussion, we provide some constructive suggestions for improving the utility of publicly available annotated data.\n"}, {"title": "On the Uncertain Single-View Depths in Colonoscopies", "abstract": "Estimating depth information from endoscopic images is a\npre-requisite for a wide set of AI-assisted technologies, such as accurate\nlocalization and measurement of tumors, or identification of non-inspected\nareas. As the domain specificity of colonoscopies \u00e2\u0080\u0093deformable\nlow-texture environments with fluids, poor lighting conditions and abrupt\nsensor motions\u00e2\u0080\u0093 pose challenges to multi-view 3D reconstructions, single-view\ndepth learning stands out as a promising line of research. Depth\nlearning can be extended in a Bayesian setting, which enables continual\nlearning, improves decision making and can be used to compute\nconfidence intervals or quantify uncertainty for in-body measurements.\nIn this paper, we explore for the first time Bayesian deep networks for\nsingle-view depth estimation in colonoscopies. Our specific contribution\nis two-fold: 1) an exhaustive analysis of scalable Bayesian networks for\ndepth learning in different datasets, highlighting challenges and conclusions\nregarding synthetic\u00e2\u0080\u0093to\u00e2\u0080\u0093real domain changes and supervised vs. self-supervised\nmethods; and 2) a novel teacher-student approach to deep\ndepth learning that takes into account the teacher uncertainty.\n"}, {"title": "One-Shot Segmentation of Novel White Matter Tracts via Extensive Data Augmentation", "abstract": "Deep learning based methods have achieved state-of-the-art performance for automated white matter (WM) tract segmentation. In these methods, the segmentation model needs to be trained with a large number of manually annotated scans, which can be accumulated throughout time. When novel WM tracts, i.e., tracts not included in the existing annotated WM tracts, are to be segmented, additional annotations of these novel WM tracts need to be collected. Since tract annotation is time-consuming and costly, it is desirable to make only a few annotations of novel WM tracts for training the segmentation model, and previous work has addressed this problem by transferring the knowledge learned for segmenting existing WM tracts to the segmentation of novel WM tracts. However, accurate segmentation of novel WM tracts can still be challenging in the one-shot setting, where only one scan is annotated for the novel WM tracts. In this work, we explore the problem of one-shot segmentation of novel WM tracts. Since in the one-shot setting the annotated training data is extremely scarce, based on the existing knowledge transfer framework, we propose to further perform extensive data augmentation for the single annotated scan, where synthetic annotated training data is produced. We have designed several different strategies that mask out regions in the single annotated scan for data augmentation. To avoid learning from potentially conflicting information in the synthetic training data produced by different data augmentation strategies, we choose to perform each strategy separately for network training and obtain multiple segmentation models. Then, the segmentation results given by these models are ensembled for the final segmentation of novel WM tracts. Our method was evaluated on public and in-house datasets. The experimental results show that our method improves the accuracy of one-shot segmentation of novel WM tracts.\n"}, {"title": "Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images", "abstract": "Developing an AI-assisted gland segmentation method from histology images is critical for automatic cancer diagnosis and prognosis; however, the high cost of pixel-level annotations hinders its applications to broader diseases. Existing weakly-supervised semantic segmentation methods in computer vision achieve degenerative results for gland segmentation, since the characteristics and problems of glandular datasets are different from general object datasets. We observe that, unlike natural images, the key problem with histology images is the confusion of classes owning to morphological homogeneity and low color contrast among different tissues. To this end, we propose a novel method \\emph{Online Easy Example Mining} (OEEM) that encourages the network to focus on credible supervision signals rather than noisy signals, therefore mitigating the influence of inevitable false predictions in pseudo-masks. According to the characteristics of glandular datasets, we design a strong framework for gland segmentation. Our results exceed many fully-supervised methods and weakly-supervised methods for gland segmentation over 4.6% and 6.04% at mIoU, respectively.\n"}, {"title": "Online Reflective Learning for Robust Medical Image Segmentation", "abstract": "Deep segmentation models often face the failure risks when the testing image presents unseen distributions. Improving model robustness against these risks is crucial for the large-scale clinical application of deep models. In this study, inspired by human learning cycle, we propose a novel online reflective learning framework (RefSeg) to improve segmentation robustness. Based on the reflection-on-action conception, our RefSeg firstly drives the deep model to take action to obtain semantic segmentation. Then, RefSeg triggers the model to reflect itself. Because making deep models realize their segmentation failures during testing is challenging, RefSeg synthesizes a realistic proxy image from the semantic mask to help deep models build intuitive and effective reflections. This proxy translates and emphasizes the segmentation flaws. By maximizing the structural similarity between the raw input and the proxy, the reflection-on-action loop is closed with segmentation robustness improved. RefSeg runs in the testing phase and is general for segmentation models. Extensive validation on three medical image segmentation tasks with a public cardiac MR dataset and two in-house large ultrasound datasets show that our RefSeg remarkably improves model robustness and reports state-of-the-art performance over strong competitors.\n"}, {"title": "OnlyCaps-Net, a capsule only based neural network for 2D and 3D semantic segmentation", "abstract": "Since their introduction by Sabour et al., capsule networks have been extended to 2D semantic segmentation with the introduction of convolutional capsules. While extended further to 3D semantic segmentation when mixed with Convolutional Neural Networks (CNNs), no capsule-only network (to the best of our knowledge) has been able to reach CNNs\u00e2\u0080\u0099 accuracy on multilabel segmentation tasks. In this work, we propose OnlyCaps-Net, the first competitive capsule-only network for 2D and 3D multi-label semantic segmentation. OnlyCaps-Net improves both capsules\u00e2\u0080\u0099 accuracy and inference speed by replacing Sabour et al. squashing with the introduction of two novel squashing functions, i.e. softsquash or unitsquash, and the iterative routing with a new parameter free single pass routing, i.e. unit routing. Additionally, OnlyCaps-Net introduces a new parameter efficient convolutional capsule type, i.e. depthwise separable convolutional capsule.\n"}, {"title": "Only-Train-Once MR Fingerprinting for Magnetization Transfer Contrast Quantification", "abstract": "Magnetization transfer contrast magnetic resonance fingerprinting (MTC-MRF) is a novel quantitative imaging technique that simultaneously measures several tissue parameters of semisolid macromolecule and free bulk water. In this study, we propose an Only-Train-Once MR fingerprinting (OTOM) framework that estimates the free bulk water and MTC tissue parameters from MR fingerprints regardless of MRF schedule, thereby avoiding time-consuming process such as generation of training dataset and network training according to each MRF schedule. A recurrent neural network is designed to cope with two types of variants of MRF schedules: 1) various lengths and 2) various patterns. Experiments on digital phantoms and in vivo data demonstrate that our approach can achieve accurate quantification for the water and MTC parameters with multiple MRF schedules. Moreover, the proposed method is in excellent agreement with the conventional deep learning and fitting methods. The flexible OTOM framework could be an efficient tissue quantification tool for various MRF protocols.\n"}, {"title": "Opinions Vary? Diagnosis First!", "abstract": "With the advancement of deep learning techniques, an increasing number of methods have been proposed for optic disc and cup (OD/OC) segmentation from the fundus images. Clinically, OD/OC segmentation is often annotated by multiple clinical experts to mitigate the personal bias. However, it is hard to train the automated deep learning models on multiple labels. A common practice to tackle the issue is majority vote, e.g., taking the average of multiple labels. However such a strategy ignores the different expertness of medical experts. Motivated by the observation that OD/OC segmentation is often used for the glaucoma diagnosis clinically, in this paper, we propose a novel strategy to fuse the multi-rater OD/OC segmentation labels via the glaucoma diagnosis performance. Specifically, we assess the expertness of each rater through an attentive glaucoma diagnosis network. For each rater, its contribution for the diagnosis will be reflected as an expertness map. To ensure the expertness maps are general for different glaucoma diagnosis models, we further propose an Expertness Generator (ExpG) to eliminate the high-frequency components in the optimization process.Based on the obtained expertness maps, the multi-rater labels can be fused as a single ground-truth which we dubbed as Diagnosis First Ground-truth (DiagFirstGT). Experimental results show that by using DiagFirstGT as ground-truth, OD/OC segmentation networks will predict the masks with superior glaucoma diagnosis performance.\n"}, {"title": "Opportunistic Incidence Prediction of Multiple Chronic Diseases from Abdominal CT Imaging Using Multi-Task Learning", "abstract": "Opportunistic computed tomography (CT) analysis is a paradigm where CT scans that have already been acquired for routine clinical questions are reanalyzed for disease prognostication, typically aided by machine learning. While such techniques for opportunistic use of abdominal CT scans have been implemented for assessing the risk of a handful of individual disorders, their prognostic power in simultaneously assessing multiple chronic disorders has not yet been evaluated. In this retrospective study of 9,154 patients, we demonstrate that we can effectively assess 5-year incidence of chronic kidney disease (CKD), diabetes mellitus (DM), hypertension (HT), ischemic heart disease (IHD), and osteoporosis (OST) using single already-acquired abdominal CT scans. We demonstrate that a shared multi-planar CT input, consisting of an axial CT slice occurring at the L3 vertebral level, as well as carefully selected sagittal and coronal slices, enables accurate future disease incidence prediction. Furthermore, we demonstrate that casting this shared CT input into a multi-task approach is particularly valuable in the low-label regime. With just 10\\% of labels for our diseases of interest, we recover nearly 99% of fully supervised AUROC performance, representing an improvement over single-task learning.\n"}, {"title": "Optimal MRI Undersampling Patterns for Pathology Localization", "abstract": "We investigate MRI acceleration strategies for the benefit of downstream image analysis tasks. Specifically, we propose to optimize the k-space undersampling patterns according to how well a sought-after pathology could be segmented or localized in the reconstructed images. We study the effect of the proposed paradigm on the segmentation task using two classical labeled medical datasets, and on the task of pathology visualization within the bounding boxes, using the recently released FastMRI+ annotations. We demonstrate a noticeable improvement of the target metrics when the sampling pattern is optimized, e.g., for the segmentation problem at x16 acceleration, we report up to 12% improvement in Dice score over the other undersampling strategies.\n"}, {"title": "Optimal Transport based Ordinal Pattern Tree Kernel for Brain Disease Diagnosis", "abstract": "Finding a faithful connection pattern of brain network is a challenging task for most of the existing methods in brain network analysis. To process this problem, we propose a novel method called ordinal pattern tree (OPT) for representing the connection pattern of network by using the ordinal pattern relationships of edge weights in brain network. On OPT, nodes are connected by ordinal edges which make nodes have hierarchical structures. The changes of edge weights in brain network will affect ordinal edges and result in the differences of OPTs. We further leverage optimal transport distances to measure the transport costs between the nodes on the paired of OPTs. Based on these optimal transport distances, we develop a new graph kernel called optimal transport based ordinal pattern tree kernel to measure the similarity between the paired brain networks. To evaluate the effectiveness of the proposed method, we perform classification and regression experiments in functional magnetic resonance imaging data of brain diseases. The experimental results demonstrate that our proposed method can achieve significant improvement compared with the state-of-the-art graph kernel methods on classification and regression tasks.\n"}, {"title": "ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans", "abstract": "Most of the existing object detection works are based on the bounding box annotation: each object has a precise annotated box. However, for rib fractures, the bounding box annotation is very labor-intensive and time-consuming because radiologists need to investigate and annotate the rib fractures on a slice-by-slice basis. Although a few studies have proposed weakly-supervised methods or semi-supervised methods, they could not handle different forms of supervision simultaneously. In this paper, we proposed a novel omni-supervised object detection network, which can exploit multiple different forms of annotated data to further improve the detection performance. Specifically, the proposed network contains an omni-supervised detection head, in which each form of annotation data corresponds to a unique classification branch. Furthermore, we proposed a dynamic label assignment strategy for different annotated forms of data to facilitate better learning for each branch. Moreover, we also design a confidence-aware classification loss to emphasize the samples with high confidence and further improve the model\u00e2\u0080\u0099s performance. Extensive experiments conducted on the testing dataset show our proposed method outperforms other state-of-the-art approaches consistently, demonstrating the efficacy of deep omni-supervised learning on improving rib fracture detection performance.\n"}, {"title": "Orientation-guided Graph Convolutional Network for Bone Surface Segmentation", "abstract": "Due to imaging artifacts and low signal-to-noise ratio in ultrasound images, automatic bone surface segmentation networks often produce fragmented predictions that can hinder the success of ultrasound (US)-guided computer-assisted surgical procedures. Existing pixel-wise predictions often fail to capture the accurate topology of bone tissues due to a lack of supervision to enforce connectivity. In this work, we propose an orientation-guided graph convolutional network  to improve connectivity while segmenting the bone surface. We also propose an additional supervision on the orientation of the bone surface to further impose connectivity. We validated our approach on 1042 in vivo US scans of femur, knee, spine, and distal radius. Our approach improves over the state-of-the-art methods by 5.01% in connectivity metric.\n"}, {"title": "Orientation-Shared Convolution Representation for CT Metal Artifact Learning", "abstract": "During X-ray computed tomography (CT) scanning, metallic implants carrying with patients often lead to adverse artifacts in the captured CT images and then impair the clinical treatment. Against this metal artifact reduction (MAR) task, the existing deep-learning-based methods have gained promising reconstruction performance. Nevertheless, there is still some room for further improvement of MAR performance and generalization ability, since some important prior knowledge underlying this specific task has not been fully exploited. Hereby, in this paper, we carefully analyze the characteristics of metal artifacts and propose an orientation-shared convolution representation strategy to adapt the physical prior structures of artifacts, \\emph{i.e.}, rotationally symmetrical streaking patterns. The proposed method rationally adopts Fourier-series-expansion-based filter parametrization in artifact modeling, which can better separate artifacts from anatomical tissues and boost the model generalizability. Comprehensive experiments executed on synthesized and clinical datasets show the superiority of our method in detail preservation beyond the current representative MAR methods. Code will be available at \\url{https://github.com/hongwang01/OSCNet}.\n"}, {"title": "Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images", "abstract": "Recent years have witnessed a rapid development of automated methods for skin lesion diagnosis and classification. Due to an increasing deployment of such systems in clinics, it has become important to develop a more robust system towards various Out-of-Distribution (OOD) samples (unknown skin lesions and conditions). However, the current deep learning models trained for skin lesion classification tend to classify these OOD samples incorrectly into one of their learned skin lesion categories. To address this issue, we propose a simple yet strategic approach that improves the OOD detection performance while maintaining the multi-class classification accuracy for the known categories of skin lesion. To specify, this approach is built upon a realistic scenario of a long-tailed and fine-grained OOD detection task for skin lesion images. Through this approach, 1) First, we target the mixup amongst middle and tail classes to address the long-tail problem. 2) Later, we combine the above mixup strategy with prototype learning to address the fine-grained nature of the dataset. The unique contribution of this paper is two-fold, justified by extensive experiments. First, we present a realistic problem setting of OOD task for skin lesion. Second, we propose an approach to target the long-tailed and fine-grained aspects of the problem setting simultaneously to increase the OOD performance.\n"}, {"title": "Overlooked Trustworthiness of Saliency Maps", "abstract": "Various saliency visualization methods have been proposed to explain artificial intelligence (AI) models towards building the trustworthiness of AI-driven medical image computing applications. However, an important question has yet to be answered - are the saliency maps themselves trustworthy? The trustworthiness of saliency methods has largely been overlooked. This paper first proposes the criteria and methods to evaluate the trustworthiness of saliency maps. Then, a series of systematic studies are performed on a large-scale dataset with a commonly adopted deep neural network. The results show that: (i) Saliency maps may not be relevant to the model outputs; (ii) Saliency maps lack resistance and can be tampered without changing the model output. By demonstrating these risks of the current saliency methods, we suggest the community using saliency maps with caution when explaining AI models.\n"}, {"title": "Parameter-free latent space transformer for zero-shot bidirectional cross-modality liver segmentation", "abstract": "In this paper, we address the domain shift in cross CT-MR liver segmentation task with a latent space investigation. Domain adaptation between modalities is of significant importance in clinical practice, as different diagnostic procedures require different imaging modalities, such as CT and MR. Thus, training a convolutional neural network (CNN) with one modality may not be sufficient for application in another one. Most domain adaptation methods need to use data and ground truths of both source and target domain in the training process. Different from these techniques, we propose a zero-shot bidirectional cross-modality liver segmentation method by investigating a parameter-free latent space through the prior knowledge from CT and MR images. Experiments on the CHAOS, the subset of LiTS and the local TACE datasets demonstrate that our method can well deal with the problem of CNN failure caused by domain shift and yields promising segmentation results.\n"}, {"title": "Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation", "abstract": "We present a new encoder-decoder Vision Transformer architecture, Patcher, for medical image segmentation. Unlike standard Vision Transformers, it employs Patcher blocks that segment an image into large patches, each of which is further divided into small patches. Transformers are applied to the small patches within a large patch, which constrains the receptive field of each pixel. We intentionally make the large patches overlap to enhance intra-patch communication. The encoder employs a cascade of Patcher blocks with increasing receptive fields to extract features from local to global levels. This design allows Patcher to benefit from both the coarse-to-fine feature extraction common in CNNs and the superior spatial relationship modeling of Transformers. We also propose a new mixture-of-experts (MoE) based decoder, which treats the feature maps from the encoder as experts and selects a suitable set of expert features to predict the label for each pixel. The use of MoE enables better specializations of the expert features and reduces interference between them during inference. Extensive experiments demonstrate that Patcher outperforms state-of-the-art Transformer- and CNN-based approaches significantly on stroke lesion segmentation and polyp segmentation. Code for Patcher is released to facilitate related research.\n"}, {"title": "Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising", "abstract": "The acquisition conditions for low-dose and high-dose CT images are usually different, so that the shifts in the CT numbers often occur. Accordingly, unsupervised deep learning-based approaches, which learn the target image distribution, often introduce CT number distortions and result in detrimental effects in diagnostic performance. To address this, here we propose a novel unsupervised learning approach for lowdose CT reconstruction using patch-wise deep metric learning. The key idea is to learn embedding space by pulling the positive pairs of image patches which shares the same anatomical structure, and pushing the negative pairs which have same noise level each other. Thereby, the network is trained to suppress the noise level, while retaining the original global CT number distributions even after the image translation. Experimental results conrm that our deep metric learning plays a critical role in producing high quality denoised images without CT number shift.\n"}, {"title": "PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model", "abstract": "Early prediction of pathological complete response (pCR)\nfollowing neoadjuvant chemotherapy (NAC) for breast cancer plays a\ncritical role in surgical planning and optimizing treatment strategies.\nRecently, machine and deep-learning based methods were suggested for\nearly pCR prediction from multi-parametric MRI (mp-MRI) data includ-\ning dynamic contrast-enhanced MRI and diffusion-weighted MRI (DWI)\nwith moderate success. We introduce PD-DWI, a physiologically decom-\nposed DWI machine-learning model to predict pCR from DWI and clin-\nical data. Our model first decomposes the raw DWI data into the var-\nious physiological cues that are influencing the DWI signal and then\nuses the decomposed data, in addition to clinical variables, as the in-\nput features of a radiomics-based XGBoost model. We demonstrated the\nadded-value of our PD-DWI model over conventional machine-learning\napproaches for pCR prediction from mp-MRI data using the publicly\navailable Breast Multi-parametric MRI for prediction of NAC Response\n(BMMR2) challenge. Our model substantially improves the area under\nthe curve (AUC), compared to the current best result on the leaderboard\n(0.8849 vs. 0.8397) for the challenge test set. PD-DWI has the potential\nto improve prediction of pCR following NAC for breast cancer, reduce\noverall mp-MRI acquisition times and eliminate the need for contrast-\nagent injection.\n"}, {"title": "Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound", "abstract": "Over the past decades, the incidence of thyroid cancer has been increasing globally. Accurate and early diagnosis allows timely treatment and helps to avoid over-diagnosis. Clinically, a nodule is commonly evaluated from both transverse and longitudinal views using thyroid ultrasound. However, the appearance of the thyroid gland and lesions can vary dramatically across individuals. Identifying key diagnostic information from both views requires specialized expertise. Furthermore, finding an optimal way to integrate multi-view information also relies on the experience of clinicians and adds further difficulty to accurate diagnosis.\nTo address these, we propose a personalized diagnostic tool that can customize its decision-making process for different patients. It consists of a multi-view classification module for feature extraction and a personalized weighting allocation network that generates optimal weighting for different views. It is also equipped with a self-supervised view-aware contrastive loss to further improve the model robustness towards different patient groups. Experimental results show that the proposed framework can better utilize multi-view information and outperform the competing methods. \n"}, {"title": "Personalized dMRI Harmonization on Cortical Surface", "abstract": "The inter-site variability of diffusion magnetic resonance imaging (dMRI) hinders the aggregation of the dMRI data from multiple centers. This necessitates dMRI harmonization for removing the non-biological site-effects. Recently, the emergence of high-resolution dMRI data across various connectome imaging studies allows the large-scale analysis of cortical micro-structure. Existing harmonization methods, however, perform poorly in the harmonization of dMRI data in cortical areas because they rely on image registration methods to factor out anatomical variations, which have known difficulty in aligning cortical folding patterns. To overcome this fundamental challenge in dMRI harmonization, we propose a framework of personalized dMRI harmonization on the cortical surface to improve the dMRI harmonization of gray matter by adaptively estimating the inter-site harmonization mappings. In our experiments, we demonstrate the effectiveness of the proposed method by applying it to harmonize dMRI across the Human Connectome Project (HCP) and the Lifespan Human Connectome Projects in Development (HCPD) studies and achieved much better performance in comparison with conventional methods based on image registration.\n"}, {"title": "PET denoising and uncertainty estimation based on NVAE model using quantile regression loss", "abstract": "Deep learning-based methods have shown their superior performance for medical imaging, but their clinical application is still rare. One reason may come from their uncertainty. As data-driven models, deep learning-based methods are sensitive to imperfect data. Thus, it is important to quantify the uncertainty, especially for PET denoising tasks where the noise is very similar to small tumors. In this paper, we proposed a Nouveau variational autoencoder (NVAE) based model using quantile regression loss for simultaneous PET image denoising and uncertainty estimation. Quantile regression loss was performed as the reconstruction loss to avoid the variance shrinkage problem caused by the traditional reconstruction probability loss. The variance and mean can be directly calculated from the estimated quantiles under the Logistic assumption, which is more efficient than Monte Carlo sampling. Experiment based on real 11C-DASB dataset verified that the denoised PET images of the proposed method have a higher mean(\u00c2\u00b1SD) peak signal-to-noise ratio (PSNR) (40.64\u00c2\u00b15.71) and structural similarity index measure (SSIM)(0.9807\u00c2\u00b10.0063) than Unet-based denoising (PSNR, 36.18\u00c2\u00b15.55; SSIM, 0.9614\u00c2\u00b10.0121) and NVAE model using Monte Carlo sampling (PSNR, 37.00\u00c2\u00b15.35; SSIM, 0.9671\u00c2\u00b10.0095).\n"}, {"title": "PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation", "abstract": "The success of Transformer in computer vision has attracted increasing attention in the medical imaging community. Especially for medical image segmentation, many excellent hybrid architectures based on convolutional neural networks (CNNs) and Transformer have been presented and achieve impressive performance. However, most of these methods, which embed modular Transformer into CNNs, struggle to reach their full potential. In this paper, we propose a novel hybrid architecture for medical image segmentation called PHTrans, which parallelly hybridizes Transformer and CNN in main building blocks to produce hierarchical representations from global and local features and adaptively aggregate them, aiming to fully exploit their strengths to obtain better segmentation performance. Specifically, PHTrans follows the U-shaped encoder-decoder design and introduces the parallel hybird module in deep stages, where convolution blocks and the modified 3D Swin Transformer learn local features and global dependencies separately, then a sequence-to-volume operation unifies the dimensions of the outputs to achieve feature aggregation. Extensive experimental results on both Multi-Atlas Labeling Beyond the Cranial Vault and Automated Cardiac Diagnosis Challeng datasets corroborate its effectiveness, consistently outperforming state-of-the-art methods. \nThe code is available at: \\url{https://github.com/lseventeen/PHTrans\n"}, {"title": "Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography", "abstract": "Displacement estimation is a critical step of virtually all Ultrasound Elastography (USE) techniques. Two main features make this task unique compared to the general optical flow problem: the high-frequency nature of ultrasound radio-frequency (RF) data and the governing laws of physics on the displacement field. Recently, the architecture of the optical flow networks has been modified to be able to use RF data. Also, semi-supervised and unsupervised techniques have been employed for USE by considering prior knowledge of displacement continuity in the form of the first- and second-derivative regularizers. Despite these attempts, no work has considered the tissue compression pattern, and displacements in axial and lateral directions have been assumed to be independent. However, tissue motion pattern is governed by laws of physics in USE, rendering the axial and the lateral displacements highly correlated. In this paper, we propose Physically Inspired ConsTraint for Unsupervised Regularized Elastography (PICTURE), where we impose constraints on the Poisson\u00e2\u0080\u0099s ratio to improve lateral displacement estimates. Experiments on phantom and \\textit{in vivo} data show that PICTURE substantially improves the quality of the lateral displacement estimation.\n"}, {"title": "Physiological Model based Deep Learning Framework for Cardiac TMP Recovery", "abstract": "Recovering cardiac transmembrane potential (TMP) from\nbody surface potential (BSP) plays an important role in the noninvasive\ndiagnosis of heart diseases. However, most current solutions for TMP\nrecovery are typically proposed and designed to follow a static mapping\nparadigm between TMP and BSP, which ignores the inherent dynamic\nactivation process of cardiomyocytes during the cardiac cycle. In this\npaper, we propose to introduce the physiological information of this dy-\nnamic activation process in the objective functions. Based on this, we\nfurther establish a physiological model based deep learning framework\nfor cardiac TMP recovery. First, the objective functions of our physio-\nlogical model are deduced via a two-variable diffusion-reaction system,\nwhere the static mapping and the dynamic activation process of car-\ndiomyocytes are jointly modeled. Then, a data-driven Kalman Filtering\nnetwork (KFNet) is adopted to solve the proposed objective functions.\nSpecifically, the KFNet consists of two components: a state transfer net-\nwork (SSNet) is employed for directly predicting the prior estimation;\nfurthermore, a Kalman gain network (KGNet) is employed for adaptively\nlearning the gain coefficients. In our experiments, the proposed physio-\nlogical model is verified on the 1200 simulated subjects. The quantified\nanalysis shows the proposed method can accurately recover the TMP,\nwith the low LE values 10.5 for the ectopic pacing location task and\nthe high SSIM values 0.75 for the myocardial infarction detection task.\nThese powerful performances completely verify the effectiveness of our\nmodel.\n"}, {"title": "Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs", "abstract": "Optical coherence tomography angiography (OCTA) can non-invasively image the eye\u00e2\u0080\u0099s circulatory system. In order to reliably characterize the retinal vasculature, there is a need to automatically extract quantitative metrics from these images. The calculation of such biomarkers requires a precise semantic segmentation of the blood vessels. However, deep-learning-based methods for segmentation mostly rely on supervised training with voxel-level annotations, which are costly to obtain."}, {"title": "Point Beyond Class: A Benchmark for Weakly Semi-Supervised Abnormality Localization in Chest X-Rays", "abstract": "Accurate abnormality localization in chest X-rays (CXR) can benefit the clinical diagnosis of various thoracic diseases. However, the lesion-level annotation can only be performed by experienced radiologists, and it is tedious and time-consuming, thus difficult to acquire. Such a situation results in a difficulty to develop a fully-supervised abnormality localization system for chest X-rays. In this regard, we propose to train the CXR abnormality localization framework via a weakly semi-supervised strategy, termed Point Beyond Class (PBC), which utilizes a small number of fully annotated CXRs with lesion-level bounding boxes and extensive weakly annotated samples by points. Such a point annotation setting can provide weakly instance-level information for abnormality localization with a marginal annotation cost. Particularly, the core idea behind our PCB is to learn a robust and accurate mapping from the point annotations to the bounding boxes against the variance of annotated points. To achieve that, a regularization term, namely multi-point consistency, is proposed, which drives the model to generate the consistent bounding box from different point annotations inside the same abnormality. Furthermore, a self-supervision, termed symmetric consistency, is also proposed to deeply exploit the useful information from the weakly annotated data for abnormality localization. Experimental results on RSNA and VinDr-CXR datasets justify the effectiveness of the proposed method. An improvement of ~5% in mAP can be achieved by our PBC, compared to the current state-of-the-art method (i.e. Point DETR).\n"}, {"title": "Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image", "abstract": "Image enhancement approaches often assume that the noise is signal independent, and approximate the degradation model as zero-mean additive Gaussian. However, this assumption does not hold for biomedical imaging systems where sensor-based sources of noise are proportional to signal strengths, and the noise is better represented as a Poisson process. In this work, we explore a sparsity and dictionary learning-based approach and present a novel self-supervised learning method for single-image denoising where the noise is approximated as a Poisson process, requiring no clean ground-truth data. Specifically, we approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network that enforces sparsity with respect to the weights of the network. Since the sparse representations are based on the underlying image, it is able to suppress the spurious components (noise) in the image patches, thereby introducing implicit regularization for denoising tasks through the network structure. Experiments on two bio-imaging datasets demonstrate that our method outperforms the state-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results demonstrate that, in addition to higher performance on standard quantitative metrics, we are able to recover much more subtle details than other compared approaches. Our code is made publicly available at https://github.com/tacalvin/Poisson2Sparse\n"}, {"title": "Pose-based Tremor Classification for Parkinson\u00e2\u0080\u0099s Disease Diagnosis from Video", "abstract": "Parkinson\u00e2\u0080\u0099s disease (PD) is a progressive neurodegenerative disorder that results in a variety of motor dysfunction symptoms, including tremors, bradykinesia, rigidity and postural instability. The diagnosis of PD mainly relies on clinical experience rather than a definite medical test, and the diagnostic accuracy is only about 73-84% since it is challenged by the subjective opinions or experiences of different medical experts. Therefore, an efficient and interpretable automatic PD diagnosis system is valuable for supporting clinicians with more robust diagnostic decision-making. To this end, we propose to classify Parkinson\u00e2\u0080\u0099s tremor since it is one of the most predominant symptoms of PD with strong generalizability. Different from other computer-aided time and resource-consuming Parkinson\u00e2\u0080\u0099s Tremor (PT) classification systems that rely on wearable sensors, we propose SPAPNet, which only requires consumer-grade non-intrusive video recording of camera-facing human movements as input to provide undiagnosed patients with low-cost PT classification results as a PD warning sign. For the first time, we propose to use a novel attention module with a lightweight pyramidal channel-squeezing-fusion architecture to extract relevant PT information and filter the noise efficiently. This design aids in improving both classification performance and system interpretability. Experimental results show that our system outperforms state-of-the-arts by achieving a balanced accuracy of 90.9% and an F1-score of 90.6% in classifying PT with the non-PT class. \n"}, {"title": "Position-prior Clustering-based Self-attention Module for Knee Cartilage Segmentation", "abstract": "The morphological changes in knee cartilage (especially femoral and tibial cartilages) are closely related to the progression of knee osteoarthritis, which is expressed by magnetic resonance (MR) images and assessed on the cartilage segmentation results. Thus, it is necessary to propose an effective automatic cartilage segmentation model for longitudinal research on osteoarthritis. In this research, to relieve the problem of inaccurate discontinuous segmentation caused by the limited receptive field in convolutional neural networks, we proposed a novel position-prior clustering-based self-attention module (PCAM). In PCAM, long-range dependency between each class center and feature point is captured by self-attention allowing contextual information re-allocated to strengthen the relative features and ensure the continuity of segmentation result. The clutsering-based method is used to estimate class centers, which fosters intra-class consistency and further improves the accuracy of segmentation results. The position-prior excludes the false positives from side-output and makes center estimation more precise. Sufficient experiments are conducted on OAI-ZIB dataset. The experimental results show that the segmentation performance of combination of segmentation network and PCAM obtains an evident improvement compared to original model, which proves the potential application of PCAM in medical segmentation tasks. The source code is publicly available from link: https://github.com/LeongDong/PCAMNet\n"}, {"title": "Predicting molecular traits from tissue morphology through self-interactive multi-instance learning", "abstract": "Previous efforts to learn histology features that correlate with specific genetic/molecular traits resort to tile-level multi-instance learning (MIL) which relies on a fixed pretrained model for feature extraction and an instance-bag classifier. We argue that such a two-step approach is not optimal at capturing both fine-grained features at tile level and global features at slide level optimal to the task. We propose a self-interactive MIL that iteratively feedbacks training information between the fine-grained and global context features. We validate the proposed approach on 4 subtyping tasks: EMT status (ovarian), KRAS mutation (colon and lung), EGFR mutation (colon), and HER2 status (breast). Our approach yields an average improvement of 7.05% ~ 8.34% (in terms of AUC) over the baseline.\n"}, {"title": "Predicting Spatio-Temporal Human Brain Response Using fMRI", "abstract": "The transformation and transmission of brain stimuli reflect the dynamical brain activity in space and time. Compared with functional magnetic resonance imaging (fMRI), magneto- or electroencephalography (M/EEG) fast couples to the neural activity through generated magnetic fields. However, the MEG signal is inhomogeneous throughout the whole brain, which is affected by the signal-to-noise ratio, the sensors\u00e2\u0080\u0099 location and distance. Current non-invasive neuroimaging modalities such as fMRI and M/EEG excel high resolution in space or time but not in both. To solve the main limitations of current technique for brain activity recording, we propose a novel recurrent memory optimization approach to predict the internal behavioral states in space and time. The proposed method uses Optimal Polynomial Projections to capture the long temporal history with robust online compression. The training process takes the pairs of fMRI and MEG data as inputs and predicts the recurrent brain states through the Siamese network. In the testing process, the framework only uses fMRI data to generate the corresponding neural response in space and time. The experimental results with Human connectome project (HCP) show that the predicted signal could reflect the neural activity with high spatial resolution as fMRI and high temporal resolution as MEG signal. The experimental results demonstrate for the first time that the proposed method is able to predict the brain response in both milliseconds and millimeters using only fMRI signal.\n"}, {"title": "Privacy Preserving Image Registration", "abstract": "Image registration is a key task in medical imaging applications, allowing to represent medical images in a common spatial reference frame. Current literature on image registration is generally based on the assumption that images are usually accessible to the researcher, from which the spatial transformation is subsequently estimated. This common assumption may not be met in current practical applications, since the sensitive nature of medical images may ultimately require their analysis under privacy constraints, preventing to share the image content in clear form.\nIn this work, we formulate the problem of image registration under a privacy preserving regime, where images are assumed to be confidential and cannot be disclosed in clear. \nWe derive our privacy preserving image registration framework by extending classical registration paradigms to account for advanced cryptographic tools, such as secure multi-party computation and homomorphic encryption, that enable the execution of operations without leaking the underlying data. To overcome the problem of performance and scalability of cryptographic tools in high dimensions, we first propose to optimize the underlying image registration operations using gradient approximations. We further revisit the use of homomorphic encryption and use a packing method to allow the encryption and multiplication of large matrices more efficiently.\nWe demonstrate our privacy preserving framework in linear and non-linear registration problems, evaluating its accuracy and scalability with respect to standard image registration. Our results show that privacy preserving image registration is feasible and can be adopted in sensitive medical imaging applications.\n"}, {"title": "ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical Image Classification", "abstract": "Medicalimageclassificationhasbeenwidelyadoptedinmed- ical image analysis. However, due to the difficulty of collecting and la- beling data in the medical area, medical image datasets are usually highly-imbalanced. To address this problem, previous works utilized class samples as prior for re-weighting or re-sampling but the feature repre- sentation is usually still not discriminative enough. In this paper, we adopt the contrastive learning to tackle the long-tailed medical imbalance problem. Specifically, we first propose the category prototype and adver- sarial proto-instance to generate representative contrastive pairs. Then, the prototype recalibration strategy is proposed to address the highly imbalanced data distribution. Finally, a unified proto-loss is designed to train our framework. The overall framework, namely as Prototype- aware Contrastive learning (ProCo), is unified as a single-stage pipeline in an end-to-end manner to alleviate the imbalanced problem in med- ical image classification, which is also a distinct progress than existing works as they follow the traditional two-stage pipeline. Extensive exper- iments on two highly-imbalanced medical image classification datasets demonstrate that our method outperforms the existing state-of-the-art methods by a large margin.\n"}, {"title": "Prognostic Imaging Biomarker Discovery in Survival Analysis for Idiopathic Pulmonary Fibrosis", "abstract": "Imaging biomarkers derived from medical images play an important role in diagnosis, prognosis, and therapy response assessment. Developing prognostic imaging biomarkers which can achieve reliable survival prediction is essential for prognostication across various diseases and imaging modalities. In this work, we propose a method for discovering patch-level imaging patterns which we then use to predict mortality risk and identify prognostic biomarkers. Specifically, a contrastive learning model is first trained on patches to learn patch representations, followed by a clustering method to group similar underlying imaging patterns. The entire medical image can be thus represented by a long sequence of patch representations and their cluster assignments. Then a memory-efficient clustering Vision Transformer is proposed to aggregate all the patches to predict mortality risk of patients and identify high-risk patterns. To demonstrate the effectiveness and generalizability of our model, we test the survival prediction performance of our method on two sets of patients with idiopathic pulmonary fibrosis (IPF), a chronic, progressive, and life-threatening interstitial pneumonia of unknown etiology. Moreover, by comparing the high-risk imaging patterns extracted by our model with existing imaging patterns utilised in clinical practice, we can identify a novel biomarker that may help clinicians improve risk stratification of IPF patients.\n"}, {"title": "Progression models for imaging data with Longitudinal Variational Auto Encoders", "abstract": "Disease progression models are crucial to understanding degenerative diseases. Mixed-effects models have been consistently used to model clinical assessments or biomarkers extracted from medical images, allowing missing data imputation and prediction at any timepoint. However, such progression models have seldom been used for entire medical images. In this work, a Variational Auto Encoder is coupled with a temporal linear mixed-effect model to learn a latent representation of the data such that individual trajectories follow straight lines over time and are characterised by a few interpretable parameters. A Monte Carlo estimator is devised to iteratively optimize the networks and the statistical model. We apply this method on a synthetic data set to illustrate the disentanglement between time dependant changes and inter-subjects variability, as well as the predictive capabilities of the method. We then apply it to 3D MRI and FDG-PET data from the Alzheimer\u00e2\u0080\u0099s Disease Neuroimaging Initiative (ADNI) to recover well documented patterns of structural and metabolic alterations of the brain.\n"}, {"title": "Progressive Deep Segmentation of Coronary Artery via Hierarchical Topology Learning", "abstract": "Coronary artery segmentation is a critical yet challenging step in coronary artery stenosis diagnosis.\nMost existing studies ignore important contextual anatomical information and vascular topologies, leading to limited performance.\nTo this end, this paper proposes a progressive deep-learning based framework for accurate coronary artery segmentation by leveraging contextual anatomical information and vascular topologies.\nThe proposed framework consists of a spatial anatomical dependency (SAD) module and a hierarchical topology learning (HTL) module.\nSpecifically, the SAD module coarsely segments heart chambers and coronary artery for region proposals, and captures spatial relationship between coronary artery and heart chambers.\nThen, the HTL module adopts a multi-task learning mechanism to improve the coarse coronary artery segmentation by simultaneously predicting the hierarchical vascular topologies i.e., key points, centerlines, and neighboring cube-connectivity.\nExtensive evaluations, ablation studies, and comparisons with existing methods show that our method achieves state-of-the-art segmentation performance.\n"}, {"title": "Progressive Subsampling for Oversampled Data - Application to Quantitative MRI", "abstract": "We present PROSUB: PROgressive SUBsampling, a deep learning based, automated methodology that subsamples an oversampled data set (e.g. channels of multi-channeled 3D images) with minimal loss of information.  We build upon a state-of-the-art dual-network approach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI (qMRI) measurement sampling-reconstruction challenge, but suffers from deep learning training instability, by subsampling with a hard decision boundary.  PROSUB uses the paradigm of recursive feature elimination (RFE) and progressively subsamples measurements during deep learning training, improving optimization stability.  PROSUB also integrates a neural architecture search (NAS) paradigm, allowing the network architecture hyperparameters to respond to the subsampling process.  We show PROSUB outperforms the winner of the MUDI MICCAI challenge, producing large improvements >$18 \\% $ MSE on the MUDI challenge sub-tasks and qualitative improvements on downstream processes useful for clinical applications.  We also show the benefits of incorporating NAS and analyze the effect of PROSUB\u00e2\u0080\u0099s components.  As our method generalizes beyond MRI measurement selection-reconstruction, to problems that subsample and reconstruct multi-channeled data, our code is https://github.com/sbb-gh/PROSUB\n"}, {"title": "Prostate Cancer Histology Synthesis using StyleGAN Latent Space Annotation", "abstract": "The latent space of a generative adversarial network (GAN) may model pathologically-significant semantics with unsupervised learning. To explore this phenomenon, we trained and tested a StyleGAN2 on a high quality prostate histology dataset covering the prostate cancer (PCa) diagnostic spectrum. Our pathologist annotated synthetic images to identify learned PCa regions in the GAN latent space. New points were drawn from these regions, synthesized into images, and given to a pathologist for annotation. 77% of the new points received the same annotation, and 98% of the latent points received the same or adjacent diagnostic stage annotation. This confirms the GAN network can accurately disentangle and model PCa features without exposure to labels in the training process.\n"}, {"title": "PRO-TIP: Phantom for RObust automatic ultrasound calibration by TIP detection", "abstract": "We propose a novel method to automatically calibrate tracked ultrasound probes.\nTo this end we design a custom phantom consisting of nine cones with different heights.\nThe tips are used as key points to be matched between multiple sweeps.\nWe extract them using a convolutional neural network to segment the cones in every ultrasound frame and then track them across the sweep.\nThe calibration is robustly estimated using RANSAC and later refined employing image based techniques.\nOur phantom can be 3D-printed and offers many advantages over state-of-the-art methods. \nThe phantom design and algorithm code are freely available online.\nSince our phantom does not require a tracking target on itself, ease of use is improved over currently used techniques.\nThe fully automatic method generalizes to new probes and different vendors, as shown in our experiments.\nOur approach produces results comparable to calibrations obtained by a domain expert.\n"}, {"title": "Prototype Learning of Inter-network Connectivity for ASD Diagnosis and Personalized Analysis", "abstract": "In recent studies, deep learning has shown great potential to explore topological properties of functional connectivity (FC), e.g., graph neural networks (GNN), for brain disease diagnosis, e.g, Autism spectrum disorder (ASD). However, many of the existing methods integrate the information locally, e.g., among neighboring nodes in a graph, which hinders from learning complex patterns of FC globally. In addition, their analysis for discovering imaging biomarkers is confined to providing the most discriminating regions without consideration of individual variations over the average FC patterns of groups, i.e., patients and normal controls. To address these issues, we propose a unified framework that globally captures properties of inter-network connectivity for classification and provides individual-specific group characteristics for interpretation via prototype learning. In our experiments over the ABIDE dataset, we validated the effectiveness of the proposed framework by comparing to competing topological deep learning methods in the literature. Furthermore, we analyzed individually specified functional mechanisms of ASD for neurological interpretation.\n"}, {"title": "Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification", "abstract": "Deep learning models were frequently reported to learn from shortcuts like dataset biases. As deep learning is playing an increasingly important role in the modern healthcare system, it is of great need to combat shortcut learning in medical data as well as develop unbiased and trustworthy models. In this paper, we study the problem of developing debiased chest X-ray diagnosis models from the biased training data without knowing exactly the bias labels. We start with the observations that the imbalance of bias distribution is one of the key reasons causing shortcut learning, and the dataset biases are preferred by the model if they were easier to be learned than the intended features. Based on these observations, we propose a novel algorithm, pseudo bias-balanced learning, which first captures and predicts per-sample bias labels via generalized cross entropy loss and then trains a debiased model using pseudo bias labels and bias-balanced softmax function. We constructed several chest X-ray datasets with various dataset bias situations and demonstrated with extensive experiments that our proposed method achieved consistent improvements over state-of-the-art approaches.\n"}, {"title": "Radiological Reports Improve Pre-Training for Localized Imaging Tasks on Chest X-Rays", "abstract": "Self-supervised pre-training on unlabeled images has shown promising results in the medical domain. Recently, methods using text-supervision from companion text like radiological reports improved upon these results even further.\nHowever, most works in the medical domain focus on image classification downstream tasks and do not study more localized tasks like semantic segmentation or object detection.\nWe therefore propose a novel evaluation framework consisting of 18 localized tasks, including semantic segmentation and object detection, on five public chest radiography datasets.\nUsing our proposed evaluation framework, \nwe study the effectiveness of existing text-supervised methods and compare them with image-only self-supervised methods and transfer from classification in more than 1200 evaluation runs.\nOur experiments show that text-supervised methods\noutperform all other methods on 13 out of 18 tasks making them the preferred method.\nIn conclusion, image-only contrastive methods provide a strong baseline if no reports are available while transfer from classification, even in-domain, does not perform well in pre-training for localized tasks.\n"}, {"title": "RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization", "abstract": "Stain variations often decrease the generalization ability of deep learning based approaches in digital histopathology analysis. Two separate proposals, namely stain normalization (SN) and stain augmentation (SA), have been spotlighted to reduce the generalization error, where the former alleviates the stain shift across different medical centers using template image and the latter enriches the accessible stain styles by the simulation of more stain variations. However, their applications are bounded by the selection of template images and the construction of unrealistic styles. To address the problems, we unify SN and SA with a novel RandStainNA scheme, which constrains variable stain styles in a practicable range to train a stain agnostic deep learning model. The RandStainNA is applicable to stain normalization in a collection of color spaces i.e. HED, HSV, LAB. Additionally, we propose a random color space selection scheme to gain extra performance improvement. We evaluate our method by two diagnostic tasks i.e. tissue subtype classification and nuclei segmentation, with various network backbones. The performance superiority over both SA and SN yields that the proposed RandStainNA can consistently improve the generalization ability, that our models can cope with more incoming clinical datasets with unpredicted stain styles. The codes is available at https://github.com/yiqings/RandStainNA.\n"}, {"title": "Real-Time 3D Reconstruction of Human Vocal Folds via High-Speed Laser-Endoscopy", "abstract": "Conventional video endoscopy and high-speed video endoscopy of the human larynx solely provides practitioners with information about the two-dimensional lateral and longitudinal deformation of vocal folds.\nHowever, experiments have shown that vibrating human vocal folds have a significant vertical component.\nBased upon an endoscopic laser projection unit (LPU) connected to a high-speed camera, we propose a fully-automatic and real-time capable approach for the robust 3D reconstruction of human vocal folds.\nWe achieve this by estimating laser ray correspondences by taking epipolar constraints of the LPU into account.\nUnlike previous approaches only reconstructing the superior area of the vocal folds, our pipeline is based on a parametric reinterpretation of the M5 vocal fold model as a Tensor product surface.\nNot only are we able to generate visually authentic deformations of a dense vibrating vocal fold model, but we are also able to easily generate metric measurements of points of interest on the reconstructed surfaces.\nFurthermore, we drastically lower the effort needed for visualizing and measuring the dynamics of the human laryngeal area during phonation.\nAdditionally, we publish the first publicly available labeled in-vivo dataset of laser-based high-speed laryngoscopy videos. The source code and dataset are available at https://henningson.github.io/Vocal3D/.\n"}, {"title": "Recurrent Implicit Neural Graph for Deformable Tracking in Endoscopic Videos", "abstract": "Tracking points in robotic assisted surgery will help to enable models in augmented reality and image guidance applications.\nFor these applications, both speed and accuracy are critical.\nCurrent dense convolutional neural networks can be costly, especially so when we only desire to track user defined regions.\nFaster methods use keypoints and their movement as a way to estimate flow in an image.\nIn this paper we introduce a recurrent implicit neural graph (RING) which estimates flow efficiently.\nRING interpolates the flow at any selected query points with a implicit neural representation (also known as coordinate-based representation) that takes the surrounding points and history of the tracked (query) points as input.\nRING is able to track an arbitrary number of image points.\nWe demonstrate that RING estimates point motion  better than methods that do not use a state.\nWe evaluate RING both photometrically and using ground truth depth data.\nFinally we demonstrate RING\u00e2\u0080\u0099s real-time effectiveness in timing experiments.\n"}, {"title": "Reducing Positional Variance in Cross-sectional Abdominal CT Slices with Deep Conditional Generative Models", "abstract": "2D low-dose single-slice abdominal computed tomography (CT) slice enables direct measurements of body composition, which are critical to quantitatively characterizing health relationships on aging. However, longitudinal analysis of body composition changes using 2D abdominal slices is challenging due to positional variance between longitudinal slices acquired in different years. To reduce the positional variance, we extend the conditional generative models to our C-SliceGen that takes an arbitrary axial slice in the abdominal region as the condition and generates a defined vertebral level slice by estimating the structural changes in the latent space. Experiments on 1170 subjects from an in-house dataset and 50 subjects from BTCV MICCAI Challenge 2015 show that our model can generate high quality images in terms of realism and similarity. External experiments on 20 subjects from the Baltimore Longitudinal Study of Aging (BLSA) dataset that contains longitudinal single abdominal slices validate that our method can harmonize the slice positional variance in terms of muscle and visceral fat area. Our approach provides a promising direction of mapping slices from different vertebral levels to a target slice to reduce positional variance for single slice longitudinal analysis. \n"}, {"title": "RefineNet: An Automated Framework to Generate Task and Subject-Specific Brain Parcellations for Resting-State fMRI Analysis", "abstract": "Parcellations used in resting-state fMRI (rs-fMRI) analyses are derived from group-level information, and thus ignore both subject-level functional differences and the downstream task. In this paper, we introduce RefineNet, a Bayesian-inspired deep network architecture that adjusts region boundaries based on individual functional connectivity profiles. RefineNet uses an iterative voxel reassignment procedure that considers neighborhood information while balancing temporal coherence of the refined parcellation. We validate RefineNet on rs-fMRI data from three different datasets, each one geared towards a different predictive task: (1) cognitive fluid intelligence prediction using the HCP dataset (regression), (2) autism versus control diagnosis using the ABIDE II dataset (classification), and (3) language localization using an rs-fMRI brain tumor dataset (segmentation). We demonstrate that RefineNet improves the performance of existing deep networks from the literature on each of these tasks. We also show that RefineNet produces anatomically meaningful subject-level parcellations with higher temporal coherence. \n"}, {"title": "Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images", "abstract": "Top-down instance segmentation framework has shown its superiority in object detection compared to the bottom-up framework. While it is efficient in addressing over-segmentation, top-down instance segmentation suffers from over-crop problem. However, a complete segmentation mask is crucial for biological image analysis as it delivers important morphological properties such as shapes and volumes. In this paper, we propose a region proposal rectification (RPR) module to address this challenging incomplete segmentation problem. In particular, we offer a progressive ROIAlign module to introduce neighbor information into a series of ROIs gradually. The ROI features are fed into an attentive feed-forward network (FFN) for proposal box regression. With additional neighbor information, the proposed RPR module shows significant improvement in correction of region proposal locations and thereby exhibits favorable instance segmentation performances on three biological image datasets compared to state-of-the-art baseline methods. Experimental results demonstrate that the proposed RPR module is effective in both anchor-based and anchor-free top-down instance segmentation approaches, suggesting the proposed method can be applied to general top-down instance segmentation of biological images.\n"}, {"title": "Region-guided CycleGANs for Stain Transfer in Whole Slide Images", "abstract": "In whole slide imaging, commonly used staining techniques based on hematoxylin and eosin (H&E) and immunohistochemistry (IHC) stains accentuate different aspects of the tissue landscape. In the case of detecting metastases, IHC provides a distinct readout that is readily interpretable by pathologists. IHC, however, is a more expensive approach and not available at all medical centers. Virtually generating IHC images from H&E using deep neural networks thus becomes an attractive alternative. Deep generative models such as CycleGANs learn a semantically-consistent mapping between two image domains, while emulating the textural properties of each domain. They are therefore a suitable choice for stain transfer applications. However, they remain fully unsupervised, and possess no mechanism for enforcing biological consistency in stain transfer. In this paper, we propose an extension to CycleGANs in the form of a region of interest discriminator. This allows the CycleGAN to learn from unpaired datasets where, in addition, there is a partial annotation of objects for which one wishes to enforce consistency. We present a use case on whole slide images, where an IHC stain provides an experimentally generated signal for metastatic cells. We demonstrate the superiority of our approach over prior art in stain transfer on histopathology tiles over two datasets. Our code and model are available at https://github.com/jcboyd/miccai2022-roigan.\n"}, {"title": "Regression Metric Loss: Learning a Semantic Representation Space for Medical Images", "abstract": "Regression plays an essential role in many medical imaging applications for estimating various clinical risk or measurement scores. While training strategies and loss functions have been studied for the deep neural networks in medical image classification tasks, options for regression tasks are very limited. One of the key challenges is that the high-dimensional feature representation learned by existing popular loss functions like Mean Squared Error or L1 loss is hard to interpret. In this paper, we propose a novel Regression Metric Loss (RM-Loss), which endows the representation space with the semantic meaning of the label space by finding a representation manifold that is isometric to the label space. Experiments on two regression tasks, i.e. coronary artery calcium score estimation and bone age assessment, show that RM-Loss is superior to the existing popular regression losses on both performance and interpretability.\n"}, {"title": "Reinforcement Learning Driven Intra-modal and Inter-modal Representation Learning for 3D Medical Image Classification", "abstract": "Multi-modality 3D medical images play an important role in the clinical practice, as each modality captures specific characteristics of the underlying anatomical information. Due to the effectiveness of exploring the complementary information among different modalities, multi-modality learning has attracted increased attention recently, which can be realized by Deep Learning (DL) models. However, it remains a challenging task for two reasons. First, the prediction confidence of multi-modality learning network cannot be guaranteed when the model is trained with volume-level labels, which provide weak supervision to learn 3D information. Second, it is difficult to effectively exploit the complementary information across modalities and also preserve the modality-specific properties when fusion. In this paper, we present a novel Reinforcement Learning (RL) driven approach to comprehensively address these challenges, where an independent learning mechanism is proposed to choose reliable and informative features within modality and explore complementary representations across modalities with the guidance of dynamic weights. Particularly, two Recurrent Neural Networks (RNN) based agents are utilized for representation learning within a single modality (intra-learning) and among different modalities (inter-learning), which are trained via Proximal Policy Optimization (PPO) with the confidence increment of the prediction as the reward. To validate the proposed method, we take the 3D image classification as an example and conduct experiments on a multi-modality brain tumor MRI data. Experimental results show that the classification performance is improved by 5.9\\% when employing the proposed RL-based multi-modality representation learning.\n"}, {"title": "Reinforcement learning for active modality selection during diagnosis", "abstract": "Diagnosis through imaging generally requires the combination of several modalities. Algorithms for data fusion allow merging information from different sources, mostly  combining all images in a single step. In contrast, much less attention has been given to the incremental addition of new data descriptors, and the consideration of their costs (which can cover economic costs but also patient comfort and safety)."}, {"title": "Reliability of quantification estimates in MR Spectroscopy: CNNs vs. traditional model fitting", "abstract": "Magnetic Resonance Spectroscopy (MRS) and Spectroscopic Imaging (MRSI) are non-invasive techniques to map tissue contents of many metabolites in situ in humans. Quantification is traditionally done via model fitting (MF), and Cramer Rao Lower Bounds (CRLBs) are used as a measure of fitting uncertainties. Signal-to-noise is limited due to clinical time constraints and MF can be very time-consuming in MRSI with thousands of spectra. Deep Learning (DL) has introduced the possibility to speed up quantitation, while reportedly preserving accuracy and precision. However, questions arise about how to access quantification uncertainties in the case of DL. In this work, an optimal-performance DL architecture that uses spectrograms as input and maps absolute concentrations of metabolites referenced to water content as output was taken to investigate this in detail. Distributions of predictions and Monte-Carlo dropout were used to investigate data and model-related uncertainties, exploiting ground truth knowledge in a synthetic setup mimicking realistic brain spectra with metabolic composition that uniformly varies from healthy to pathological cases. Bias and CRLBs from MF are then compared to DL-related uncertainties. It is confirmed that DL is a dataset-biased technique where accuracy and precision of predictions scale with metabolite SNR but hint towards bias and increased uncertainty at the edges of the explored parameter space (i.e. for very high and very low concentrations), even at infinite SNR (noiseless training and testing). Moreover, training with uniform datasets or if augmented with critical cases showed to be insufficient to prevent biases. This is dangerous in a clinical context that requires the algorithm to be unbiased also for concentrations far from the norm, which may well be the focus of the investigation since these correspond to pathology, the target of the diagnostic investigation.\n"}, {"title": "Reliability-aware Contrastive Self-ensembling for Semi-supervised Medical Image Classification", "abstract": "Self-ensembling framework has proven to be a powerful paradigm for semi-supervised medical image classification by leveraging abundant unlabeled data. However, the unlabeled data used in most of self-ensembling methods are equally weighted, which adversely affects the classification performance of models when difference exists among unlabeled data acquired from different populations, equipment and environments. To address this issue, we propose a novel reliability-aware contrastive self-ensembling framework, which can leverage the reliable unlabeled data selectively. Concretely, we introduce a weight function to the mean teacher paradigm for mapping the probability predictions of unlabeled data to corresponding weights that reflect their reliability. Hence, we can safely leverage the predictions of related unlabeled data under different perturbations to construct a reliable consistency loss. Besides, we further design a novel reliable contrastive loss to achieve better intra-class compactness and inter-class separability for the normalized embeddings derived from related unlabeled data. As a result, our reliability-aware scheme enables the contrastive self-ensembling framework concurrently capture both the reliable data-level and data-structure-level information, thereby improving the robustness and generalization power of the model. Experiments on two publicly available medical image datasets demonstrate the superiority of the proposed method. Our model is available at https://github.com/Mwnic/RAC-MT.\n"}, {"title": "ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification", "abstract": "Whole slide image (WSI) classification often relies on deep weakly supervised multiple instance learning (MIL) methods to handle gigapixel resolution images and slide-level labels. Yet the descent performance of deep learning comes from harnessing massive datasets and diverse samples, urging the need of efficient training pipelines for scaling to large datasets and data augmentation techniques for diversifying samples. However, current MIL-based WSI classification pipelines are memory-expensive and computation-inefficient since they usually assemble tens of thousands of patches as bags for computation. On the other hand, data augmentations, despite their popularity in other tasks, are much unexplored for WSI MIL frameworks. To address them, we propose ReMix, a general and efficient framework for MIL based WSI classification. It comprises two steps: reduce and mix. First, it reduces the number of instances in WSI bags by substituting instances with instance prototypes, i.e., patch cluster centroids. Then, we propose \u00e2\u0080\u009cMix-the-bag\u00e2\u0080\u009d aug- mentation that contains four online, stochastic and flexible latent space augmentations. It brings diverse and reliable class-identity-preserving semantic changes in the latent space while enforcing semantic-perturbation invariance. We evaluate ReMix on two public datasets with two state-of-the-art MIL methods. In our experiments, consistent improvements in precision, accuracy, and recall have been achieved but with orders of magnitude reduced training time and memory consumption, demonstrating ReMix\u00e2\u0080\u0099s effectiveness and efficiency. Code is available."}, {"title": "RemixFormer: A Transformer Model for Precision Skin Tumor Differential Diagnosis via Multi-modal Imaging and Non-imaging Data", "abstract": "Skin tumor is one of the most common diseases worldwide and the survival rate could be drastically increased if the cancerous lesions were identified early. Intrinsic visual ambiguities displayed by skin tumors in multi-modal imaging data impose huge amounts of challenges to diagnose them precisely, especially at the early stage. To achieve high diagnosis accuracy or precision, all possibly available clinical data (imaging and/or non-imaging) from multiple sources are used, and even the missing-modality problem needs to be tackled when some modality may become unavailable. To this end, we first devise a new disease-wise pairing of all accessible patient data if they fall into the same disease category as a remix operation of data samples. A novel cross-modality-fusion module is also proposed and integrated with our transformer-based multi-modality deep classification framework that can effectively perform multi-source data fusion (i.e., clinical images, dermoscopic images and accompanied with clinical patient-wise metadata) for skin tumors. Extensive quantitative experiments are conducted. We achieve an absolute 6.5% increase in averaged F1 and 2.8% in accuracy for the classification of five common skin tumors by comparing to the prior leading method on Derm7pt dataset of 1011 cases. More importantly, our method obtains an overall 88.5% classification accuracy using a large-scale in-house dataset of 5601 patients and in ten skin tumor classes (pigmented and non-pigmented). This experiment further validates the robustness and implies the potential clinical usability of our method, in a more realistic and pragmatic clinic setting.\n"}, {"title": "Removal of Confounders via Invariant Risk Minimization for Medical Diagnosis", "abstract": "While deep networks have demonstrated state-of-the-art performance in medical image analysis, they suffer from biases caused by undesirable confounding variables (e.g., sex, age, race). Traditional statistical methods for removing confounders are often incompatible with modern deep networks. To address this challenge, we introduce a novel learning framework, named ReConfirm, based on the invariant risk minimization (IRM) theory to eliminate the biases caused by confounding variables and make deep networks more robust. Our approach allows end-to-end model training while capturing causal features responsible for pathological findings instead of spurious correlations. We evaluate our approach on NIH chest X-ray classification tasks where sex and age are confounders. \n"}, {"title": "RepsNet: Combining Vision with Language for Automated Medical Reports", "abstract": "Writing reports by analyzing medical images is error-prone for inexperienced practitioners and time consuming for experienced ones. In this work, we present RepsNet that adapts pre-trained vision and language models to interpret medical images and generate automated reports in natural language. RepsNet consists of an encoder-decoder model: the encoder aligns the images with natural language descriptions via contrastive learning, while the decoder predicts answers by conditioning on encoded images and prior context of descriptions retrieved by nearest neighbor search. We formulate the problem in a visual question answering setting to handle both categorical and descriptive natural language answers. We perform experiments on two challenging tasks of medical visual question answering (VQA-Rad) and report generation (IU-Xray) on radiology image datasets. Results show that RepsNet outperforms state-of-the-art methods with 81.08 % classification accuracy on VQA-Rad 2018 and 0.58 BLEU-1 score on IU-Xray. Supplementary details are available at: https://sites.google.com/view/repsnet\n"}, {"title": "Residual Wavelon Convolutional Networks for Characterization of Disease Response on MRI", "abstract": "Wavelets have shown significant promise for medical image decomposition and artifact pre-processing by representing inputs via shifted and scaled components of a specified mother wavelet function. However, wavelets could also be leveraged within deep neural networks as activation functions for neurons (called wavelons) in the hidden layer. Integrating wavelons into a convolutional neural network architecture (termed a ``wavelon network\u00e2\u0080\u009d (WN)) offers additional flexibility and stability during optimization, but the resulting model complexity has caused it to be limited to low-dimensional applications. Towards addressing these issues, we present the Residual Wavelon Convolutional Network (RWCN), a novel integrated WN architecture that employs weighted skip connections (to enable residual learning) together with image convolutions and wavelet activation functions to more efficiently capture high-dimensional disease response-specific patterns from medical imaging data. In addition to developing the analytical basis for wavelet activation functions as used in this work, we implemented RWCNs by adapting the popular VGG and ResNet architectures. Evaluation was conducted within three different challenging clinical problems: (a) predicting pathologic complete response (pCR) to neoadjuvant chemoradiation via 153 pre-treatment T2-weighted (T2w) MRI scans in rectal cancers, (b) evaluating pCR after chemoradiation via 100 post-treatment T2w MRIs in rectal cancers, as well as (c) risk stratifying patients who will or will not require surgery after aggressive medication in Crohn\u00e2\u0080\u0099s disease using 73 baseline MRI scans. In comparison to 4 state-of-the-art alternative models (VGG-16, VGG-19, ResNet-18, ResNet-50), RWCN architectures yielded significantly improved and more efficient classifier performance on unseen data in multi-institutional validation cohorts (hold-out accuracies of 0.82, 0.85, and 0.88, respectively).\n"}, {"title": "Rethinking Breast Lesion Segmentation in Ultrasound: A New Video Dataset and A Baseline Network", "abstract": "Automatic breast lesion segmentation in ultrasound (US) videos is an essential prerequisite for early diagnosis and treatment. This challenging task remains under-explored due to the lack of availability of annotated US video dataset.\nThough recent works have achieved better performance in natural video object segmentation by introducing promising Transformer architectures, they still suffer from spatial inconsistency as well as huge computational costs. Therefore, in this paper, we first present a new benchmark dataset designed for US video segmentation. Then, we propose a dynamic parallel spatial-temporal Transformer (DPSTT) to improve the performance of lesion segmentation in US videos with higher computational efficiency. Specifically, the proposed DPSTT disentangles the non-local Transformer along the temporal and spatial dimensions, respectively. The temporal Transformer attends temporal lesion movement on different frames at the same regions,  and the spatial Transformer focuses on similar context information between the previous and the current frames. Furthermore, we propose a dynamic selection scheme to effectively sample the most relevant frames from all the past frames, and thus prevent out of memory during inference. Finally, we conduct extensive experiments to evaluate the efficacy of the proposed DPSTT on the new US video benchmark dataset. \n"}, {"title": "Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches", "abstract": "Surgical captioning plays an important role in surgical instruction prediction and report generation. However, the majority of captioning models still rely on the heavy computational object detector or feature extractor to extract regional features. In addition, the detection model requires additional bounding box annotation which is costly and needs skilled annotators. These lead to inference delay and limit the captioning model to deploy in real-time robotic surgery. For this purpose, we design an end-to-end detector and feature extractor-free captioning model by utilizing the patch-based shifted window technique. We propose Shifted Window-Based Multi-Layer Perceptrons Transformer Captioning model (SwinMLP-TranCAP) with faster inference speed and less computation. SwinMLP-TranCAP replaces the multi-head attention module with window-based multi-head MLP. Such deployments primarily focus on image understanding tasks, but very few works investigate the caption generation task. SwinMLP-TranCAP is also extended into a video version for video captioning tasks using 3D patches and windows. Compared with previous detector-based or feature extractor-based models, our models greatly simplify the architecture design while maintaining performance on two surgical datasets.\n"}, {"title": "Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need", "abstract": "Data diversity and volume are crucial to the success of training deep learning models, while in the medical imaging field, the difficulty and cost of data collection and annotation are especially huge. Specifically in robotic surgery, data scarcity and imbalance have heavily affected the model accuracy and limited the design and deployment of deep learning-based surgical applications such as surgical instrument segmentation. Considering this, we rethink the surgical instrument segmentation task and propose a one-to-many data generation solution that gets rid of the complicated and expensive process of data collection and annotation from robotic surgery. In our method, we only utilize a single surgical background tissue image and a few open-source instrument images as the seed images and apply multiple augmentations and blending techniques to synthesize amounts of image variations. In addition, we also introduce the chained augmentation mixing during training to further enhance the data diversities. The proposed approach is evaluated on the real datasets of the EndoVis-2018 and EndoVis-2017 surgical scene segmentation. Our empirical analysis suggests that without the high cost of data collection and annotation, we can achieve decent surgical instrument segmentation performance. Moreover, we also observe that our method can deal with novel instrument prediction in the deployment domain. We hope our inspiring results will encourage researchers to emphasize data-centric methods to overcome demanding deep learning limitations besides data shortage, such as class imbalance, domain adaptation, and incremental learning. Our code is available at https://github.com/lofrienger/Single_SurgicalScene_For_Segmentation.\n"}, {"title": "Retrieval of surgical phase transitions using reinforcement learning", "abstract": "In minimally invasive surgery, surgical workflow segmentation from video analysis is a well studied topic. The conventional approach defines it as a multi-class classification problem, where individual video frames are attributed a surgical phase label."}, {"title": "Revealing Continuous Brain Dynamical Organization with Multimodal Graph Transformer", "abstract": "Brain large-scale dynamics is constrained by the heterogeneity of intrinsic anatomical substrate. Little is known how the spatio-temporal dynamics adapt for the heterogeneous structural connectivity (SC). Modern neuroimaging modalities make it possible to study the intrinsic brain activity at the scale of seconds to minutes. Diffusion magnetic resonance imaging (dMRI) and functional MRI reveals the large-scale SC across different brain regions. Electrophysiological methods (i.e. MEG/EEG) provide direct measures of neural activity and exhibits complex neurobiological temporal dynamics which could not be solved by fMRI. However, most of existing multimodal analytical methods collapse the brain measurements either in space or time domain and fail to capture the spatio-temporal circuit dynamics. In this paper, we propose a novel spatio-temporal graph Transformer model to integrate the structural and functional connectivity in both spatial and temporal domain. The proposed method learns the heterogeneous node and graph representation via contrastive learning and multi-head attention based graph Transformer using multimodal brain data (i.e. fMRI, MRI, MEG and behavior performance). The proposed contrastive graph Transformer representation model incorporates the heterogeneity map constrained by T1-to-T2-weighted (T1w/T2w) to improve the model fit to structure-function interactions. The experimental results with multimodal resting state brain measurements demonstrate the proposed method could highlight the local properties of large-scale brain spatio-temporal dynamics and capture the dependence strength between functional connectivity and behaviors. In summary, the proposed method enables the complex brain dynamics explanation for different modal variants.\n"}, {"title": "Rib Suppression in Digital Chest Tomosynthesis", "abstract": "Digital chest tomosynthesis (DCT) is a technique to produce sectional 3D images of a human chest for pulmonary disease screening, with 2D X-ray projections taken within an extremely limited range of angles. However, under the limited angle scenario, DCT contains strong artifacts caused by the presence of ribs, jamming the imaging quality of the lung area. Recently, great progress has been achieved for rib suppression in a single X-ray image, to reveal a clearer lung texture. We firstly extend the rib suppression problem to the 3D case at the software level. We propose a Tomosynthesis RIb SuPpression and Lung Enhancement Network (TRIPLE-Net) to model the 3D rib component and provide a rib-free DCT. TRIPLE-Net takes the advantages from both 2D and 3D domains, which models the ribs in DCT with the exact FBP procedure and 3D depth information, respectively. The experiments on simulated datasets and clinical data have shown the effectiveness of TRIPLE-Net to preserve lung details as well as improve the imaging quality of pulmonary diseases. Finally, an expert user study confirms our findings. Our code is available at https://github.com/sunyh1/Rib-Suppression-in-Digital-Chest-Tomosynthesis.\n"}, {"title": "Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining", "abstract": "Retrospective analysis of brain MRI scans acquired in the clinic has the potential to enable neuroimaging studies with sample sizes much larger than those found in research datasets. However, analysing such clinical images ``in the wild\u00e2\u0080\u0099\u00e2\u0080\u0099 is challenging, since subjects are scanned with highly variable protocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent advances in convolutional neural networks (CNNs) and domain randomisation for image segmentation, best represented by the publicly available method \u00e2\u0080\u009cSynthSeg\u00e2\u0080\u009d, may enable morphometry of clinical MRI at scale. In this work, we first evaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000 scans acquired at Massachusetts General Hospital. We show that SynthSeg is generally robust, but frequently falters on scans with low signal-to-noise ratio or poor tissue contrast. Next, we propose \u00e2\u0080\u009cSynthSeg+\u00e2\u0080\u009d, a novel method that greatly mitigates these problems using a hierarchy of conditional segmentation and denoising CNNs. We show that this method is considerably more robust than SynthSeg, while also outperforming cascaded networks and state-of-the-art segmentation denoising methods. Finally, we apply our approach to a proof-of-concept volumetric study of ageing, where it closely replicates atrophy patterns observed in research studies conducted on high-quality, 1mm, T1-weighted scans. The code and trained model are publicly available at https://github.com/BBillot/SynthSeg.\n"}, {"title": "RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans", "abstract": "In clinical practice, anisotropic volumetric medical images with low through-plane resolution are commonly used due to short acquisition time and lower storage cost. Nevertheless, the coarse resolution may lead to difficulties in medical diagnosis by either physicians or computer-aided diagnosis algorithms. Deep learning-based volumetric super-resolution (SR) methods are feasible ways to improve resolution, with convolutional neural networks (CNN) at their core. Despite recent progress, these methods are limited by inherent properties of convolution operators, which ignore content relevance and cannot effectively model long-range dependencies. In addition, most of the existing methods use pseudo-paired volumes for training and evaluation, where pseudo low-resolution (LR) volumes are generated by a simple degradation of their high-resolution (HR) counterparts. However, the domain gap between pseudo- and real-LR volumes leads to the poor performance of these methods in practice. In this paper, we build the first public real-paired dataset RPLHR-CT as a benchmark for volumetric SR, and provide baseline results by re-implementing four state-of-the-art CNN-based methods. Considering the inherent shortcoming of CNN, we also propose a transformer volumetric super-resolution network (TVSRN) based on attention mechanisms, dispensing with convolutions entirely. This is the first research to use a pure transformer for CT volumetric SR. The experimental results show that TVSRN significantly outperforms all baselines on both PSNR and SSIM. Moreover, the TVSRN method achieves a better trade-off between the image quality, the number of parameters, and the running time. Data and code are available at https://github.com/smilenaxx/RPLHR-CT.\n"}, {"title": "RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation", "abstract": "Accurately segmenting temporal frames of cine magnetic resonance imaging (MRI) is a crucial step in various real-time MRI guided cardiac interventions. To achieve fast and accurate visual assistance, there are strict requirements on the maximum latency and minimum throughput of the segmentation framework. State-of-the-art neural networks on this task are mostly hand-crafted to satisfy these constraints while achieving high accuracy. On the other hand, existing literature has demonstrated the power of neural architecture search (NAS) in automatically identifying the best neural architectures for various medical applications, within which differentiable NAS is a prevailing and efficient approach. However, they are mostly guided by accuracy, sometimes with computation complexity, but the importance of real-time constraints are overlooked. A major challenge is that such constraints are non-differentiable and thus are not compatible with the widely used differentiable NAS frameworks. In this paper, we present a strategy that can\ndirectly handle real-time constraints in differentiable NAS frameworks,\nnamed RT-DNAS. Experiments on extended 2017 MICCAI ACDC\ndataset show that compared with state-of-the-art manually and auto-\nmatically designed architectures, RT-DNAS is able to identify neural\narchitectures that can achieve better accuracy while satisfying the real-\ntime constraints\n"}, {"title": "RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment", "abstract": "Coronary CT Angiography (CCTA) is susceptible to various distortions (e.g., artifacts and noise), which severely compromise the exact diagnosis of cardiovascular diseases. The appropriate CCTA Vessel-level Image Quality Assessment (CCTA VIQA) algorithm can be used to reduce the risk of error diagnosis. The primary challenges of CCTA VIQA are that the local part of coronary that determines final quality is hard to locate. To tackle the challenge, we formulate CCTA VIQA as a multiple-instance learning (MIL) problem, and exploit Transformer-based MIL module (termed as T-MIL) to aggregate the multiple instances along the coronary centerline into the final quality. However, not all instances are informative for final quality. There are some quality-irrelevant/negative instances intervening the exact quality assessment(e.g., instances covering only background or the coronary in instances is not identifiable). Therefore, we propose a Progressive Reinforcement learning based Instance Discarding module (termed as PRID) to progressively remove quality-irrelevant/negative instances for CCTA VIQA. Based on the above two modules, we propose a Reinforced Transformer Network (RTN) for automatic CCTA VIQA based on end-to-end optimization. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on the real-world\nCCTA dataset, exceeding previous MIL methods by a large margin.\n"}, {"title": "S3R: Self-supervised Spectral Regression for Hyperspectral Histopathology Image Classification", "abstract": "Benefited from the rich and detailed spectral information in hyperspectral images (HSI), HSI offers great potential for a wide variety of medical applications such as computational pathology. But, the lack of adequate annotated data and the high spatiospectral dimensions of HSIs usually make classification networks prone to overfit. Thus, learning a general representation which can be transferred to the downstream tasks is imperative. To our knowledge, no appropriate self-supervised pre-training method has been designed for HSIs. In this paper, we introduce an efficient and effective Self-supervised Spectral Regression (S$^3$R) method, which exploits the low rank characteristic in the spectral domain of HSI. More concretely, we propose to learn a set of linear coefficients that can be used to represent one band by the remaining bands via masking out these bands. Then, the band is restored by using the learned coefficients to reweight the remaining bands. Two pre-text tasks are designed: (1) S$^3$R-CR, which regresses the linear coefficients, so that the pre-trained model understands the inherent structures of HSIs and the pathological characteristics of different morphologies; (2) S$^3$R-BR, which regresses the missing band, making the model to learn the holistic semantics of HSIs. Compared to prior arts, i.e., contrastive learning and masked image modeling methods, which focuses on natural images, S$^3$R converges at least 3 times faster, and achieves significant improvements up to 14\\% in accuracy when transferring to HSI classification tasks. \n"}, {"title": "S5CL: Unifying Fully-Supervised, Self-Supervised, and Semi-Supervised Learning Through Hierarchical Contrastive Learning", "abstract": "In computational pathology, we often face a scarcity of annotations and a large amount of unlabeled data. One method for dealing with this is semi-supervised learning which is commonly split into a self-supervised pretext task and a subsequent model fine-tuning. Here, we compress this two-stage training into one by introducing S5CL, a unified framework for fully-supervised, self-supervised, and semi-supervised learning. With three contrastive losses defined for labeled, unlabeled, and pseudo-labeled images, S5CL can learn feature representations that reflect the hierarchy of distance relationships: similar images and augmentations are embedded the closest, followed by different looking images of the same class, while images from separate classes have the largest distance. Moreover, S5CL allows us to flexibly combine these losses to adapt to different scenarios. Evaluations of our framework on two public histopathological datasets show strong improvements in the case of sparse labels: for a H&E-stained colorectal cancer dataset, the accuracy increases by up to 9% compared to supervised cross-entropy loss; for a highly imbalanced dataset of single white blood cells from leukemia patient blood smears, the F1-score increases by up to 6%.\n"}, {"title": "Sample hardness based gradient loss for long-tailed cervical cell detection", "abstract": "Due to the difficulty of cancer samples collection and annotation, cervical cancer datasets usually exhibit a long-tailed data distribution. When training a detector to detect the cancer cells in a WSI (Whole Slice Image) image captured from the TCT (Thinprep Cytology Test) specimen, head categories (e.g. normal cells and inflammatory cells) typically have a much larger number of samples than tail categories (e.g. cancer cells). Most existing state-of-the-art long-tailed learning methods in object detection focus on category distribution statistics to solve the problem in the long-tailed scenario, without considering the ``hardness\u00e2\u0080\u0099\u00e2\u0080\u0099 of each sample. To address this problem, in this work we propose a Grad-Libra Loss that leverages the gradients to dynamically calibrate the degree of hardness of each sample for different categories, and re-balance the gradients of positive and negative samples. Our loss can thus help the detector to put more emphasis on those hard samples in both head and tail categories. Extensive experiments on a long-tailed TCT WSI image dataset show that the mainstream detectors, e.g. RepPoints, FCOS, ATSS, YOLOF, etc. trained using our proposed Gradient-Libra Loss, achieved much higher (7.8%) mAP than that trained using cross-entropy classification loss. \n"}, {"title": "SAPJNet: Sequence-Adaptive Prototype-Joint Network for Small Sample Multi-Sequence MRI Diagnosis", "abstract": "Multi-sequence magnetic-resonance-imaging (MRI) images have complementary information that can greatly improve the reliability of diagnosis. However, automated diagnosis of small sample multi-sequence MR images is a challenging task due to: 1) Divergent representation. The difference between sequences and the weak correlation between contained features make the representation extracted from the network tend to diverge, which is profitless to robust classification. 2) Sparse distribution. The small sample size is reflected in the sparse distribution of the prototype, making the network only learn rough demarcation, which is inadequate to medical images with small class interval. In this paper, we propose for the first time a network (SAPJNet) that can adapt to both multi-sequence and small sample conditions, enabling high-quality automatic diagnosis of small-sample multi-sequence MR images, which is of great help to improve clinical diagnostic efficiency. 1) The sequence-adaptive transformer (SAT) of SAPJNet generates joint representations as disease prototypes by filtering intra-sequence features and aggregating inter-sequence features. 2) A prototype optimization strategy (POS) of SAPJNet constrains the prototype distribution by approximating the intra-class prototype and alienating the inter-class prototype. The SAPJNet achieved optimal performance in three tasks: risk assessment of Pulmonary arterial hypertension, classification of idiopathic inflammatory myopathies, and identification of knee abnormalities, with at least a 10%, 10%, and 6.7% improvement in accuracy over all comparison methods.\n"}, {"title": "SATr: Slice Attention with Transformer for Universal Lesion Detection", "abstract": "Universal Lesion Detection (ULD) in computed tomography plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by multi-slice-input detection approaches which model 3D context from multiple adjacent CT slices, but such methods still experience difficulty in obtaining a global representation among different slices and within each individual slice since they only use convolution-based fusion operations. In this paper, we propose a novel Slice Attention Transformer (SATr) block which can be easily plugged into convolution-based ULD backbones to form hybrid network structures. Such newly formed hybrid backbones can better model long-distance feature dependency via the cascaded self-attention modules in the Transformer block while still holding a strong power of modeling local features with the convolutional operations in the original backbone. Experiments with five state-of-the-art methods show that the proposed SATr block can provide an almost free boost to lesion detection accuracy without extra hyperparameters or unique network designs.\n"}, {"title": "Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction", "abstract": "Unrolled neural networks have enabled state-of-the-art reconstruction performance and fast inference times for the accelerated magnetic resonance imaging (MRI) reconstruction task. However, these approaches depend on fully-sampled scans as ground truth data which is either costly or not possible to acquire in many clinical medical imaging applications; hence, reducing dependence on data is desirable. In this work, we propose modeling the proximal operators of unrolled neural networks with scale-equivariant convolutional neural networks in order to improve the data-efficiency and robustness to drifts in scale of the images that might stem from the variability of patient anatomies or change in field-of-view across different MRI scanners. Our approach demonstrates strong improvements over the state-of-the-art unrolled neural networks under the same memory constraints both with and without data augmentations on both in-distribution and out-of-distribution scaled images without significantly increasing the train or inference time.\n"}, {"title": "Screening of Dementia on OCTA Images via Multi-projection Consistency and Complementarity", "abstract": "It has been suggested that the retinal vasculature alternations are associated with dementia in recent clinical studies, and the eye examination may facilitate the early screening of dementia. Optical Coherence Tomography Angiography (OCTA) has shown its superiority in visualizing superficial vascular complex (SVC), deep vascular complex (DVC), and choriocapillaris, and it has been extensively used in clinical practice. However, the information in OCTA is far from fully mined by existing methods, which straightforwardly analyze the multiple projections of OCTA by average or concatenation. These methods do not take into account the relationship between multiple projections. Accordingly, a Multi-projection Consistency and complementarity Learning Network (MUCO-Net) is proposed in this paper to explore the diagnosis of dementia based on OCTA. Firstly, a consistency and complementarity attention (CsCp) module is developed to understand the complex relationships among various projections. Then, a cross-view fusion (CVF) module is introduced to combine the multi-scale features from the CsCp. In addition, the number of input flows of the proposed modules is flexible to boost the interactions across the features from different projections. In the experiment, MUCO-Net is implemented on two OCTA datasets to screen for dementia and diagnose fundus diseases. The effectiveness of MUCO-Net is demonstrated by its superior performance to state-of-the-art methods. \n"}, {"title": "Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations", "abstract": "Recently, weakly-supervised image segmentation using weak annotations like scribbles has gained great attention, since such annotations are much easier to obtain compared to time-consuming and label-intensive labeling at the pixel/voxel level. However, because scribbles lack structure information of region of interest (ROI), existing scribble-based methods suffer from poor boundary localization. Furthermore, most current methods are designed for 2D image segmentation, which do not fully leverage  the  volumetric  information  if  directly  applied  to  image  slices. In  this  paper,  we  propose  a  scribble-based  volumetric  image  segmentation,  Scribble2D5,  which  tackles  3D  anisotropic  image  segmentation and improves boundary prediction. To achieve this, we augment a 2.5D attention UNet with a proposed label propagation module to extend semantic information from scribbles and a combination of static and active boundary prediction to learn ROI\u00e2\u0080\u0099s boundary and regularize its shape. Extensive experiments on three public datasets demonstrate Scribble2D5 significantly outperforms current scribble-based methods and approaches the performance of fully-supervised ones. Our code is available online.\n"}, {"title": "Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision", "abstract": "Medical image segmentation plays an irreplaceable role in computer-assisted diagnosis, treatment planning and following-up. Collecting and annotating a large-scale dataset is crucial to training a powerful segmentation model, but producing high-quality segmentation masks is an expensive and time-consuming procedure. Recently, weakly-supervised learning that uses sparse annotations (points, scribbles, bounding boxes) for network training has achieved encouraging performance and shown the potential for annotation cost reduction. However, due to the limited supervision signal of sparse annotations, it is still challenging to employ them for networks training directly. In this work, we propose a simple yet efficient scribble-supervised image segmentation method and apply it to cardiac MRI segmentation. Specifically, we employ a dual-branch network with one encoder and two slightly different decoders for image segmentation and dynamically mix the two decoders\u00e2\u0080\u0099 predictions to generate pseudo labels for auxiliary supervision. By combining the scribble supervision and auxiliary pseudo labels supervision, the dual-branch network can efficiently learn from scribble annotations end-to-end. Experiments on the public ACDC dataset show that our method performs better than current scribble-supervised segmentation methods and also outperforms several semi-supervised segmentation methods. Code and data at: https://anonymous.4open.science/r/MICCAI22-4E2B.\n"}, {"title": "SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors", "abstract": "Optical coherence tomography (OCT) is a non-invasive 3D modality widely used in ophthalmology for imaging the retina. Achieving automated, anatomically coherent retinal layer segmentation on OCT is important for the detection and monitoring of different retinal diseases, like Age-related Macular Disease (AMD) or Diabetic Retinopathy. However, the majority of state-of-the-art layer segmentation methods are based on purely supervised deep-learning, requiring a large amount of pixel-level annotated data that is expensive and hard to obtain. With this in mind, we introduce a semi-supervised paradigm into the retinal layer segmentation task that makes use of the information present in large-scale unlabeled datasets as well as anatomical priors. In particular, a novel fully differentiable approach is used for converting surface position regression into a pixel-wise structured segmentation, allowing to use both 1D surface and 2D layer representations in a coupled fashion to train the model. In particular, these 2D segmentations are used as anatomical factors that, together with learned style factors, compose disentangled representations used for reconstructing the input image. In parallel, we propose a set of anatomical priors to improve network training when a limited amount of labeled data is available. We demonstrate on the real-world dataset of scans with intermediate and wet-AMD that our method outperforms state-of-the-art when using our full training set, but more importantly largely exceeds state-of-the-art when it is trained with a fraction of the labeled data. \n"}, {"title": "SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer", "abstract": "Clinically, the accurate annotation of lesions/tissues can significantly facilitate the disease diagnosis. For example, the segmentation of optic disc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the segmentation of skin lesions on dermoscopic images is helpful to the melanoma diagnosis, etc. With the advancement of deep learning techniques, a wide range of methods proved the lesions/tissues segmentation can also facilitate the automated disease diagnosis models. However, existing methods are limited in the sense that they can only capture static regional correlations in the images. Inspired by the global and dynamic nature of Vision Transformer, in this paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans) to transfer the segmentation knowledge to the disease diagnosis network. Specifically, we first propose an asymmetric multi-scale interaction strategy to correlate each single low-level diagnosis feature with multi-scale segmentation features. Then, an effective strategy called SeA-block is adopted to vitalize diagnosis feature via correlated segmentation features. To model the segmentation-diagnosis interaction, SeA-block first embeds the diagnosis feature based on the segmentation information via the encoder, and then transfers the embedding back to the diagnosis feature space by a decoder. Experimental results demonstrate that SeATrans surpasses a wide range of state-of-the-art (SOTA) segmentation-assisted diagnosis methods on several disease diagnosis tasks.\n"}, {"title": "Segmentation of Whole-brain Tractography: A Deep Learning Algorithm Based on 3D Raw Curve Points", "abstract": "Segmentation of whole-brain fiber tractography into anatomically meaningful fiber bundles is an important step for visualizing and quantitatively assessing white matter tracts. The fiber streamlines in whole-brain fiber tractography are 3D curves, and they are densely and complexly connected throughout the brain. Due to the huge volume of curves, varied connection complexity, and imaging technology limitations, whole-brain tractography segmentation is still a difficult task.  In this study, a novel deep learning architecture has been proposed for segmenting whole-brain tractography into 10 major white matter bundles and the \u00e2\u0080\u009cother fibers\u00e2\u0080\u009d category. The proposed PointNet based CNN architecture takes the whole-brain fiber curves in the form of 3D raw curve points and successfully segments them using a manually created large-scale training dataset. To improve segmentation performance, the approach employs two channel-spatial attention modules. The proposed method was tested on healthy adults across the lifespan with imaging data from the ADNI project database. In terms of experimental evidence, the proposed deep learning architecture demonstrated solid performance that is better than the state-of-the-art. \n"}, {"title": "Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification", "abstract": "Vision Transformers (ViT) are competing to replace Convolutional Neural Networks (CNN) for various computer vision tasks in medical imaging such as classification and segmentation. While the vulnerability of CNNs to adversarial attacks is a well-known problem, recent works have shown that ViTs are also susceptible to such attacks and suffer significant performance degradation under attack. The vulnerability of ViTs to carefully engineered adversarial samples raises serious concerns about their safety in clinical settings. In this paper, we propose a novel self-ensembling method to enhance the robustness of ViT in the presence of adversarial attacks. The proposed Self-Ensembling Vision Transformer (SEViT) leverages the fact that feature representations learned by initial blocks of a ViT are relatively unaffected by adversarial perturbations. Learning multiple classifiers based on these intermediate feature representations and combining these predictions with that of the final ViT classifier can provide robustness against adversarial attacks. Measuring the consistency between the various predictions can also help detect adversarial samples. Experiments on two modalities (chest X-ray and fundoscopy) demonstrate the efficacy of SEViT architecture to defend against various adversarial attacks in the gray-box (attacker has full knowledge of the target model, but not the defense mechanism) setting. Code: https://github.com/faresmalik/SEViT\n"}, {"title": "Self-learning and One-shot Learning based Single-slice Annotation for 3D Medical Image Segmentation", "abstract": "As deep learning methods continue to improve medical image segmentation performance, data annotation is still a big bottleneck due to the labor-intensive and time-consuming burden on medical experts, especially for 3D images. To significantly reduce annotation efforts while attaining competitive segmentation accuracy, we propose a self-learning and one-shot learning based framework for 3D medical image segmentation by annotating only one slice of each 3D image. Our approach takes two steps: (1) self-learning of a reconstruction network to learn semantic correspondence among 2D slices within 3D images, and (2) representative selection of single slices for one-shot manual annotation and propagating the annotated data with the well-trained reconstruction network. Extensive experiments verify that our new framework achieves comparable performance with less than 1% annotated data compared with fully supervised methods and generalizes well on several out-of-distribution testing sets.\n"}, {"title": "SelfMix: A Self-adaptive Data Augmentation Method for Lesion Segmentation", "abstract": "Deep learning-based methods have obtained promising results in various organ segmentation tasks, due to their effectiveness in learning feature representation. However, accurate segmentation of lesions\ncan still be challenging due to 1) the lesions provide less information than normal organs; 2) the available number of labeled lesions is more limited than normal organs; 3) the morphology, shape, and size of lesions are more diverse than normal organs. To increase the number of lesion samples and further boost the performance of various lesion segmentation, in this paper, we propose a simple but effective lesion-aware data augmentation method called Self-adaptive Data Augmentation (SelfMix). Compared with existing data augmentation methods, such as Mixup, CutMix, and CarveMix, our proposed SelfMix have three-fold advances: 1) Solving the challenges that the generated tumor images are facing the problem of distortion by absorbing both tumor and non-tumor information; 2) SelfMix is tumor-aware, which can adaptively adjust the fusing weights of each lesion voxels based on the geometry and size information from the tumor itself; 3) SelfMix is the first one that notices non-tumor information. To evaluate the proposed data augmentation method, experiments were performed on two public lesion segmentation datasets. The results show that our method improves the lesion segmentation accuracy compared with other data augmentation approaches.\n"}, {"title": "Self-Rating Curriculum Learning for Localization and Segmentation of Tuberculosis on Chest Radiograph", "abstract": "Tuberculosis (TB) is the second leading cause of infectious disease death, and chest X-ray is one of the most commonly used methods to detect TB. In this work, we bring forward a Self-Rating Curriculum Learning (SRCL) method to exploit the task of localization and segmentation of tuberculosis on chest radiographs. A total number of 12,000 CXR images of healthy sub-jects and bacteriologically-confirmed TB patients, retrospectively collected from multi-center local hospitals, are used in the study. A classical instance localization and segmentation framework (Mask-RCNN with backbone Resnet-50) is presented to compare traditional one-step training method and our proposed SRCL method in metrics and efficiency. First, a teacher model with self-rating function without human participation is developed to output the rating score of each sample, and all the samples are classified into three categories, namely easy set, moderate set and hard set, by using kernel density estimate (KDE) plot. After grouping the cases images in order of difficulty, the SRCL training is conducted on progressively harder images in three stages. We evaluate the proposed SRCL method in metrics and efficiency. Results indicate that the proposed SRCL method is able to boost the performance of the compared traditional method.\n"}, {"title": "Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT)", "abstract": "Vision transformers, with their ability to more efficiently model long-range context, have demonstrated impressive accuracy gains in several computer vision and medical image analysis tasks including segmentation. However, such methods need large labeled datasets for training, which is hard to obtain for medical image analysis. Self-supervised learning (SSL) has demonstrated success in medical image segmentation using convolutional networks. In this work, we developed a self-distillation learning with masked image modeling method to perform SSL for vision transformers (SMIT) applied to 3D multi-organ segmentation from CT and MRI. Our contribution is a dense pixel-wise regression within masked patches called masked image prediction, which we combined with masked patch token distillation as pretext task to pre-train vision transformers. We show our approach is more accurate and requires fewer fine tuning datasets than other pretext tasks. Unlike prior medical image methods, which typically used image sets arising from disease sites and imaging modalities corresponding to the target tasks, we used 3,643 CT scans (602,708 images) arising from head and neck, lung, and kidney cancers as well as COVID-19 for pre-training and applied it to abdominal organs segmentation from MRI pancreatic cancer patients as well as publicly available 13 different abdominal organs segmentation from CT. Our method showed clear accuracy improvement (average DSC of 0.875 from MRI and 0.878 from CT) with reduced requirement for fine-tuning datasets over commonly used pretext tasks focusing on full image reconstruction. Extensive comparisons against multiple current SSL methods was done. Code will be made available upon acceptance for publication."}, {"title": "Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion", "abstract": "3D patient body modeling is critical to the success of automated patient positioning for smart medical scanning and operating rooms. Existing CNN-based end-to-end patient modeling solutions typically require a) customized network designs demanding large amount of relevant training data, covering extensive realistic clinical scenarios (e.g., patient covered by sheets), which leads to suboptimal generalizability in practical deployment, b) expensive 3D human model annotations, i.e., requiring huge amount of manual effort, resulting in systems that scale poorly. To address these issues, we propose a generic modularized 3D patient modeling method consists of (a) a multi-modal keypoint detection module with attentive fusion for 2D patient joint localization, to learn complementary cross-modality patient body information, leading to improved keypoint localization robustness and generalizability in a wide variety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy occlusions); and (b) a self-supervised 3D mesh regression module which does not require expensive 3D mesh parameter annotations to train, bringing immediate cost benefits for clinical deployment. We demonstrate the efficacy of the proposed method by extensive patient positioning experiments on both public and  clinical data. Our evaluation results achieve superior patient positioning performance across various imaging modalities in real clinical scenarios.\n"}, {"title": "Self-Supervised Depth Estimation in Laparoscopic Image using 3D Geometric Consistency", "abstract": "Depth estimation is a crucial step for image-guided intervention in robotic surgery and laparoscopic imaging system. Since per-pixel depth ground truth is difficult to acquire for laparoscopic image data, it is rarely possible to apply supervised depth estimation to surgical applications. As an alternative, self-supervised methods have been introduced to train depth estimators using only synchronized stereo image pairs. However, most recent work focused on the left-right consistency in 2D and ignored valuable inherent 3D information on the object in real world coordinates, meaning that the left-right 3D geometric structural consistency is not fully utilized. To overcome this limitation, we present M3Depth, a self-supervised depth estimator to leverage 3D geometric structural information hidden in stereo pairs while keeping monocular inference. The method also removes the influence of border regions unseen in at least one of the stereo images via masking, to enhance the correspondences between left and right images in overlapping areas. Intensive experiments show that the method outperforms previous self-supervised approaches on both a public dataset and a newly acquired dataset by a large margin, indicating a good generalization across different phantoms and laparoscopes. \n"}, {"title": "Self-Supervised Learning of Morphological Representation for 3D EM Segments with Cluster-Instance Correlations", "abstract": "Morphological analysis of various cells is essential for understanding brain functions. However, the massive data volume of electronic microscopy (EM) images brings significant challenges for cell segmentation and analysis. While obtaining sufficient data annotation for supervised deep learning methods is laborious and tedious, we propose the first self-supervised approach for learning 3D morphology representations from ultra-scale EM segments without any data annotations. Our approach, MorphConNet, leverages contrastive learning in both instance level and cluster level to enforce similarity between two augmented versions of the same segment and the compactness of representation distributions within clusters. Through experiments on the dense segmentation of the full-brain EM volume of an adult fly FAFB-FFN1, our MorphConNet shows effectiveness in learning morphological representation for accurate classification of cellular subcompartments such as somas, neurites, and glia. The self-supervised morphological representation will also facilitate other morphological analysis tasks in neuroscience. \n"}, {"title": "Self-Supervised Pre-Training for Nuclei Segmentation", "abstract": "The accurate segmentation of nuclei is crucial for cancer diagnosis and further clinical treatments. For semantic segmentation of nuclei, Vision Transformers (VT) have the potentiality to outperform Convolutional Neural Network (CNN) based models due to their ability to model long-range dependencies (i.e., global context). Usually, VT and CNN models are pre-trained with large-scale natural image dataset (i.e., ImageNet) in fully-supervised manner. However, pre-training nuclei segmentation models with ImageNet is not much helpful because of morphological and textural differences between natural image domain and medical image domain. Also, ImageNet-like large-scale annotated histology dataset rarely exists in medical image domain. In this paper, we propose a novel region-level Self-Supervised Learning (SSL) approach and corresponding triplet loss for pre-training semantic nuclei  segmentation model with unannotated histology images extracted from Whole Slide Images (WSI). Our proposed region-level SSL is based on the observation that, non-background (i.e., nuclei) patches of an input image are difficult to predict from surrounding neighbor patches, and vice versa. We empirically demonstrate the superiority of our proposed SSL incorporated VT model on two public nuclei segmentation datasets.\n"}, {"title": "Semi-supervised histological image segmentation via hierarchical consistency enforcement", "abstract": "Acquiring pixel-level annotations for histological image segmentation is time- and labor-consuming. Semi-supervised learning enables learning from the unlabeled and limited amount of labeled data. A challenging issue is the inconsistent and uncertain predictions on unlabeled data. To enforce invariant predictions over the perturbations applied to the hidden feature space, we propose a Mean-Teacher based hierarchical consistency enforcement (HCE) framework and a novel hierarchical consistency loss (HC-loss) with learnable and self-guided mechanisms. Specifically, the HCE takes the perturbed versions of the hierarchical features from the encoder as input to the auxiliary decoders, and encourages the predictions of the auxiliary decoders and the main decoder to be consistent. The HC-loss facilitates the teacher model to generate reliable guidance and enhances the consistency among all the decoders of the student model. The proposed method is simple, yet effective, which can easily be extended to other frameworks. The quantitative and qualitative experimental results indicate the effectiveness of the hierarchical consistency enforcement on the MoNuSeg and CRAG datasets.\n"}, {"title": "Semi-supervised Learning for Nerve Segmentation in Corneal Confocal Microscope Photography", "abstract": "Corneal nerve fiber medical indicators are promising metrics for diagnosis of diabetic peripheral neuropathy. However, automatic nerve segmentation still faces the issues of insufficient data and expensive annotations. We propose a semi-supervised learning framework for CCM image segmentation. It includes self-supervised pre-training, supervised fine-tuning and self-training. The contrastive learning for pre-training pays more attention to global features and ignores local semantics, which is not friendly to the downstream segmentation task. Consequently, we adopt pre-training using masked image modeling as a proxy task on unlabeled images. After supervised fine-tuning, self-training is employed to make full use of unlabeled data. Experimental results show that our proposed method is effective and better than the supervised learning using nerve annotations with three-pixel-width dilation.\n"}, {"title": "Semi-supervised learning with data harmonisation for biomarker discovery from resting state fMRI", "abstract": "Computational models often overfit on neuroimaging datasets (which are high-dimensional and consist of small sample sizes), resulting in poor inferences such as ungeneralisable biomarkers. One solution is to pool datasets (of similar disorders) from other sites to augment the small dataset, but such efforts have to handle variations introduced by site effects and inconsistent labelling. To overcome these issues, we propose an encoder-decoder-classifier architecture that combines semi-supervised learning with harmonisation of data across sites. The architecture is trained end-to-end via a novel multi-objective loss function. Using the architecture on multi-site fMRI datasets such as ADHD-200 and ABIDE, we obtained significant improvement on classification performance and showed how site-invariant biomarkers were disambiguated from site-specific ones. Our findings demonstrate the importance of accounting for both site effects and labelling inconsistencies when combining datasets from multiple sites to overcome the paucity of data. With the proliferation of neuroimaging research conducted on retrospectively aggregated datasets, our architecture offers a solution to handle site differences and labelling inconsistencies in such datasets. Code is available at https://github.com/SCSE-Biomedical-Computing-Group/SHRED.\n"}, {"title": "Semi-Supervised Medical Image Classification with Temporal Knowledge-Aware Regularization", "abstract": "Semi-supervised learning (SSL) for medical image classification has achieved exceptional success on efficiently exploiting knowledge from unlabeled data with limited labeled data. Nevertheless, recent SSL methods suffer from misleading hard-form pseudo labeling, exacerbating the confirmation bias issue due to rough training process. Moreover, the training schemes excessively depend on the quality of generated pseudo labels, which is vulnerable against the inferior ones. In this paper, we propose TEmporal knowledge-Aware Regularization (TEAR) for semi-supervised medical image classification. Instead of using hard pseudo labels to train models roughly, we design Adaptive Pseudo Labeling (AdaPL), a mild learning strategy that relaxes hard pseudo labels to soft-form ones and provides a cautious training. AdaPL is built on a novel theoretically derived loss estimator, which approximates the loss of unlabeled samples according to the temporal information across training iterations, to adaptively relax pseudo labels. To release the excessive dependency of biased pseudo labels, we take advantage of the temporal knowledge and propose Iterative Prototype Harmonizing (IPH) to encourage the model to learn discriminative representations in an unsupervised manner. The core principle of IPH is to maintain the harmonization of clustered prototypes across different iterations. Both AdaPL and IPH can be easily incorporated into prior pseudo labeling-based models to extract features from unlabeled medical data for accurate classification. Extensive experiments on three semi-supervised medical image datasets demonstrate that our method outperforms state-of-the-art approaches. The code is available at https://github.com/CityU-AIM-Group/TEAR.\n"}, {"title": "Semi-Supervised Medical Image Segmentation Using Cross-Model Pseudo-Supervision with Shape Awareness and Local Context Constraints", "abstract": "In semi-supervised medical image segmentation, the limited amount of labeled data available for training is often insufficient to learn the variability and complexity of target regions. To overcome these challenges, we propose a novel framework based on cross-model pseudo-supervision that generates anatomically plausible predictions using shape awareness and local context constraints. Our framework consists of two parallel networks, a shape-aware network and a shape-agnostic network, which provide pseudo-labels to each other for using unlabeled data effectively. The shape-aware network implicitly captures information on the shape of target regions by adding the prediction of the other network as input. On the other hand, the shape-agnostic network leverages Monte-Carlo dropout uncertainty estimation to generate reliable pseudo-labels to the other network. The proposed framework also comprises a new loss function that enables the network to learn the local context of the segmentation, thus improving the overall segmentation accuracy. Experiments on two publicly-available datasets show that our method outperforms state-of-the-art approaches for semi-supervised segmentation and better preserves anatomical morphology compared to these approaches.\n"}, {"title": "Semi-Supervised PR Virtual Staining for Breast Histopathological Images", "abstract": "Progesterone receptor (PR) plays a vital role in diagnosing and treating breast cancer, but PR staining is costly and time-consuming, seriously hindering its application in clinical practice. The recent rapid development of deep learning technology provides an opportunity to address this problem by virtual staining. However, supervised methods acquire pixel-level paired H&E and PR images, which almost cannot be implemented clinically. In addition, unsupervised methods lack effective constraint information, and the staining results are not reliable sometimes. In this paper, we propose a semi-supervised PR virtual staining method without any pathologist annotation. Firstly, we register the consecutive slides and obtain the patch-level labels of H&E images from the registered consecutive PR images. Furthermore, by designing a Pos/Neg classifier and corresponding constraints, the output images maintain the Pos/Neg consistency with the input images, enabling the output images to be more accurate. Experimental results show that our method can effectively generate PR images from H&E images and maintain structural and pathological consistency with the reference. Compared with existing methods, our approach achieves the best performance.\n"}, {"title": "Semi-Supervised Spatial Temporal Attention Network for Video Polyp Segmentation", "abstract": "Deep learning-based polyp segmentation approaches have achieved great success in image datasets. However, the frame-by-frame annotation of polyp videos requires a large amount of workload, which limits the application of polyp segmentation algorithms in clinical videos. In this paper, we address the semi-supervised video polyp segmentation task, which requires only sparsely annotated frames to train a video polyp segmentation network. We propose a novel spatial-temporal attention network which is composed of the Temporal Local Context Attention (TLCA) module and Proximity Frame Time-Space Attention (PFTSA) module. Specifically, the TLCA module is to refine the prediction of the current frame using the prediction results of the nearby frames in the video clip. PFTSA module utilizes a simple yet powerful hybrid transformer architecture to capture long-range dependencies in time and space efficiently. Combined with consistency constraints, the network fuses representations of proximity frames at different scales to generate pseudo-masks for unlabeled images. We further propose a pseudo-mask-based training method. Additionally, we re-masked a subset of LDPolypVideo and applied it as a semi-supervised polyp segmentation dataset for our experiments. Experimental results show that our proposed semi-supervised approach can outperform existing image-level semi-supervised and fully supervised methods with sparse annotation. Source code will be made available.\n"}, {"title": "Sensor Geometry Generalization to Untrained Conditions in Quantitative Ultrasound Imaging", "abstract": "Recent improvements in deep learning have brought great progress in ultrasonic lesion quantification. However, the learning-based scheme performs properly only when a certain level of similarity between train and test condition is ensured. However, real-world test condition expects diverse untrained probe geometry from various manufacturers, which undermines the credibility of learning-based ultrasonic approaches. In this paper, we present a meta-learned deformable sensor generalization network that generates consistent attenuation coefficient (AC) image regardless of the probe condition. The proposed method was assessed through numerical simulation and in-vivo breast patient measurements. The numerical simulation shows that the proposed network outperforms existing state-of-the-art domain generalization methods for the AC reconstruction under unseen probe conditions. In in-vivo studies, the proposed network provides consistent AC images irrespective of various probe conditions and demonstrates great clinical potential in differential breast cancer diagnosis.\n"}, {"title": "SETMIL: Spatial Encoding Transformer-based Multiple Instance Learning for Pathological Image Analysis", "abstract": "Considering the huge size of the gigapixel whole slide image (WSI), multiple instance learning (MIL) is normally employed to address pathological image analysis tasks, where learning an informative and effective representation of each WSI plays a central role but remains challenging due to the weakly supervised nature of MIL. To this end, we present a novel Spatial Encoding Transformer-based MIL method, SETMIL, which has the following advantages. (1) It is a typical embedded-space MIL method and therefore has the advantage of generating the bag embedding by comprehensively encoding all instances with a fully trainable transformer-based aggregating module. (2) SETMIL leverages spatial-encoding-transformer layers to update the representation of an instance by aggregating both neighbouring instances and globally-correlated instances simultaneously. (3) The joint absolute-relative position encoding design in the aggregating module further improves the context-information-encoding ability of SETMIL. (4) SETMIL designs a transformer-based pyramid multi-scale fusion module to comprehensively encode the information with different granularity using multi-scale receptive fields and make the obtained representation enriched with multi-scale context information. Extensive experiments demonstrated the superior performance of SETMIL in challenging pathological image analysis tasks such as gene mutation and lymph node metastasis prediction. \n"}, {"title": "SGT: Scene Graph-Guided Transformer for Surgical Report Generation", "abstract": "The robotic surgical report reflects the operations during surgery and relates to the subsequent treatment. Therefore, it is especially important to generate accurate surgical reports. Given that there are numerous interactions between instruments and tissue in the surgical scene, we propose a Scene Graph-guided Transformer (SGT) to solve the issue of surgical report generation. The model is based on the structure of transformer to understand the complex interactions between tissue and the instruments from both global and local perspectives. On the one hand, we propose a relation driven attention to facilitate the comprehensive description of the interaction in a generated report via sampling of numerous interactive relationships to form a diverse and representative augmented memory. On the other hand, to characterize the specific interactions in each surgical image, a simple yet ingenious approach is proposed for homogenizing the input heterogeneous scene graph, which plays an effective role in modeling the local interactions by injecting the graph-induced attention into the encoder. The dataset from clinical nephrectomy is utilized for performance evaluation and the experimental results show that our SGT model can significantly improve the quality of the generated surgical medical report, far exceeding the other state-of-the-art methods. The code is public available at: https://github.com/ccccchenllll/SGT_master."}, {"title": "Shape-Aware Weakly/Semi-Supervised Optic Disc and Cup Segmentation with Regional/Marginal Consistency", "abstract": "Glaucoma is a chronic eye disease that permanently impairs vision. Vertical cup to disc ratio (vCDR) is essential for glaucoma screening. Thus, accurately segmenting the optic disc (OD) and optic cup (OC) from colour fundus images is essential. Previous fully-supervised methods achieved accurate segmentation results; then, they calculated the vCDR with offline post-processing step. However, a large set of labeled segmentation images are required for the training, which is costly and time-consuming. To solve this, we propose a weakly/semi-supervised framework with the benefits of geometric associations and specific domain knowledge between pixel-wise segmentation probability map (PM), geometry-aware modified signed distance function representations (mSDF), and local boundary region of interest characteristics (B-ROI). Firstly, we propose a dual consistency regularisation-based semi-supervised paradigm, where the regional and marginal consistency benefits the proposed model from the objects\u00e2\u0080\u0099 inherent region and boundary coherence of a large amount of unlabeled data. Secondly, for the first time, we exploit the domain-specific knowledge between the boundary and region in terms of the perimeter and area of an oval shape of OD & OC, where a differentiable vCDR estimating module is proposed for the end-to-end training. Thus, our model does not need any offline post-process to generate vCDR. Furthermore, without requiring any additional laborious annotations, the supervision on vCDR can serve as a weakly-supervision for OD & OC region and boundary segmentation. Experiments on six large-scale datasets demonstrate that our method outperforms state-of-the-art semi-supervised approaches for segmentation of the optic disc and optic cup, and estimation of vCDR for glaucoma assessment in colour fundus images, respectively. The implementation code is made available.\n"}, {"title": "Shape-based features of white matter fiber-tracts associated with outcome in Major Depression Disorder", "abstract": "Major depression is a leading cause of disability due to its trend to recurrence and treatment resistance. Currently, there are no biomarkers which could potentially identify patients with risk of treatment resistance."}, {"title": "ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation", "abstract": "Cardiac  segmentation is an essential step for the diagnosis of cardiovascular diseases. However, pixel-wise dense labeling is both costly and time-consuming. Scribble, as a form of sparse annotation, is more accessible than full annotations. However, it\u00e2\u0080\u0099s particularly challenging to train a segmentation network with weak supervision from scribbles. To tackle this problem, we propose a new scribble-guided method for cardiac segmentation, based on the Positive-Unlabeled (PU) learning framework and shape consistency regularization, and termed as ShapePU. To leverage unlabeled pixels via PU learning, we first present an Expectation-Maximization (EM) algorithm to estimate the proportion of each class in the unlabeled pixels. Given the estimated ratios, we then introduce the marginal probability maximization to identify the classes of unlabeled pixels. To exploit shape knowledge, we apply cutout operations to training images, and penalize the inconsistent segmentation results. Evaluated on two open datasets, i.e, ACDC and MSCMRseg, our scribble-supervised ShapePU surpassed the fully supervised approach respectively by 1.4% and 9.8% in average Dice, and outperformed the state-of-the-art weakly supervised and PU learning methods by large margins.\n"}, {"title": "Show, Attend and Detect: Towards Fine-grained Assessment of Abdominal Aortic Calcification on Vertebral Fracture Assessment Scans", "abstract": "More than 55,000 people  world-wide die from Cardiovascular Disease (CVD) each day. Calcification of the abdominal aorta is an established marker of asymptomatic CVD. It can be observed on scans taken for vertebral fracture assessment from Dual Energy X-ray Absorptiometry machines. Assessment of Abdominal Aortic Calcification (AAC) and timely intervention may help to reinforce public health messages around CVD risk factors and improve disease management, reducing the global health burden related to CVDs. Our research addresses this problem by proposing a novel and reliable framework for automated \u00e2\u0080\u009cfine-grained\u00e2\u0080\u009d assessment of AAC. Inspired by the vision-to-language models, our method performs sequential scoring of calcified lesions along the length of the abdominal aorta on DXA scans; mimicking the human scoring process.\n"}, {"title": "Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans", "abstract": "In the management of lung nodules, we are desirable to predict nodule evolution in terms of its diameter variation on Computed Tomography (CT) scans and then provide a follow-up recommendation according to the predicted result of the growing trend of the nodule. In order to improve the performance of growth trend prediction for lung nodules, it is vital to compare the changes of the same nodule in consecutive CT scans. Motivated by this, we screened out 4,666 subjects with more than two consecutive CT scans from the National Lung Screening Trial (NLST) dataset to organize a temporal dataset called NLSTt. In specific, we first detect and pair regions of interest (ROIs) covering the same nodule based on registered CT scans. After that, we predict the texture category and diameter size of the nodules through models. Last, we annotate the evolution class of each nodule according to its changes in diameter. Based on the built NLSTt dataset, we propose a siamese encoder to simultaneously exploit the discriminative features of 3D ROIs detected from consecutive CT scans. Then we novelly design a spatial-temporal mixer (STM) to leverage the interval changes of the same nodule in sequential 3D ROIs and capture spatial dependencies of nodule regions and the current 3D ROI. According to the clinical diagnosis routine, we employ hierarchical loss to pay more attention to growing nodules. The extensive experiments on our organized dataset demonstrate the advantage of our proposed method. We also conduct experiments on an in-house dataset to evaluate the clinical utility of our method by comparing it against skilled clinicians.\n"}, {"title": "Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency", "abstract": "Segmenting both bone surface and the corresponding acoustic shadow are fundamental tasks in ultrasound (US) guided orthopedic procedures. However, these tasks are challenging due to minimal and blurred bone surface response in US images, cross-machine discrepancy, imaging artifacts, and low signal-to-noise ratio. Notably, bone shadows are caused by a significant acoustic impedance mismatch between the soft tissue and bone surfaces. To leverage this mutual information between these highly related tasks, we propose a single end-to-end network with a shared transformer-based encoder and task independent decoders for simultaneous bone and shadow segmentation. To share complementary features, we propose a cross task feature transfer block which learns to transfer meaningful features from decoder of shadow segmentation to that of bone segmentation and vice-versa. We also introduce a correspondence consistency loss which makes sure that network utilizes the inter-dependency between the bone surface and its corresponding shadow to refine the segmentation. Validation against expert annotations shows that the method outperforms the previous state-of-the-art for both bone surface and shadow segmentation.\n"}, {"title": "Skin Lesion Recognition with Class-Hierarchy Regularized Hyperbolic Embeddings", "abstract": "In practice, many medical datasets have an underlying taxonomy defined over the disease label space. However, existing classification algorithms for medical diagnoses often assume semantically independent labels. In this study, we aim to leverage class hierarchy with deep learning algorithms for more accurate and reliable skin lesion recognition. We propose a hyperbolic network to jointly learn image embeddings and class prototypes. The hyperbola provably provides a space for modeling hierarchical relations better than Euclidean geometry. Meanwhile, we restrict the distribution of hyperbolic prototypes with a distance matrix that is encoded from the class hierarchy. Accordingly, the learned prototypes preserve the semantic class relations in the embedding space and we can predict the label of an image by assigning its feature to the nearest hyperbolic class prototype. We use an in-house skin lesion dataset which consists of around 230k dermoscopic images on 65 skin diseases to verify our method. Extensive experiments provide evidence that our model can achieve higher accuracy with less severe classification errors compared to that models without considering class relations. \n"}, {"title": "SLAM-TKA: Real-time Intra-operative Measurement of Tibial Resection Plane in Conventional Total Knee Arthroplasty", "abstract": "Total knee arthroplasty (TKA) is a common orthopaedic surgery to replace a damaged knee joint with artificial implants. The inaccuracy of achieving the planned implant position can result in the risk of implant component aseptic loosening, wear out, and even a joint revision, and those failures most of the time occur on the tibial side in the conventional jig-based TKA (CON-TKA). This study aims to precisely evaluate the accuracy of the proximal tibial resection plane intra-operatively in real-time such that the evaluation processing changes very little on the CON-TKA operative procedure. Two X-ray radiographs captured during the proximal tibial resection phase together with a preoperative patient-specific tibia 3D mesh model segmented from computed tomography (CT) scans and a trocar pin 3D mesh model are used in the proposed simultaneous localisation and mapping (SLAM) system to estimate the proximal tibial resection plane. Validations using both simulation and in-vivo datasets are performed to demonstrate the robustness and the potential clinical value of the proposed algorithm.\n"}, {"title": "SMESwin Unet: Merging CNN and Transformer for Medical Image Segmentation", "abstract": "Vision transformer is the new favorite paradigm in medical image segmentation since last year, which surpassed the traditional CNN counterparts in quantitative metrics. The significant advantage of ViTs is to utilize the attention layers to model global relations between tokens. However, the increased representation capacity of ViTs comes with corresponding shortcomings: short of CNN\u00e2\u0080\u0099s inductive biases (locality), translation invariance, and hierarchical structure of visual information. Consequently, well-trained ViTs require more data than CNNs. As high quality data in medical imaging area is always limited, we propose SMESwin UNet. Firstly, based on Channel-wise Cross fusion Transformer (CCT) we fuse multi-scale semantic features and attention maps by designing a compound structure with CNN and ViTs (named MCCT). Secondly, we introduce superpixel by dividing the pixel-level feature into district-level to avoid the interference of meaningless parts of the image. Finally, we used External Attention to consider the correlations among all data samples, which may further reduce the limitation of small datasets. According to our experiments, the proposed superpixel and MCCT-based Swin Unet (SMESwin Unet) achieves better performance than CNNs and other Transformer-based architectures on three medical image segmentation datasets (nucleus, cells, and glands). \n"}, {"title": "Sparse Interpretation of Graph Convolutional Networks for Multi-Modal Diagnosis of Alzheimer\u00e2\u0080\u0099s Disease", "abstract": "The interconnected quality of brain regions in neurological disease has immense importance for the development of biomarkers and diagnostics.  While Graph Convolutional Network (GCN) methods are fundamentally compatible with discovering the connected role of brain regions in disease, current methods apply limited consideration for node features and their connectivity in brain network analysis.  In this paper, we propose a sparse interpretable GCN framework (SGCN) for the identification and classification of Alzheimer\u00e2\u0080\u0099s disease (AD) using brain imaging data with multiple modalities. SGCN applies an attention mechanism with sparsity to identify the most discriminative subgraph structure and important node features for the detection of AD.  The model learns the sparse importance probabilities for each node feature and edge with entropy, L1, and mutual information regularization. We then utilized this information to find signature regions of interest (ROIs), and emphasize the disease-specific brain network connections by detecting the significant difference of connectives between regions in healthy control (HC), and AD groups. We evaluated SGCN on the ADNI database with imaging data from three modalities, including VBM-MRI, FDG-PET, and AV45-PET, and observed that the important probabilities it learned are effective for disease status identification and the sparse interpretability of disease-specific ROI features and connections. The salient ROIs detected and the most discriminative network connections interpreted by our method show a high correspondence with previous neuroimaging evidence associated with AD.\n"}, {"title": "Spatial-hierarchical Graph Neural Network with Dynamic Structure Learning for Histological Image Classification", "abstract": "Graph neural network (GNN) has achieved tremendous success in histological image classification, as it can explicitly model the notion and interaction of different biological entities (e.g., cell, tissue and etc.). However, the potential of GNN has not been fully unleashed for histological image analysis due to (1) the fixed design mode of graph structure and (2) the insufficient interactions between multi-level entities. In this paper, we proposed a novel spatial-hierarchical GNN framework (SHGNN) equipped with a dynamic structure learning (DSL) module for effective histological image classification. Compared with traditional GNNs, the proposed framework has two compelling characteristics. First, the DSL module integrates the positional attribute and semantic representation of entities to learn the adjacency relationship of them during the training process. Second, the proposed SHGNN can extract rich and discriminative features by mining the spatial features of different entities via graph convolutions and aggregating the semantic of multi-level entities via a vision transformer (ViT) based interaction mechanism. We evaluate the proposed framework on our collected colorectal cancer staging (CRCS) dataset and the public breast carcinoma subtyping (BRACS) dataset. Experimental results demonstrate that our proposed method yield superior classification results compared to state-of-the-arts.\n"}, {"title": "Spatiotemporal Attention for Early Prediction of Hepatocellular Carcinoma based on Longitudinal Ultrasound Images", "abstract": "Early screening is an important way to reduce the mortality of hepatocellular carcinoma (HCC) and improve its prognosis. As a noninvasive, economic, and safe procedure, B-mode ultrasound is currently the most common imaging modality for diagnosing and monitoring HCC. However, because of the difficulty of extracting effective image features and modeling longitudinal data, few studies have focused on early prediction of HCC based on longitudinal ultrasound images. In this paper, to address the above challenges, we propose a spatiotemporal attention network (STA-HCC) that adopts a convolutional-neural-network\u00e2\u0080\u0093transformer framework. The convolutional neural network includes a feature-extraction backbone and a proposed regions-of-interest attention block, which learns to localize regions of interest automatically and extract effective features for HCC prediction. The transformer can capture long-range dependencies and nonlinear dynamics from ultrasound images through a multihead self-attention mechanism. Also, an age-based position embedding is proposed in the transformer to embed a more-appropriate positional relationship among the longitudinal ultrasound images. Experiments conducted on our dataset of 6170 samples collected from 619 cirrhotic subjects show that STA-HCC achieves impressive performance, with an area under the receiver-operating-characteristic curve of 77.5%, an accuracy of 70.5%, a sensitivity of 69.9%, and a specificity of 70.5%. The results show that our method achieves state-of-the-art performance compared with other popular sequence models.\n"}, {"title": "Spatio-temporal motion correction and iterative reconstruction of in-utero fetal fMRI", "abstract": "Resting-state functional Magnetic Resonance Imaging (fMRI) is a powerful imaging technique for studying functional development of the brain in utero. However, unpredictable and excessive movement of fetuses have limited its clinical applicability. Previous studies have focused primarily on the accurate estimation of the motion parameters employing a single step 3D interpolation at each individual time frame to recover a motion-free 4D fMRI image. Using only information from a 3D spatial neighborhood neglects the temporal structure of fMRI and useful information from neighboring timepoints. Here, we propose a novel technique based on four dimensional iterative reconstruction of the motion scattered fMRI slices. Quantitative evaluation of the proposed method on a cohort of real clinical fetal fMRI data indicates improvement of reconstruction quality compared to the conventional 3D interpolation approaches.\n"}, {"title": "Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising", "abstract": "Fluoroscopy is an imaging technique that uses X-ray to obtain a real-time 2D video of the interior of a 3D object, helping surgeons to observe pathological structures and tissue functions especially during intervention. However, it suffers from heavy noise that mainly arises from the clinical use of a low dose X-ray, thereby necessitating the technology of fluoroscopy denoising. Such denoising is challenged by the relative motion between the object being imaged and the X-ray imaging system. We tackle this challenge by proposing a self-supervised, three-stage framework that exploits the domain knowledge of fluoroscopy imaging. (i) Stabilize: we first construct a dynamic panorama based on optical flow calculation to stabilize the non-stationary background induced by the motion of the X-ray detector. (ii) Decompose: we then propose a novel mask-based Robust Principle Component Analysis (RPCA) decomposition method to separate a video with detector motion into a low-rank background and a sparse foreground. Such a decomposition accommodates the reading habit of experts. (iii) Denoise: we finally denoise the background and foreground separately by a self-supervised learning strategy and fuse the denoised parts into the final output via a bilateral, spatiotemporal filter. To assess the effectiveness of our work, we curate a dedicated fluoroscopy dataset of 27 videos (1,568 frames) and corresponding ground truth. Our experiments demonstrate that it achieves significant improvements in terms of denoising and enhancement effects when compared with standard approaches. Finally, expert rating confirms this efficacy.\n"}, {"title": "Stay focused - Enhancing model interpretability through guided feature training", "abstract": "In computer-assisted surgery, artificial intelligence (AI) methods need to be interpretable, as a clinician has to understand a model\u00e2\u0080\u0099s decision. To improve the visual interpretability of convolutional neural network, we propose to indirectly guide the feature development process of the model with augmented training data in which unimportant regions in an image have been blurred. On a public dataset, we show that our proposed training workflow results in better visual interpretability of the model and improves the overall model performance. To numerically evaluate heat maps, produced by explainable AI methods, we propose a new metric evaluating the focus with regards to a mask of the region of interest. Further, we are able to show that the resulting model is more robust against changes in the background by focusing the features onto the important areas of the scene and therefore improve model generalization."}, {"title": "Stepwise Feature Fusion: Local Guides Global", "abstract": "Colonoscopy, currently the most efficient and recognized colon polyp detection technology, is necessary for early screening and prevention of colorectal cancer. However, due to the varying size and complex morphological features of colonic polyps as well as the indistinct boundary between polyps and mucosa, accurate segmentation of polyps is still challenging. Deep learning has become popular for accurate polyp segmentation tasks with excellent results. However, due to the structure of polyps image and the varying shapes of polyps, it easy for existing deep learning models to overfitting the current dataset. As a result, the model may not process unseen colonoscopy data. To address this, we propose a new State-Of-The-Art model for medical image segmentation, the SSFormer, which uses a pyramid Transformer encoder to improve the generalization ability of models. Specifically, our proposed Progressive Locality Decoder can be adapted to the pyramid Transformer backbone to emphasize local features and restrict attention dispersion. The SSFormer achieves statet-of-the-art performance in both learning and generalization assessment. \n"}, {"title": "Stereo Depth Estimation via Self-Supervised Contrastive Representation Learning", "abstract": "Accurate stereo depth estimation is crucial for 3D reconstruction in surgery. Self-supervised approaches are more preferable than supervised approaches when limited data is available for training but they can not learn clear discrete data representations.\nIn this work, we propose a two-phase training procedure which entails: (1) Performing Contrastive Representation Learning (CRL) of left and right views to learn discrete stereo features (2) Utilising the trained CRL model to learn disparity via self-supervised training based on the photometric loss. For efficient and scalable CRL training on stereo images we introduce a momentum pseudo-supervised contrastive loss.\nQualitative and quantitative performance evaluation on minimally invasive surgery and autonomous driving data shows that our approach achieves higher image reconstruction score and lower depth error when compared to state-of-the-art self-supervised models. This verifies that contrastive learning is effective in optimising stereo-depth estimation with self-supervised models.\n"}, {"title": "Stroke lesion segmentation from low-quality and few-shot MRIs via similarity-weighted self-ensembling framework", "abstract": "Ischemic stroke lesion is one of the prevailing diseases with the highest mortality in low- and middle-income countries. Although deep learning-based segmentation methods have the great potential to improve the medical resource imbalance and reduce stroke risk in these countries, existing segmentation studies are difficult to be deployed in these low-resource settings because they have such high requirements for the data amount (plenty-shot) and quality (high-field and high resolution) that are usually unavailable in these countries. In this paper, we propose a SimIlarity-weiGhed self-eNsembling framework (SIGN) to segment stroke lesions from low-quality and few-shot MRI data by leveraging publicly available glioma data. To overcome the low-quality challenge, a novel Identify-to-Discern Network employs attention mechanisms to identify lesions from a global perspective and progressively refine the coarse prediction via focusing on the ambiguous regions. To overcome the few-shot challenge, a new Soft Distribution-aware Updating strategy trains the Identify-to-Discern Network in the direction beneficial to tumor segmentation via respective optimizing schemes and adaptive similarity evaluation on glioma and stroke data. The experiment indicates our method outperforms existing few-shot methods and achieves the Dice of 76.84% after training with 14-case low-quality stroke lesion data, illustrating the effectiveness of our method and the potential to be deployed in low resource settings. Code is available in: https://github.com/MINDLAB1/SIGN.\n"}, {"title": "Structure-consistent Restoration Network for Cataract Fundus Image Enhancement", "abstract": "Fundus photography is a routine examination in clinics to diagnose and monitor ocular diseases. However, for cataract patients, the fundus image always suffers quality degradation caused by the clouding lens. The degradation prevents reliable diagnosis by ophthalmologists or computer-aided systems. To improve the certainty in clinical diagnosis, restoration algorithms have been proposed to enhance the quality of fundus images.  Unfortunately, challenges remain in the deployment of these algorithms, such as collecting sufficient training data and preserving retinal structures. In this paper, to circumvent the strict deployment requirement, a structure-consistent restoration network (SCR-Net) for cataract fundus images is developed from synthesized data that shares an identical structure. A synthesized cataract set (SCS) is first simulated to collect cataract fundus images sharing identical structures. Then high-frequency components (HFCs) are extracted from the SCS to constrain structure consistency such that the structure preservation in SCR-Net is enforced. The experiments demonstrate the effectiveness of SCR-Net in the comparison with state-of-the-art methods and the follow-up clinical applications.\n"}, {"title": "Super-Focus: Domain Adaptation for Embryo Imaging via Self-Supervised Focal Plane Regression", "abstract": "In recent years, the field of embryo imaging has seen an influx of work using machine learning. These works take advantage of large microscopy datasets collected by fertility clinics as routine practice through relatively standardised imaging setups. Nevertheless, systematic variations still exist between datasets and can harm the ability of machine learning models to perform well across different clinics. In this work, we present Super-Focus, a method for correcting systematic variations present in embryo focal stacks by artificially generating focal planes. We demonstrate that these artificially generated planes are realistic to human experts and that using Super-Focus as a pre-processing step improves the ability of a cell instance segmentation model to generalise across multiple clinics.\n"}, {"title": "SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency", "abstract": "Intra-voxel incoherent motion (IVIM) analysis of fetal lungs Diffusion-Weighted MRI (DWI) data shows potential in providing quantitative imaging bio-markers that reflect, indirectly, diffusion and pseudo-diffusion for non-invasive fetal lung maturation assessment. However, long acquisition times, due to the large number of different ``b-value\u00e2\u0080\u0099\u00e2\u0080\u0099 images required for IVIM analysis, precluded clinical feasibility. \nWe introduce SUPER-IVIM-DC a deep-neural-networks (DNN) approach which couples supervised loss with a data-consistency term to enable IVIM analysis of DWI data acquired with a limited number of b-values.\nWe demonstrated the added-value of SUPER-IVIM-DC over both classical and recent DNN approaches for IVIM analysis through numerical simulations, healthy volunteer study, and IVIM analysis of fetal lung maturation from fetal DWI data. \nOur numerical simulations and healthy volunteer study show that SUPER-IVIM-DC estimates of the IVIM model parameters from limited DWI data had lower normalized root mean-squared error compared to previous DNN-based approaches. Further, SUPER-IVIM-DC estimates of the pseudo-diffusion fraction parameter from limited DWI data of fetal lungs correlate better with gestational age compared to both to classical and DNN-based approaches (0.555 vs. 0.463 and 0.310). \nSUPER-IVIM-DC has the potential to reduce the long acquisition times associated with IVIM analysis of DWI data and to provide clinically feasible bio-markers for non-invasive fetal lung maturity assessment."}, {"title": "Supervised Contrastive Learning to Classify Paranasal Anomalies in the Maxillary Sinus", "abstract": "Using deep learning techniques, anomalies in the paranasal sinus system can be detected automatically in MRI images and can be further analyzed and classified based on their volume, shape and other parameters like local contrast. However, due to limited training data, traditional supervised learning methods often fail to generalize. Existing deep learning methods in paranasal anomaly classification have been used to diagnose at most one anomaly. In our work, we consider three anomalies. Specifically, we employ a 3D CNN to separate maxillary sinus volumes without anomaly from maxillary sinus volumes with anomaly. To learn robust representations from a small labelled dataset, we propose a novel learning paradigm that combines contrastive loss and cross-entropy loss. Particularly, we use a supervised contrastive loss that encourages embeddings of maxillary sinus volumes with and without anomaly to form two distinct clusters while the cross-entropy loss encourages the 3D CNN to maintain its discriminative ability. We report that optimising with both losses is advantageous over optimising with only one loss. We also find that our training strategy leads to label efficiency. With our method, a 3D CNN classifier achieves an AUROC of 0.85\u00c2\u00b10.03 while a 3D CNN classifier optimised with cross entropy loss achieves an AUROC of 0.66\u00c2\u00b10.1. Our source code is available at https://github.com/dawnofthedebayan/SupConCE_MICCAI_22.\n"}, {"title": "Supervised Deep Learning for Head Motion Correction in PET", "abstract": "Head movement is a major limitation in brain positron emission tomography (PET) imaging, which results in image artifacts and quantification errors. Head motion correction plays a critical role in quantitative image analysis and diagnosis of nervous system diseases. However, to date, there is no approach that can track head motion continuously without using an external device. Here, we develop a deep learning-based algorithm to predict rigid motion for brain PET by leveraging existing dynamic PET scans with gold-standard motion measurements from external Polaris Vicra tracking. We propose a novel Deep Learning for Head Motion Correction (DL-HMC) methodology that consists of three components: (i) PET input data encoder layers; (ii) regression layers to estimate the six rigid motion transformation parameters; and (iii) feature-wise transformation (FWT) layers to condition the network to tracer time-activity. The input of DL-HMC is sampled pairs of one-second 3D cloud representations of the PET data and the output is the prediction of six rigid transformation motion parameters. We trained this network in a supervised manner using the Vicra motion tracking information as gold-standard. We quantitatively evaluate DL-HMC by comparing to gold-standard Vicra measurements and qualitatively evaluate the reconstructed images as well as perform region of interest standard uptake value (SUV) measurements. An algorithm ablation study was performed to determine the contributions of each of our DL-HMC design choices to network performance. Our results demonstrate accurate motion prediction performance for brain PET using a data-driven registration approach without external motion tracking hardware. All code is publicly available on GitHub: https://github.com/OnofreyLab/dl-hmc_miccai2022."}, {"title": "Suppressing Poisoning Attacks on Federated Learning for Medical Imaging", "abstract": "Collaboration among multiple data-owning entities (e.g., hospitals) can accelerate the training process and yield better machine learning models due to the availability and diversity of data. However, privacy concerns make it challenging to exchange data while preserving confidentiality. Federated Learning (FL) is a promising solution that enables collaborative training through exchange of model parameters instead of raw data. However, most existing FL solutions work under the assumption that participating clients are honest and thus can fail against poisoning attacks from malicious parties, whose goal is to deteriorate the global model performance. In this work, we propose a robust aggregation rule called Distance-based Outlier Suppression (DOS) that is resilient to byzantine failures. The proposed method computes the distance between local parameter updates of different clients and obtains an outlier score for each client using Copula-based Outlier Detection (COPOD). The resulting outlier scores are converted into normalized weights using a softmax function, and a weighted average of the local parameters is used for updating the global model. DOS aggregation can effectively suppress parameter updates from malicious clients without the need for any hyperparameter selection, even when the data distributions are heterogeneous. Evaluation on two medical imaging datasets (CheXpert and HAM10000) demonstrates the higher robustness of DOS method against a variety of poisoning attacks in comparison to other state-of-the-art methods.\n"}, {"title": "Surgical Scene Segmentation Using Semantic Image Synthesis with a Virtual Surgery Environment", "abstract": "The previous image synthesis research for surgical vision\nhad limited results for real-world applications with simple simulators,\nincluding only a few organs and surgical tools and outdated segmentation\nmodels to evaluate the quality of the image. Furthermore, none of\nthe research released complete datasets to the public enabling the open\nresearch. Therefore, we release a new dataset to encourage further study\nand provide novel methods with extensive experiments for surgical scene\nsegmentation using semantic image synthesis with a more complex virtual\nsurgery environment. First, we created three cross-validation sets of\nreal image data considering demographic and clinical information from\n40 cases of real surgical videos of gastrectomy with the da Vinci Surgical\nSystem (dVSS). Second, we created a virtual surgery environment in the\nUnity engine with five organs from real patient CT data and 22 the da\nVinci surgical instruments from actual measurements. Third, We converted\nthis environment photo-realistically with representative semantic\nimage synthesis models, SEAN and SPADE. Lastly, we evaluated it with\nvarious state-of-the-art instance and semantic segmentation models. We\nsucceeded in highly improving our segmentation models with the help of\nsynthetic training data. More methods, statistics, and visualizations on\nhttps://sisvse.github.io/.\n"}, {"title": "Surgical Skill Assessment via Video Semantic Aggregation", "abstract": "Automated video-based assessment of surgical skills is a promising task in assisting young surgical trainees, especially in poor-resource areas. Existing works often resort to a CNN-LSTM joint framework that models long-term relationships by LSTMs on spatially pooled short-term CNN features. However, this practice would inevitably neglect the difference among semantic concepts such as tools, tissues, and background in the spatial dimension, impeding the subsequent temporal relationship modeling. In this paper, we propose a novel skill assessment framework, Video Semantic Aggregation (ViSA), which discovers different semantic parts and aggregates them across spatiotemporal dimensions. The explicit discovery of semantic parts provides an explanatory visualization that helps understand the neural network\u00e2\u0080\u0099s decisions. It also enables us to further incorporate auxiliary information such as the kinematic data to improve representation learning and performance. The experiments on two datasets show the competitiveness of ViSA compared to state-of-the-art methods. Source code is available at: bit.ly/MICCAI2022ViSA.\n"}, {"title": "Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer", "abstract": "Visual question answering (VQA) in surgery is largely unexplored. Expert surgeons are scarce and are often overloaded with clinical and academic workloads. This overload often limits their time answering questionnaires from patients, medical students or junior residents related to surgical procedures. At times, students and junior residents also refrain from asking too many questions during classes to reduce disruption. While computer-aided simulators and recording of past surgical procedures have been made available for them to observe and improve their skills, they still hugely rely on medical experts to answer their questions. Having a Surgical-VQA system as a reliable \u00e2\u0080\u0098second opinion\u00e2\u0080\u0099 could act as a backup and ease the load on the medical experts in answering these questions. The lack of annotated medical data and the presence of domain-specific terms has limited the exploration of VQA for surgical procedures. In this work, we design a Surgical-VQA task that answers questionnaires on surgical procedures based on the surgical scene. Extending the  MICCAI endoscopic vision challenge 2018 dataset and workflow recognition dataset further, we introduce two Surgical-VQA datasets with classification and sentence-based answers. To perform Surgical-VQA, we employ vision-text transformers models. We further introduce a residual MLP-based VisualBert encoder model that enforces interaction between visual and text tokens, improving performance in classification-based answering. Furthermore, we study the influence of the number of input image patches and temporal visual features on the model performance in both classification and sentence-based answering.\n"}, {"title": "Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomic, and Demographic Data", "abstract": "Integrating cross-department multi-modal data (e.g., radiology, pathology, genomic, and demographic data) is ubiquitous in brain cancer diagnosis and survival prediction. To date, such an integration is typically conducted by human physicians (and panels of experts), which can be subjective and semi-quantitative. Recent advances in multi-modal deep learning, however, have opened a door to leverage such a process in a more objective and quantitative manner. Unfortunately, the prior arts of using four modalities on brain cancer survival prediction are limited by a \u00e2\u0080\u009ccomplete modalities\u00e2\u0080\u009d setting (i.e., with all modalities available). Thus, there are still open questions on how to effectively predict brain cancer survival from incomplete radiology, pathology, genomic, and demographic data (e.g., one or more modalities might not be collected for a patient). For instance, should we use both complete and incomplete data, and more importantly, how do we use such data? To answer the preceding questions, we generalize the multi-modal learning on cross-department multi-modal data to a missing data setting. Our contribution is three-fold: 1) We introduce a multi-modal learning with missing data (MMD) pipeline with competitive performance and less hardware consumption; 2) We extend multi-modal learning on radiology, pathology, genomic, and demographic data into missing data scenarios; 3) A large-scale public dataset (with 962 patients) is collected to systematically evaluate glioma tumor survival prediction using four modalities. The proposed method improved the C-index of survival prediction from 0.7624 to 0.8053.\n"}, {"title": "SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI", "abstract": "Volumetric reconstruction of fetal brains from multiple stacks of MR slices, acquired in the presence of almost unpredictable and often severe subject motion, is a challenging task that is highly sensitive to the initialization of slice-to-volume transformations. We propose a novel slice-to-volume registration method using Transformers trained on synthetically transformed data, which model multiple stacks of MR slices as a sequence. With the attention mechanism, our model automatically detects the relevance between slices and predicts the transformation of one slice using information from other slices. We also estimate the underlying 3D volume to assist slice-to-volume registration and update the volume and transformations alternately to improve accuracy. Results on synthetic data show that our method achieves lower registration error and better reconstruction quality compared with existing state-of-the-art methods. Experiments with real-world MRI data are also performed to demonstrate the ability of the proposed model to improve the quality of 3D reconstruction under severe fetal motion.\n"}, {"title": "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI", "abstract": "Fast MRI aims to reconstruct a high fidelity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.\n"}, {"title": "Swin-VoxelMorph: A Symmetric Unsupervised Learning Model for Deformable Medical Image Registration Using Swin Transformer", "abstract": "Deformable medical image registration is widely used in medical image processing with the invertible and one-to-one mapping between images. While state-of-the-art image registration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on computer vision tasks. Existing models neglect to employ attention mechanisms to handle the long-range cross-image relevance in embedding learning, limiting such approaches to identify the semantically meaningful correspondence of anatomical structures. These methods also ignore the topology preservation and invertibility of the transformation although they achieve fast image registration. In this paper, we propose a novel, symmetric unsupervised learning network Swin-VoxelMorph based on the Swin Transformer which minimizes the dissimilarity between images and estimates both forward and inverse transformations simultaneously. Specifically, we propose 3D Swin-UNet, which applies hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to estimate the registration fields. Besides, our objective loss functions can guarantee substantial diffeomorphic properties of the predicted transformations. We verify our method on two datasets including ADNI and PPMI, and it achieves state-of-the-art registration accuracy while maintaining desirable diffeomorphic properties.\n"}, {"title": "Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator", "abstract": "Understanding the underlying relationship between tongue and oropharyngeal muscle deformation seen in tagged-MRI and intelligible speech plays a vital role in advancing speech motor control theories and treatment of speech related-disorders. Because of their heterogeneous representations, however, direct mapping between the two modalities (two-dimensional plus time tagged-MRI sequence and one-dimensional waveform) is not straightforward. Instead, we resort to two-dimensional spectrograms as an intermediate means, covering both pitch and resonance, from which to develop an end-to-end deep learning framework to translate from a sequence of tagged-MRI to its corresponding audio waveform with limited dataset size.~Our framework hinges on a novel fully convolutional asymmetry translator with guidance of a self residual attention scheme to specifically exploit the moving muscular structures during speech.~In addition, a pairwise correlation of the samples with the same utterances is utilized with a latent space representation disentanglement scheme.~Furthermore, an adversarial training approach with generative adversarial networks is incorporated to provide enhanced realism on our generated spectrograms.~Our experimental results, carried out with a total of 63 tagged-MRI sequences alongside speech acoustics, show that our framework enabled the generation of clear audio waveforms from a sequence of tagged-MRI unseen in training, surpassing competing methods. Thus, our framework provided the potential to aid in better understanding the relationship between the two modalities.\n"}, {"title": "Task-oriented Self-supervised Learning for Anomaly Detection in Electroencephalography", "abstract": "Accurate automated analysis of electroencephalography(EEG) would largely help clinicians effectively monitor and diagnose patients with various brain diseases. Compared to supervised learning with la- belled disease EEG data which can train a model to analyze specific diseases but would fail to monitor previously unseen statuses, anomaly detection based on only normal EEGs can detect any potential anomaly in new EEGs. Different from existing anomaly detection strategies which do not consider any property of unavailable abnormal data during model development, a task-oriented self-supervised learning approach is proposed here which makes use of available normal EEGs and expert knowledge about abnormal EEGs to train a more effective feature extractor for the subsequent development of anomaly detector. In addition, a specific two-branch convolutional neural network with larger kernels is designed as the feature extractor such that it can more easily extract both larger-scale and small-scale features which often appear in unavailable abnormal EEGs. The effectively designed and trained feature extractor has shown to be able to extract better feature representations from EEGs for development of anomaly detector based on normal data and future anomaly detection for new EEGs, as demonstrated on three EEG datasets. The code is available at https://github.com/ironing/EEG-AD.\n"}, {"title": "Task-relevant Feature Replenishment for Cross-centre Polyp Segmentation", "abstract": "Colonoscopy images from different centres usually exhibit appearance variations, making the models trained on one domain unable to generalize well to another. To tackle this issue, we propose a novel Task-relevant Feature Replenishment based Network (TRFR-Net) for cross-centre polyp segmentation via retrieving task-relevant knowledge for sufficient discrimination capability with style variations alleviated. Specifically, we first design a domain-invariant feature decomposition (DIFD) module placed after each encoding block to extract domain-shared information for segmentation. Then we develop a task-relevant feature replenishment (TRFR) module to distill informative context from the residual features of each DIFD module and dynamically aggregate these task-relevant parts, providing extra information for generalized segmentation learning. To further bridge the domain gap leveraging structural similarity, we devise a Polyp-aware Adversarial Learning (PPAL) module to align prediction feature distribution, where more emphasis is imposed on the polyp-related alignment. Experimental results on three public datasets demonstrate the effectiveness of our proposed algorithm. The code is available at: https://github.com/CathyS1996/TRFRNet.\n"}, {"title": "TBraTS: Trusted Brain Tumor Segmentation", "abstract": "Despite recent improvements in the accuracy of brain tumor segmentation, the results still exhibit low levels of confidence and robustness. Uncertainty estimation is one effective way to change this situation, as it provides a measure of confidence in the segmentation results. In this paper, we propose a trusted brain tumor segmentation network which can generate robust segmentation results and reliable uncertainty estimations without excessive computational burden and modification of the backbone network. In our method, uncertainty is modeled explicitly using subjective logic theory, which treats the predictions of backbone neural network as subjective opinions by parameterizing the class probabilities of the segmentation as a Dirichlet distribution. Meanwhile, the trusted segmentation framework learns the function that gathers reliable evidence from the feature leading to the final segmentation results. Overall, our unified trusted segmentation framework endows the model with reliability and robustness to out-of-distribution samples. To evaluate the effectiveness of our model in robustness and reliability, qualitative and quantitative experiments are conducted on the BraTS 2019 dataset.\n"}, {"title": "Test Time Transform Prediction for Open Set Histopathological Image Recognition", "abstract": "Tissue typology annotation in Whole Slide histological images is a complex and tedious, yet necessary task for the development of computational pathology models. We propose to address this problem by applying Open Set Recognition techniques to the task of jointly classifying tissue that belongs to a set of annotated classes, e.g. clinically relevant tissue categories, while rejecting in test time Open Set samples, i.e. images that belong to categories not present in the training set. To this end, we introduce a new approach for Open Set histopathological image recognition based on training a model to accurately identify image categories and simultaneously predict which data augmentation transform has been applied. In test time, we measure model confidence in predicting this transform, which we expect to be lower for images in the Open Set. We carry out comprehensive experiments in the context of colorectal cancer assessment from histological images, which provide evidence on the strengths of our approach to automatically identify samples from unknown categories. Code is released at \\url{https://github.com/\u00e2\u0080\u0094\u00e2\u0080\u0094\u00e2\u0080\u0093/t3po}.\n"}, {"title": "Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift", "abstract": "Class distribution plays an important role in learning deep classifiers. When the proportion of each class in the test set differs from the training set, the performance of classification nets usually degrades. Such a label distribution shift problem is common in medical diagnosis since the prevalence of disease vary over location and time. In this paper, we propose the first method to tackle label shift for medical image classification, which effectively adapt the model learned from a single training label distribution to arbitrary unknown test label distribution. Our approach innovates distribution calibration to learn multiple representative classifiers, which are capable of handling different one-dominating-class distributions. When given a test image, the diverse classifiers are dynamically aggregated via the consistency-driven test-time adaptation, to deal with the unknown test label distribution. We validate our method on two important medical image classification tasks including liver fibrosis staging and COVID-19 severity prediction. Our experiments clearly show the decreased model performance under label shift. With our method, model performance significantly improves on all the test datasets with different label shifts for both medical image diagnosis tasks. Code is available at https://github.com/med-air/TTADC.\n"}, {"title": "Test-Time Adaptation with Shape Moments for Image Segmentation", "abstract": "Supervised learning is well-known to fail at generalization under distribution shifts. In typical clinical settings, the source data is inaccessible and the target distribution is represented with a handful of samples: adaptation can only happen at test time on a few (or even a single) subject(s). We investigate test-time single-subject adaptation for segmentation, and propose a Shape-guided Entropy Minimization objective for tackling this task. During inference for a single testing subject, our loss is minimized with respect to the batch normalization\u00e2\u0080\u0099s scale and bias parameters. We show the potential of integrating various shape priors to guide adaptation to plausible solutions, and validate our method in two challenging scenarios: MRI-to-CT adaptation of cardiac segmentation and cross-site adaptation of prostate segmentation. Our approach exhibits substantially better performances than the existing test-time adaptation methods. Even more surprisingly, it fares better than state-of-the-art domain adaptation methods, although it forgoes training on additional target data during adaptation. Our results question the usefulness of training on target data in segmentation adaptation, and points to the substantial effect of shape priors on test-time inference. Our framework can be readily used for integrating various priors and for adapting any segmentation network, and our code is available anonymously.\n"}, {"title": "Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology", "abstract": "Histopathology whole slide images (WSIs) can reveal significant inter-hospital variability such as illumination, color or optical artifacts. These variations, caused by the use of different protocols across medical centers (staining, scanner), can strongly harm algorithms generalization on unseen protocols. This motivates the development of new methods to limit such loss of generalization. In this paper, to enhance robustness on unseen target protocols, we propose a new test-time data augmentation based on multi domain image-to-image translation. It allows to project images from unseen protocol into each source domain before classifying them and ensembling the predictions. This test-time augmentation method results in a significant boost of performances for domain generalization. To demonstrate its effectiveness, our method has been evaluated on two different histopathology tasks where it outperforms conventional domain generalization, standard H&E specific color augmentation/normalization and standard test-time augmentation techniques. Our code is publicly available at https://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.\n"}, {"title": "TGANet: Text-guided attention for improved polyp segmentation", "abstract": "Colonoscopyisagoldstandardprocedurebutishighlyoperator- dependent. Automated polyp segmentation, a precancerous precursor, can minimize missed rates and timely treatment of colon cancer at an early stage. Even though there are deep learning methods developed for this task, variability in polyp size can impact model training, thereby limiting it to the size attribute of the majority of samples in the training dataset that may provide sub-optimal results to differently sized polyps. In this work, we exploit size-related and polyp number-related features in the form of text attention during training. We introduce an auxiliary clas- sification task to weight the text-based embedding that allows network to learn additional feature representations that can distinctly adapt to differently sized polyps and can adapt to cases with multiple polyps. Our experimental results demonstrate that these added text embeddings im- prove the overall performance of the model compared to state-of-the-art segmentation methods. We explore four different datasets and provide insights for size-specific improvements. Our proposed text-guided atten- tion network (TGANet) can generalize well to variable-sized polyps in different datasets. Codes are available at https://github.com/nikhilroxtomar/TGANet.\n"}, {"title": "The (de)biasing effect of GAN-based augmentation methods on skin lesion images", "abstract": "New medical datasets are now more open to the public, allowing for better and more extensive research. Although prepared with the utmost care, new datasets might still be a source of spurious correlations that affect the learning process. Moreover, data collections are usually not large enough and are often unbalanced. One approach to alleviate the data imbalance is using data augmentation with Generative Adversarial Networks (GANs) to extend the dataset with high-quality images. GANs are usually trained on the same biased datasets as the target data, resulting in more biased instances. This work explored unconditional and conditional GANs to compare their bias inheritance and how the synthetic data influenced the models. We provided extensive manual data annotation of possibly biasing artifacts on the well-known ISIC dataset with skin lesions.\nIn addition, we examined classification models trained on both real and synthetic data with counterfactual bias explanations. Our experiments showed that GANs inherited biases and sometimes even amplified them, leading to even stronger spurious correlations. Manual data annotation and synthetic images are publicly available for reproducible scientific research.\n"}, {"title": "The Dice loss in the context of missing or empty labels: introducing \u00ce\u00a6 and \u00cf\u00b5", "abstract": "Albeit the Dice loss is one of the dominant loss functions in medical image segmentation, most research omits a closer look at its derivative, i.e. the real motor of the optimization when using gradient descent. In this paper, we highlight the peculiar action of the Dice loss in the presence of missing or empty labels. First, we formulate a theoretical basis that gives a general description of the Dice loss and its derivative. It turns out that the choice of the reduction dimensions Phi and the smoothing term epsilon is non-trivial and greatly influences its behavior. We find and propose heuristic combinations of Phi and epsilon that work in a segmentation setting with either missing or empty labels. Second, we empirically validate these findings in a binary and multiclass segmentation setting using two publicly available datasets. We confirm that the choice of Phi and epsilon is indeed pivotal. With Phi chosen such that the reductions happen over a single batch (and class) element and with a negligible epsilon, the Dice loss deals with missing labels naturally and performs similarly compared to recent adaptations specific for missing labels. With Phi chosen such that the reductions happen over multiple batch elements or with a heuristic value for epsilon, the Dice loss handles empty labels correctly. We believe that this work highlights some essential perspectives and hope that it encourages researchers to better describe their exact implementation of the Dice loss in future work.\n"}, {"title": "The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning", "abstract": "The manifold hypothesis is a core mechanism behind the success of deep learning, so understanding the intrinsic manifold structure of image data is central to studying how neural networks learn from the data. Intrinsic dataset manifolds and their relationship to learning difficulty have recently begun to be studied for the common domain of natural images, but little such research has been attempted for radiological images. We address this here. First, we compare the intrinsic manifold dimensionality of radiological and natural images. We also investigate the relationship between intrinsic dimensionality and generalization ability over a wide range of datasets. Our analysis shows that natural image datasets generally have a higher number of intrinsic dimensions than radiological images. However, the relationship between generalization ability and intrinsic dimensionality is much stronger for medical images, which could be explained as radiological images having intrinsic features that are more difficult to learn. These results give a more principled underpinning for the intuition that radiological images can be more challenging to apply deep learning to than natural image datasets common to machine learning research.  We believe rather than directly applying models developed for natural images to the radiological imaging domain, more care should be taken to developing architectures and algorithms that are more tailored to the specific characteristics of this domain. The research shown in our paper, demonstrating these characteristics and the differences from natural images, is an important first step in this direction.\n"}, {"title": "The Semi-constrained Network-Based Statistic (scNBS): integrating local and global information for brain network inference", "abstract": "Functional connectomics has become a popular topic over the last two decades. Researchers often conduct inference at the level of groups of edges, or "}, {"title": "Thoracic Lymph Node Segmentation in CT imaging via Lymph Node Station Stratification and Size Encoding", "abstract": "Visible lymph node (i.e., LN, short axis\u00e2\u0089\u00a55mm) assessment and delineation in thoracic computed tomography  (CT)  images is an indispensable step in radiology and oncology workflows.  The high demanding of clinical expertise and prohibitive laboring cost motivate the automated approaches. Previous works focus on extracting effective LN imaging  features and/or  exploiting  the  anatomical  priors  to  help  LNsegmentation. However, the performance in general is struggled with low recall/precision due to LN\u00e2\u0080\u0099s low contrast in CT and tumor-induced shape and size variations. Given that LNs reside inside the lymph node station (LN-station),  it  is  intuitive  to  directly  utilize  the  LN-station  maps  toguide LN segmentation. We propose a stratified LN-station and LN sizeencoded  segmentation  framework  by  casting  thoracic  LN-stations  into three super lymph node stations and subsequently learning the LN size variations. Four-fold cross-validation experiments on the public NIH 89-patient dataset are conducted. Compared to previous leading works, our framework produces significant performance improvements, with an average 74.2% (9.9% increases) in Dice score and 72.0% (15.6% increases)in detection recall at 4.0 (1.9 reduces) false positives per patient. When directly tested on an external dataset of 57 esophageal cancer patients, the proposed framework demonstrates good generalizability and achieves70.4% in Dice score and 70.2% in detection Recall at 4.4 false positives per patient.\n"}, {"title": "TINC: Temporally Informed Non-Contrastive Learning for Disease Progression Modeling in Retinal OCT Volumes", "abstract": "Recent contrastive learning methods achieved state-of-the-art in low label regimes. However, the training requires large batch sizes and heavy augmentations to create multiple views of an image. With non-contrastive methods, the negatives are implicitly incorporated in the loss, allowing different images and modalities as pairs. Although the meta-information (i.e., age, sex) in medical imaging is abundant, the annotations are noisy and prone to class imbalance. In this work, we exploited already existing temporal information (different visits from a patient) in a longitudinal optical coherence tomography (OCT) dataset using temporally informed non-contrastive loss (TINC) without increasing complexity and need for negative pairs. Moreover, our novel pair-forming scheme can avoid heavy augmentations and implicitly incorporates the temporal information in the pairs. Finally, these representations learned from the pretraining are more successful in predicting disease progression where the temporal information is crucial for the downstream task. More specifically, our model outperforms existing models in predicting the risk of conversion within a time frame from intermediate age-related macular degeneration (AMD) to the late wet-AMD stage.\n"}, {"title": "TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction", "abstract": "When oncologists estimate cancer patient survival, they rely on multimodal data. Even though some multimodal deep learning methods have been proposed in the literature, the majority rely on having two or more independent networks that share knowledge at a later stage in the overall model. On the other hand, oncologists do not do this in their analysis but rather fuse the information in their brain from multiple sources such as medical images and patient history. This work proposes a deep learning method that mimics oncologists\u00e2\u0080\u0099 analytical behavior when quantifying cancer and estimating patient survival. We propose TMSS, an end-to-end Transformer based Multimodal network for Segmentation and Survival predication that leverages the superiority of transformers that lies in their abilities to handle different modalities. The model was trained and validated for segmentation and prognosis tasks on the training dataset from the HEad & NeCK TumOR segmentation and the outcome prediction in PET/CT images challenge (HECKTOR). We show that the proposed prognostic model significantly outperforms state-of-the-art methods with a concordance index of 0.763 \u00c2\u00b1 0.14 while achieving a comparable dice score of 0.772 \u00c2\u00b1 0.030 to a standalone segmentation model. TMSS implementation code will be publicly available upon acceptance.\n"}, {"title": "Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency", "abstract": "The colorectal polyps classification is a critical clinical examination. To improve the classification accuracy, most computer-aided diagnosis algorithms recognize colorectal polyps by adopting Narrow-Band Imaging (NBI). However, the acquisition of this specific image requires manual switching of the light mode when polyps have been detected by using White-Light (WL) images since the NBI usually suffers from missing detection in real clinic scenarios. To avoid the above situation, we propose a novel method to directly achieve accurate white-light colonoscopy image classification by conducting structured cross-modal representation consistency. In practice, a pair of multi-modal images, i.e. NBI and WL, are fed into a shared Transformer to extract hierarchical feature representations. Then a novel designed Spatial Attention Module (SAM) are adopted to calculate the similarities between class token and patch tokens for a specific modality image. By aligning the class tokens and spatial attention maps of paired NBI and WL images at different levels, the Transformer achieves the ability to keep both global and local representation consistency for the above two modalities. Extensive experimental results illustrate the proposed method outperforms the recent studies with a margin, realizing multi-modal prediction with a single Transformer while greatly improving the classification accuracy when only with WL images.\n"}, {"title": "Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound", "abstract": "MOTIVATION: Detection of prostate cancer during transrectal ultrasound-guided biopsy is challenging. The highly heterogeneous appearance of cancer, presence of ultrasound artefacts, and noise all contribute to these difficulties. Recent advancements in high-frequency ultrasound imaging - micro-ultrasound - have drastically increased the capability of tissue imaging at high resolution. Our aim is to investigate the development of a robust deep learning model specifically for micro-ultrasound-guided prostate cancer biopsy. For the model to be clinically adopted, a key challenge is to design a solution that can confidently identify the cancer, while learning from coarse histopathology measurements of biopsy samples that introduce weak labels. METHODS: We use a dataset of micro-ultrasound images acquired from 194 patients, who underwent prostate biopsy. We train a deep model using a co-teaching paradigm to handle noise in labels, together with an evidential deep learning method for uncertainty estimation. We evaluate the performance of our model using the clinically relevant metric of accuracy vs. confidence. RESULTS: Our model achieves a well-calibrated estimation of predictive uncertainty with area under the curve of 88%. The use of co-teaching and evidential deep learning in combination yields significantly better uncertainty estimation than either alone. We also provide a detailed comparison against state-of-the-art in uncertainty estimation.\n"}, {"title": "Towards Holistic Surgical Scene Understanding", "abstract": "Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset\u00e2\u0080\u0099s multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy of our framework to spur future research on holistic surgical scene understanding.\n"}, {"title": "Towards performant and reliable undersampled MR reconstruction via diffusion model sampling", "abstract": "Magnetic Resonance (MR) image reconstruction from under-sampled acquisition promises faster scanning time. To this end, current State-of-The-Art (SoTA) approaches leverage deep neural networks and supervised training to learn a recovery model. While these approaches achieve impressive performances, the learned model can be fragile on unseen degradation, e.g. when given a different acceleration factor. These methods are also generally deterministic and provide a single solution to an ill-posed problem; as such, it can be difficult for practitioners to understand the reliability of the reconstruction. We introduce DiffuseRecon, a novel diffusion model-based MR reconstruction method. DiffuseRecon guides the generation process based on the observed signals and a pre-trained diffusion model, and does not require additional training on specific acceleration factors. DiffuseRecon is stochastic in nature and generates results from a distribution of fully-sampled MR images; as such, it allows us to explicitly visualize different potential reconstruction solutions. Lastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo sampling scheme to approximate the most likely reconstruction candidate. The proposed DiffuseRecon achieves SoTA performances reconstructing from raw acquisition signals in fastMRI and SKM-TEA.\n"}, {"title": "Towards Unsupervised Ultrasound Video Clinical Quality Assessment with Multi-Modality Data", "abstract": "Video quality assurance is an important topic in obstetric ultrasound imaging to ensure that captured videos are suitable for biometry and fetal health assessment. Previously, one successful objective\napproach to automated ultrasound image quality assurance has considered it as a supervised learning task of detecting anatomical structures defined by a clinical protocol. In this paper, we propose an alternative and purely data-driven approach that makes effective use of both spatial and temporal information and the model learns from high-quality videos without any anatomy-specific annotations. This makes it attractive for potentially scalable generalisation. In the proposed model, a 3D encoder and decoder pair bi-directionally learns a spatio-temporal representation between the video space and the feature space. A zoom-in module is introduced to encourage the model to focus on the main object in a frame. A further design novelty is the introduction of two additional modalities in model training (sonographer gaze and optical flow derived from the video). Finally, our approach is applied to identify high-quality videos for fetal head circumference measurement in freehand second-trimester ultrasound scans. Extensive experiments are conducted, and the results demonstrate the effectiveness of our approach with an AUC of 0.911.\n"}, {"title": "Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages", "abstract": "Tracking all nuclei of an embryo in noisy and dense fluorescence microscopy data is a challenging task. We build upon a recent method for nuclei tracking that combines weakly-supervised learning from a small set of nuclei center point annotations with an integer linear program (ILP) for optimal cell lineage extraction. Our work specifically addresses the following challenging properties of C. elegans embryo recordings: (1) Many cell divisions as compared to benchmark recordings of other organisms, and (2) the presence of polar bodies that are easily mistaken as cell nuclei. To cope with (1), we devise and incorporate a learnt cell division detector. To cope with (2), we employ a learnt polar body detector. We further propose automated ILP weights tuning via a structured SVM, alleviating the need for tedious manual set-up of a respective grid search.\nOur method outperforms the previous leader of the cell tracking challenge on the Fluo-N3DH-CE embryo dataset. We report a further extensive quantitative evaluation on two more C. elegans datasets. We will make these datasets public to serve as an extended benchmark for future method development. Our results suggest considerable improvements yielded by our method, especially in terms of the correctness of division event detection and the number and length of fully correct track segments. Code: https://github.com/funkelab/linajea\n"}, {"title": "TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers", "abstract": "Diffusion MRI tractography is an advanced imaging technique for quantitative mapping of the brain\u00e2\u0080\u0099s structural connectivity. Whole brain tractography (WBT) data contains over hundreds of thousands of individual fiber streamlines (estimated brain connections), and this data is usually parcellated to create compact representations for data analysis applications such as disease classification. In this paper, we propose a novel parcellation-free WBT analysis framework, TractoFormer, that leverages tractography information at the level of individual fiber streamlines and provides a natural mechanism for interpretation of results using the attention mechanism of transformers. TractoFormer includes two main contributions. First, we propose a novel and simple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber spatial relationships and any feature of interest that can be computed from individual fibers (such as FA or MD). Second, we design a network based on vision transformers (ViTs) that includes: 1) data augmentation to overcome model overfitting on small datasets, 2) identification of discriminative fibers for interpretation of results, and 3) ensemble learning to leverage fiber information from different brain regions. In a synthetic data experiment, TractoFormer successfully identifies discriminative fibers with simulated group differences. In a disease classification experiment comparing several methods, TractoFormer achieves the highest accuracy in classifying schizophrenia vs control. Discriminative fibers are identified in left hemispheric frontal and parietal superficial white matter regions, which have previously been shown to be affected in schizophrenia patients.\n"}, {"title": "TransEM: Residual Swin-Transformer based regularized PET image reconstruction", "abstract": "Positron emission tomography (PET) image reconstruction is an ill-posed inverse problem and suffers from high level of noise due to limited counts received. Recently deep neural networks especially convolutional neural networks (CNN) have been successfully applied to PET image reconstruction. However, the local characteristics of the convolution operator potentially limit the image quality obtained by current CNN-based PET image reconstruction methods. In this paper, we propose a residual swin-transformer based regularizer (RSTR) to incorporate regularization into the iterative reconstruction framework. Specifically, a convolution layer is firstly adopted to extract shallow features, then the deep feature extraction is accomplished by the swin-transformer layer. At last, both deep and shallow features are fused with a residual operation and another convolution layer. Validations on the realistic 3D brain simulated low-count data show that our proposed method outperforms the state-of-the-art methods in both qualitative and quantitative measures.\n"}, {"title": "Transformer based feature fusion for left ventricle segmentation in 4D flow MRI", "abstract": "Four-dimensional flow magnetic resonance imaging (4D Flow MRI) enables visualization of intra-cardiac blood flow and quantification of cardiac function using time-resolved three directional velocity data. Segmentation of cardiac 4D Flow data is a big challenge due to the extremely poor contrast between the blood pool and myocardium. The magnitude and velocity images from a 4D Flow acquisition provide complementary information, but how to extract and fuse these features efficiently is unknown. Automated cardiac segmentation methods from 4D Flow MRI have not been fully investigated yet. In this paper, we take the velocity and magnitude image as the inputs of two branches separately, then propose a Transformer based cross- and self-fusion layer to explore the inter-relationship from two modalities and model the intra-relationship in the same modality. A large in-house dataset of 104 subjects (91 182 2D images) was used to train and evaluate our model using several metrics including the Dice, Average Surface Distance (ASD), end-diastolic volume (EDV), end-systolic volume (ESV), Left Ventricle Ejection Fraction (LVEF) and Kinetic Energy (KE). Our method achieved a mean Dice of 86.52%, and ASD of 2.51 mm. Evaluation on the clinical parameters demonstrated competitive results, yielding a Pearson corre-lation coefficient of 83.26%, 97.4%, 96.97% and 98.92% for LVEF, EDV, ESV and KE respectively. \n"}, {"title": "Transformer based multiple instance learning for weakly supervised histopathology image segmentation", "abstract": "Hispathological image segmentation algorithms play a critical role in computer aided diagnosis technology. The development of weakly supervised segmentation algorithm alleviates the problem of medical image annotation that it is time-consuming and labor-intensive. As a subset of weakly supervised learning, Multiple Instance Learning (MIL) has been proven to be effective in segmentation. However, there is a lack of related information between instances in MIL, which limits the further improvement of segmentation performance. In this paper, we propose a novel weakly supervised method for pixel-level segmentation in histopathology images, which introduces Transformer into the MIL framework to capture global or long-range dependencies. The multi-head self-attention in the Transformer establishes the relationship between instances, which solves the shortcoming that instances are independent of each other in MIL. In addition, deep supervision is introduced to overcome the limitation of annotations in weakly supervised methods and make the better utilization of hierarchical information. The state-of-the-art results on the colon cancer dataset demonstrate the superiority of the proposed method compared with other weakly supervised methods. It is worth believing that there is a potential of our approach for various applications in medical images.\n"}, {"title": "Transformer Based Multi-task Deep Learning with Intravoxel Incoherent Motion Model Fitting for Microvascular Invasion Prediction of Hepatocellular Carcinoma", "abstract": "Prediction of microvascular invasion (MVI) in hepatocellular carcinoma (HCC) has important clinical value for treatment decisions and prognosis. Diffusion-weighted imaging (DWI) intravoxel incoherent motion (IVIM) models have been used to predict MVI in HCC. However, the parameter fitting of the IVIM model based on the typical nonlinear least squares method has a large amount of computation, and its accuracy is disturbed by noise. In addition, the performance of characterizing tumor characteristics based on the feature of IVIM parameter values is limited. In order to overcome the above difficulties, we proposed a novel multi-task deep learning network based on transformer to simultaneously conduct IVIM parameter model fitting and MVI prediction. Specifically, we utilize the transformer\u00e2\u0080\u0099s powerful long-distance feature modeling ability to encode deep features of different tasks, and then generalize self attention to cross-attention to match features that are beneficial to each task. In addition, inspired by the work of Compact Convolutional Transformer (CCT), we design the multi-task learning network based on CCT to enable the transformer to work in the small dataset of medical images. Experimental results of clinical HCC with IVIM data show that the proposed transformer based multi-task learning method is better than the current multi-task learning methods based on attention. Moreover, the performance of MVI prediction and IVIM model fitting based on multitask learning is better than those of single-task learning methods. Finally, IVIM model fitting facilitates the performance of IVIM to characterize MVI, providing an effective tool for clinical tumor characterization.\n"}, {"title": "Transformer Based Multi-View Network for Mammographic Image Classification", "abstract": "Most of the existing multi-view mammographic image analysis methods adopt a simple fusion strategy: features concatenation, which is widely used in many features fusion methods. However, concatenation based methods can\u00e2\u0080\u0099t extract cross view information very effectively because different views are likely to be unaligned. Recently, many researchers have attempted to intro-duce attention mechanism related methods into the field of multi-view mammo-graphy analysis. But these attention mechanism based methods still partly rely on convolution, so they can\u00e2\u0080\u0099t take full advantages of attention mechanism. To take full advantage of multi-view information, we propose a novel pure transf-ormer based multi-view network to solve the question of mammographic image classification. In our primary network, we use a transformer based backbone network to extract image features, a \u00e2\u0080\u009ccross view attention block\u00e2\u0080\u009d structure to fuse multi-view information, and a \u00e2\u0080\u009cclassification token\u00e2\u0080\u009d to gather all useful information to make the final prediction. Besides, we compare the performance when fusing multi-view information at different stages of the backbone network using a novel designed \u00e2\u0080\u009c(shifted) window based cross view attention block\u00e2\u0080\u009d structure and compare the results when fusing different views\u00e2\u0080\u0099 information. The results on DDSM dataset show that our networks can effectively use multi-view information to make judgments and outperform the concatenation and convolu-tion based methods.\n"}, {"title": "Transformer Lesion Tracker", "abstract": "Evaluating lesion progression and treatment response via longitudinal lesion tracking plays a critical role in clinical practice. Automated approaches for this task are motivated by prohibitive labor costs and time consumption when lesion matching is done manually. Previous methods typically lack the integration of local and global information. In this work, we propose a transformer-based approach, termed Transformer Lesion Tracker (TLT). Specifically, we design a Cross Attention-based Transformer (CAT) to capture and combine both global and local information to enhance feature extraction. We also develop a Registration-based Anatomical Attention Module (RAAM) to introduce anatomical information to CAT so that it can focus on useful feature knowledge. A Sparse Selection Strategy (SSS) is presented for selecting features and reducing memory footprint in Transformer training. In addition, we use a global regression to further improve model performance. We conduct experiments on a public dataset to show the superiority of our method and find that our model performance has improved the average Euclidean center error by at least 14.3% (6mm vs. 7mm) compared with the state-of-the-art (SOTA). Code is available at https://github.com/TangWen920812/TLT.\n"}, {"title": "Transforming the Interactive Segmentation for Medical Imaging", "abstract": "The goal of this paper is to interactively refine the automatic segmentation on challenging structures that fall behind human performance,  either due to the scarcity of available annotations or the difficulty nature of the problem itself,\nfor example, on segmenting cancer or small organs. Specifically, we propose a novel Transformer-based architecture for Interactive Segmentation~(TIS), that treats the refinement task as a procedure for grouping pixels with similar features to those clicks given by the end users.  Our proposed architecture is composed of Transformer Decoder variants,  which naturally fulfills feature comparison with the attention mechanisms. In contrast to existing approaches,  our proposed TIS is not limited to binary segmentations, and allows the user to edit masks for arbitrary number of categories. To validate the proposed approach, we conduct extensive experiments on three challenging datasets and demonstrate superior performance over the existing state-of-the-art methods.\n"}, {"title": "TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers", "abstract": "Combining information from multi-view images is crucial to improve the performance and robustness of automated methods for disease diagnosis. However, due to the non-alignment characteristics of multi-view images, building correlation and data fusion across views largely remain an open problem. In this study, we present TransFusion, a Transformer-based architecture to merge divergent multi-view imaging information using convolutional layers and powerful attention mechanisms. In particular, the Divergent Fusion Attention (DiFA) module is proposed for rich cross-view context modeling and semantic dependency mining, addressing the critical issue of capturing long-range correlations between unaligned data from different image views. We further propose the Multi-Scale Attention (MSA) to collect global correspondence of multi-scale feature representations. We evaluate TransFusion on the Multi-Disease, Multi-View & Multi-Center Right Ventricular Segmentation in Cardiac MRI (M&Ms-2) challenge cohort. TransFusion demonstrates leading performance against the state-of-the-art methods and opens up new perspectives for multi-view imaging integration towards robust medical image segmentation.\n"}, {"title": "TranSQ: Transformer-based Semantic Query for Medical Report Generation", "abstract": "Medical report generation, which aims at automatically generating coherent reports with multiple sentences for the given medical images, has received growing research interest due to its tremendous potential in facilitating clinical workflow and improving health services. Due to the highly patterned nature of medical reports, each sentence can be viewed as the description of an image observation with a specific purpose. To this end, this study proposes a novel Transformer-based Semantic Query (TranSQ) model that treats the medical report generation as a direct set prediction problem. Specifically, our model generates a set of semantic features to match plausible clinical concerns and compose the report with sentence retrieval and selection. Experimental results on two prevailing radiology report datasets, i.e., IU X-Ray and MIMIC-CXR, demonstrate that our model outperforms state-of-the-art models on the generation task in terms of both language generation effectiveness and clinical efficacy, which highlights the utility of our approach in generating medical reports with topics of clinical concern as well as sentence-level visual-semantic attention mappings. The source code is available at https://github.com/zjukongming/TranSQ.\n"}, {"title": "Trichomonas Vaginalis Segmentation in Microscope Images", "abstract": "Trichomoniasis is a common infectious disease with high incidence caused by the parasite Trichomonas vaginalis, increasing the risk of getting HIV in humans if left untreated. Automated detection of Trichomonas vaginalis from microscopic images can provide vital information for diagnosis of trichomoniasis. However, accurate Trichomonas vaginalis segmentation (TVS) is a challenging task due to the high appearance similarity between the Trichomonas and other cells (e.g., leukocyte), the large appearance variation caused by their motility, and, most importantly, the lack of large-scale annotated data for deep model training. To address these challenges, we elaborately collected the first large-scale Microscopic Image dataset of Trichomonas Vaginalis, named TVMI3K, which consists of 3,158 images covering Trichomonas of various appearances in diverse backgrounds, with high-quality annotations including object-level mask labels, object boundaries, and challenging attributes. Besides, we propose a simple yet effective baseline, termed TVNet, to automatically segment Trichomonas from microscopic images, including high-resolution fusion and foreground-background attention modules. Extensive experiments demonstrate that our model achieves superior segmentation performance and outperforms various cutting-edge object detection models both quantitatively and qualitatively, making it a promising framework to promote future research in TVS tasks.\n"}, {"title": "UASSR:Unsupervised Arbitrary Scale Super-resolution Reconstruction of Single Anisotropic 3D images via Disentangled Representation Learning?", "abstract": "Deep learning-based single image super resolution(SISR) algorithms show great potential to recover high-resolution(HR) images\nfrom low-resolution(LR) inputs. However, most studies require paired LR\nand HR images to supervise training, which are not available in clinical\npractice. In this paper, we propose an unsupervised arbitrary scale image\nDeep learning-based single image super resolution (SISR) algorithms have great potential to recover high-resolution (HR) images from low-resolution (LR) inputs. However, most studies require paired LR and HR images for a supervised training, which are difficult to organize in clinical applications. In this paper, we propose an unsupervised arbitrary scale super-resolution reconstruction (UASSR) method based on disentangled representation learning, eliminating the requirement of paired images for training. Applying our method to applications of generating HR images with smaller slice spacing from LR images with larger slice spacing at the inference stage, we design a strategy to fuse multiple reconstructed HR images from different views to achieve better super-resolution (SR) result. We conduct experiments on one publicly available dataset including 507 MR images of the knee joint and an in-house dataset containing 130 CT images of the lower spine. Results from our comprehensive experiments demonstrate superior performance of UASSR over other state-of-the-art methods.\n"}, {"title": "ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment", "abstract": "Neoadjuvant therapy (NAT) for breast cancer is a common treatment option in clinical practice. Tumor cellularity (TC), which represents the percentage of invasive tumors in the tumor bed, has been widely used to quantify the response of breast cancer to NAT. Therefore, automatic TC estimation is significant in clinical practice. However, existing state-of-the-art methods usually take it as a TC score regression problem, which ignores the ambiguity of TC labels caused by subjective assessment or multiple raters. In this paper, to efficiently leverage the label ambiguities, we proposed an Uncertainty-aware Label disTRibution leArning (ULTRA) framework for automatic TC estimation. The proposed ULTRA first converted the single-value TC labels to discrete label distributions, which effectively models the ambiguity among all possible TC labels. Furthermore, the network learned TC label distributions by minimizing the Kullback-Leibler (KL) divergence between the predicted and ground-truth TC label distributions, which better supervised the model to leverage the ambiguity of TC labels. Moreover, the ULTRA mimicked the multi-rater fusion process in clinical practice with a multi-branch feature fusion module to further explore the uncertainties of TC labels. We evaluated the ULTRA on the public BreastPathQ dataset. The experimental results demonstrate that the ULTRA outperformed the regression-based methods for a large margin and achieved state-of-the-art results. The code will be available from https://github.com/PerceptionComputingLab/ULTRA.\n"}, {"title": "Uncertainty Aware Sampling Framework of Weak-Label Learning for Histology Image Classification", "abstract": "Advances in digital pathology and deep learning have enabled robust disease classification, better diagnosis, and prognosis. \nIn real-world settings, readily available and inexpensive image-level labels from pathology reports are weak, which seriously degrades the performance of deep learning models. Weak image-level labels do not represent the complexity and heterogeneity of the analyzed WSIs. This work presents an importance-based sampling framework for robust histopathology image analysis, Uncertainty-Aware Sampling Framework (UASF). Our experiments demonstrate the effectiveness of UASF when used to grade a highly heterogeneous subtype of soft tissue sarcomas. Furthermore, our proposed model achieves better accuracy when compared to the baseline models by sampling the most relevant tiles. \n"}, {"title": "Uncertainty-aware Cascade Network for Ultrasound Image Segmentation with Ambiguous Boundary", "abstract": "Ultrasound image segmentation plays an essential role in automatic disease diagnosis. However, to achieve precise ultrasound segmentation is still a challenge caused by the ambiguous lesion boundary and imaging artifacts such as speckles and shadowing noise. Considering that the pixels with high uncertainty generally distributing in the boundary regions of prediction maps, are likely to overlap with the confused regions of ultrasound, we proposed an uncertainty-aware cascade network. Our network uses the confidence map to evaluate the uncertainty of each pixel to enhance the segmentation of ambiguous boundary. On the one hand, the confidence map fuses with the ultrasound features and predicted mask using the adaptive fusion module (AFM) which enriches the context features from different modalities. In addition, the uncertainty attention module (UAM) is proposed based on the confidence map. This module focuses on the influential features with cross attention constrained by the uncertainty of pixels which can extract the localized features of confused ultrasound regions. On the other hand, the recurrent edge correction module (RECM) further improves the segmentation of ambiguous boundary. This module increases the weights of confident features neighboring the uncertainty boundaries in order to refine the predictions of edge pixels with low confidence. We evaluated the proposed method on three public ultrasound datasets and the segmentation results show that our method achieved higher Dice scores and lower Hausdorff distance with more precise boundary details compared with state-of-the-art methods.\n"}, {"title": "Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention", "abstract": "Since radiologists have different training and clinical experiences, they may provide various segmentation annotations for a lung nodule. Conventional studies choose a single annotation as the learning target by default, but they waste valuable information of consensus or disagreements ingrained in the multiple annotations. This paper proposes an Uncertainty-Guided Segmentation Network (UGS-Net), which learns the rich visual features from the regions that may cause segmentation uncertainty and contributes to a better segmentation result. \nWith an Uncertainty-Aware Module, this network can provide a Multi-Confidence Mask (MCM), pointing out regions with different segmentation uncertainty levels. \nMoreover, this paper introduces a Feature-Aware Attention Module to enhance the learning of the nodule boundary and density differences. Experimental results show that our method can predict the nodule regions with different uncertainty levels and achieve superior performance in LIDC-IDRI dataset. \n"}, {"title": "Undersampled MRI Reconstruction with Side Information-Guided Normalisation", "abstract": "Magnetic resonance (MR) images exhibit various contrasts and appearances based on factors such as different acquisition protocols, views, manufacturers, scanning parameters, etc. This generally accessible appearance-related side information affects deep learning-based undersampled magnetic resonance imaging (MRI) reconstruction frameworks, but has been overlooked in the majority of current works. In this paper, we investigate the use of such side information as normalisation parameters in a convolutional neural network (CNN) to improve undersampled MRI reconstruction. Specifically, a Side Information-Guided Normalisation (SIGN) module, containing only few layers, is proposed to efficiently encode the side information and output the normalisation parameters. We examine the effectiveness of such a module on two popular reconstruction architectures, D5C5 and OUCR. The experimental results on both brain and knee images under various acceleration rates demonstrate that the proposed method improves on its corresponding baseline architectures with a significant margin.\n"}, {"title": "UNeXt: MLP-based Rapid Medical Image Segmentation Network", "abstract": "UNet and its latest extensions like TransUNet have been the leading medical image segmentation methods in recent years. However, these networks cannot be effectively adopted for rapid image segmentation in point-of-care applications as they are parameter-heavy, computationally complex and slow to use.  To this end, we propose UNeXt which is a Convolutional multilayer perceptron (MLP) based network for image segmentation. We design UNeXt in an effective way with an early convolutional stage and a MLP stage in the latent stage. We propose a tokenized MLP block where we efficiently tokenize and project the convolutional features and use MLPs to model the representation. To further boost the performance, we propose shifting the channels of the inputs while feeding in to MLPs so as to focus on learning local dependencies. Using tokenized MLPs in latent space reduces the number of parameters and computational complexity while being able to result in a better representation to help segmentation. The network also consists of skip connections between various levels of encoder and decoder.   We test UNeXt on multiple medical image segmentation datasets and show that we reduce the number of parameters by 72x, decrease the computational complexity by 68x, and improve the inference speed by 10x while also obtaining better segmentation performance over the  state-of-the-art medical image segmentation architectures.  Code is available at\nhttps://github.com/jeya-maria-jose/UNeXt-pytorch\n"}, {"title": "Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification", "abstract": "A large-scale labeled dataset is a key factor for the success of supervised deep learning in computer vision. However, a limited number of annotated data is very common, especially in ophthalmic image analysis, since manual annotation is time-consuming and labor-intensive. Self-supervised learning (SSL) methods bring huge opportunities for better utilizing unlabeled data, as they do not need massive annotations. With an attempt to use as many as possible unlabeled ophthalmic images, it is necessary to break the dimension barrier, simultaneously making use of both 2D and 3D images. In this paper, we propose a universal self-supervised Transformer framework, named Uni4Eye, to discover the inherent image property and capture domain-specific feature embedding in ophthalmic images. Uni4Eye can serve as a global feature extractor, which builds its basis on a Masked Image Modeling task with a Vision Transformer (ViT) architecture. We employ a Unified Patch Embedding module to replace the origin patch embedding module in ViT for jointly processing both 2D and 3D input images. Besides, we design a dual-branch multitask decoder module to simultaneously perform two reconstruction tasks on the input image and its gradient map, delivering discriminative representations for better convergence. We evaluate the performance of our pre-trained Uni4Eye encoder by fine-tuning it on six downstream ophthalmic image classification tasks. The superiority of Uni4Eye is successfully established through comparisons to other state-of-the-art SSL pre-training methods.\n"}, {"title": "Unified Embeddings of Structural and Functional Connectome via a Function-Constrained Structural Graph Variational Auto-Encoder", "abstract": "Graph theoretical analyses have become standard tools in modeling functional and anatomical connectivity in the brain. With the advent of connectomics, the primary graphs or networks of interest are structural connectome (derived from DTI tractography) and functional connectome (derived from resting-state fMRI). However, most published connectome studies have focused on either structural or functional connectome, yet complementary information between them, when available in the same dataset, can be jointly leveraged to improve our understanding of the brain. To this end, we propose a function-constrained structural graph variational autoencoder (FCS-GVAE) capable of incorporating information from both functional and structural connectome in an unsupervised fashion. This leads to a joint low-dimensional embedding that establishes a unified spatial coordinate system for comparing across different subjects. We evaluate our approach using the publicly available OASIS-3 Alzheimer\u00e2\u0080\u0099s disease (AD) dataset and show that a variational formulation is necessary to optimally encode functional brain dynamics. Further, the proposed joint embedding approach can more accurately distinguish different patient sub-populations than approaches that do not use complementary connectome information.\n"}, {"title": "Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining", "abstract": "Rich temporal information and variations in viewpoints make video data an attractive choice for learning image representations using unsupervised contrastive learning (UCL) techniques. State-of-the-art (SOTA) contrastive learning techniques consider frames within a video as positives in the embedding space, whereas the frames from other videos are considered negatives. We observe that unlike multiple views of an object in natural scene videos, an Ultrasound (US) video captures different 2D slices of an organ. Hence, there is almost no similarity between the temporally distant frames of even the same US video. In this paper, we propose to instead utilize such frames as hard negatives. We advocate mining both intra-video and cross-video negatives in a hardness-sensitive negative mining curriculum in a UCL framework to learn rich image representations. We deploy our framework to learn the representations of Gallbladder (GB) malignancy from US videos. We also construct the first large-scale US video dataset containing 64 videos and 15,800 frames for learning GB representations. We show that the standard ResNet50 backbone trained with our framework improves the accuracy of models pretrained with SOTA UCL techniques as well as supervised pretrained models on ImageNet for the GB malignancy detection task by 2-6%. We further validate the generalizability of our method on a publicly available lung US image dataset of COVID-19 pathologies and show an improvement of 1.5% compared to SOTA. Source code, dataset, and models are available at https://gbc-iitd.github.io/usucl.\n"}, {"title": "Unsupervised Cross-Disease Domain Adaptation by Lesion Scale Matching", "abstract": "Breast and thyroid lesions share many similarities in the feature representations of ultrasound images. However, there is a huge lesion scale gap between the two diseases, making it difficult to transfer knowledge between them through current methods for unsupervised domain adaptation. To address this problem, we propose alesion scale matching approach where we employ a framework of latent space search for bounding box size to re-scale the source domain images, and then the MonteCarlo Expectation Maximization algorithm is used for optimization to match the lesion scales between the two disease domains. Extensive experimental results demonstrate the feasibility of cross-disease knowledge transfer,  and  our  proposed  method  substantially  improves  the  performance  of  unsupervised  cross-disease  domain  adaptation  models,  with the Accuracy, Recall, Precision, and F1-score improved by 8.29%, 6.41%,11.25%, and 9.14% on average in the three sets of ablation experiments.\n"}, {"title": "Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification", "abstract": "Diagnosing hematological malignancies requires identification and classification of white blood cells in peripheral blood smears. Domain shifts caused by different lab procedures, staining, illumination, and microscope settings hamper the re-usability of recently developed machine learning methods on data collected from different sites. Here, we propose a cross-domain adapted autoencoder to extract features in an unsupervised manner on three different datasets of single white blood cells scanned from peripheral blood smears. The autoencoder is based on an R-CNN architecture allowing it to focus on the relevant white blood cell and eliminate artifacts in the image. To evaluate the quality of the extracted features we use a simple random forest to classify single cells. We show that thanks to the rich features extracted by the autoencoder trained on only one of the datasets, the random forest classifier performs satisfactorily on the unseen datasets, and outperforms published oracle networks in the cross-domain task. Our results suggest the possibility of employing this unsupervised approach in more complicated diagnosis and prognosis tasks without the need to add expensive expert labels to unseen data.\n"}, {"title": "Unsupervised Deep Non-Rigid Alignment by Low-Rank Loss and Multi-Input Attention", "abstract": "We propose a deep low-rank alignment network that can simultaneously perform non-rigid alignment and noise decomposition for multiple images despite severe noise and sparse corruptions. To address this challenging task, we introduce a low-rank loss in deep learning under the assumption that a set of well-aligned, well-denoised images should be linearly correlated, and thus, that a matrix consisting of the images should be low-rank. This allows us to remove the noise and corruption from input images in a self-supervised learning manner ({\\it i.e.}, without requiring supervised data). In addition, we introduce multi-input attention modules into Siamese U-nets in order to aggregate the corruption information from the set of images.  To the best of our knowledge, this is the first attempt to introduce a low-rank loss for deep learning-based non-rigid alignment. Experiments using both synthetic data and real medical image data demonstrate the effectiveness of the proposed method. The code will be publicly available.\n"}, {"title": "Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans", "abstract": "Registration of pre-operative and post-recurrence brain images is often needed to evaluate the effectiveness of brain gliomas treatment. While recent deep learning-based deformable registration methods have achieved remarkable success with healthy brain images, most of them would be unable to accurately align images with pathologies due to the absent correspondences in the reference image. In this paper, we propose a deep learning-based deformable registration method that jointly estimates regions with absent correspondence and bidirectional deformation fields. A forward-backward consistency constraint is used to aid in the localization of the resection and recurrence region from voxels with absence correspondences in the two images. Results on 3D clinical data from the BraTS-Reg challenge demonstrate our method can improve image alignment compared to traditional and deep learning-based registration approaches with or without cost function masking strategy. The source code is available at https://github.com/cwmok/DIRAC.\n"}, {"title": "Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation", "abstract": "Accurate segmentation of retinal fluids in 3D Optical Coherence Tomography images is key for diagnosis and personalized treatment of eye diseases. While deep learning has been successful at this task, trained supervised models often fail for images that do not resemble labeled examples, e.g. for images acquired using different devices. We hereby propose a novel semi-supervised learning framework for segmentation of volumetric images from new unlabeled domains. We jointly use supervised and contrastive learning, also introducing a contrastive pairing scheme that leverages similarity between nearby slices in 3D. In addition, we propose channel-wise aggregation as an alternative to conventional spatial-pooling aggregation for contrastive feature map projection. We evaluate our methods for domain adaptation from a (labeled) source domain to an (unlabeled) target domain, each containing images acquired with different acquisition devices. In the target domain, our method achieves a Dice coefficient 13.8% higher than SimCLR (a state-of-the-art contrastive framework), and leads to results comparable to an upper bound with supervised training in that domain. In the source domain, our model also improves the results by 5.4% Dice, by successfully leveraging information from many unlabeled images.\n"}, {"title": "Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization", "abstract": "Existing unsupervised domain adaptation methods based on adversarial learning have achieved good performance in several medical imaging tasks. However, these methods focus only on global distribution adaptation and ignore distribution constraints at the category level, which would lead to sub-optimal adaptation performance. This paper presents an unsupervised domain adaptation framework based on category-level regularization that regularizes the category distribution from three perspectives. Specifically, \nfor inter-domain category regularization, an adaptive prototype alignment module is proposed to align feature prototypes of the same category in the source and target domains.\nIn addition, for intra-domain category regularization, we tailored a regularization technique for the source and target domains, respectively. In the source domain, a prototype-guided discriminative loss is proposed to learn more discriminative feature representations by enforcing intra-class compactness and inter-class separability, and as a complement to traditional supervised loss.\nIn the target domain, an augmented consistency category regularization loss is proposed to force the model to produce consistent predictions for augmented/unaugmented target images, which encourages semantically similar regions to be given the same label. Extensive experiments on two publicly fundus datasets show that the proposed approach significantly outperforms other state-of-the-art comparison algorithms.\n"}, {"title": "Unsupervised Lesion-Aware Transfer Learning for Diabetic Retinopathy Grading in Ultra-Wide-Field Fundus Photography", "abstract": "Ultra-wide-field (UWF) fundus photography is a newly imaging technique with providing a broader field of view images, and it has become a popular and effective tool for the screening and diagnosis for many eye diseases, such as diabetic retinopathy (DR). However, it is practically challenge to train a robust deep learning model for DR grading in UWF images, due to the limited scale of data and manual annotations. By contrast, we may find large-scale high-quality regular color fundus photography datasets in the research community, with either image-level or pixel-level annotation. In consequence, we propose an Unsupervised Lesion-aware TRAnsfer learning framework (ULTRA) for DR grading in UWF images, by leveraging a large amount of publicly well-annotated regular color fundus images. Inspired by the clinical identification of DR severity, i.e., the decision making process of ophthalmologists based on the type and number of associated lesions, we design an adversarial lesion map generator to provide the auxiliary lesion information for DR grading.  A Lesion External Attention Module (LEAM) is introduced and integrates the lesion feature into the model, allowing a relative explainable DR grading. Extensive experimental results show the proposed method is superior to the state-of-the-art methods.\n"}, {"title": "Unsupervised Nuclei Segmentation using Spatial Organization Priors", "abstract": "In digital pathology, various biomarkers (e.g., KI67, HER2, CD3/CD8) are routinely analyzed by pathologists through immuno-histo-chemistry-stained slides. Identifying these biomarkers on patient biopsies allows for a more informed design of their treatment regimen. The diversity and specificity of these types of images make the availability of annotated databases sparse. Consequently, robust and efficient learning-based diagnostic systems are difficult to develop and apply in a clinical setting. Our study builds on the observation that the overall organization and structure of the observed tissues is similar across different staining protocols.  In this paper, we propose to leverage both the wide availability of hematoxylin-eosin stained databases and the invariance of tissue organization and structure in order to perform unsupervised nuclei segmentation on immunohistochemistry images. We implement and evaluate a generative adversarial method that relies on high-level nuclei distribution priors through comparison with largely available hematoxylin-eosin stained cell nuclei masks. Our approach shows promising results compared to classic unsupervised and supervised methods, as we quantitatively demonstrate on two publicly available datasets. Our code is publicly available to encourage further contributions.\n"}, {"title": "Unsupervised Representation Learning of Cingulate Cortical Folding Patterns", "abstract": "The human cerebral cortex is folded, making sulci and gyri over the whole cortical surface. Folding presents a very high inter-subject variability, and some neurodevelopmental disorders are correlated to local folding structures, named folding patterns. However, it is tough to characterize these patterns manually or semi-automatically using geometric distances. Here, we propose a new methodology to identify typical folding patterns. We focus on the cingulate region, known to have a clinical interest, using so-called skeletons (3D representation of folding patterns). We compare two models, beta-VAE and SimCLR, in an unsupervised setting to learn a relevant representation of these patterns. We add a decoder to SimCLR to be able to analyse latent space. Specifically, we leverage the data augmentations used in SimCLR to propose a novel kind of augmentations based on folding topology. We then apply a clustering on the latent space. Cluster folding averages, interpolation in the latent space and reconstructions reveal new pattern structures. This structured representation shows that unsupervised learning can help in the discovery of still unknown patterns. We will gain further insights into folding patterns by using new priors in the unsupervised algorithms and integrating other brain data modalities. Code and experiments are available at https://github.com/neurospin-projects/2021_jchavas_lguillon_deepcingulate.\n"}, {"title": "Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models", "abstract": "We aim to quantitatively measure the practical usability of trained medical image segmentation models: To what extent and on which samples a model\u00e2\u0080\u0099s predictions can be used/trusted. We first propose a scheme, Correctness-Confidence Rank Correlation (CCRC), to measure how predictions\u00e2\u0080\u0099 confidence estimates correlate with their correctness scores in rank. A model with a high value of CCRC means its prediction confidences reliably suggest which samples\u00e2\u0080\u0099 predictions are more likely to be correct. But, since CCRC does not capture the actual prediction correctness, it alone is insufficient to indicate whether a prediction model is both accurate and reliable to use in practice. Thus, we further propose another method, Usable Region Estimate (URE), which simultaneously quantifies predictions\u00e2\u0080\u0099 correctness and reliability of confidence assessments in one estimate. URE provides concrete information on to what extent a model\u00e2\u0080\u0099s predictions are usable. In addition, the sizes of usable regions (UR) can be utilized to compare models: A model with a larger UR can be taken as a more usable and hence better model. Experiments on six datasets validate that our proposed evaluation methods perform well, providing a concrete and concise measure for the practical usability of trained medical image segmentation models.\n"}, {"title": "USG-Net: Deep Learning-based Ultrasound Scanning-Guide for an Orthopedic Sonographer", "abstract": "Ultrasound (US) imaging has been widely utilized in medical fields. The acquisition of US images which contain pathological information is required for a better diagnosis. However, it is challenging to acquire informative US images due to their structural complexity, and it is significantly dependent on the expertise of a sonographer. Therefore, in this paper, we propose a fully automatic scanning-guide algorithm that assists unskilled sonographers to acquire informative US images. The proposed scanning-guide algorithm provides the proper directions of probe movement to search target regions and thus enables even unskilled sonographers to easily acquire ultrasound images of the target regions.  The main contribution of this paper is to (1) propose a new scanning-guide task that aims to search a Rotator Cuff Tear (RCT) region using a deep learning-based algorithm (USG-Net) and (2) construct a dataset to optimize the corresponding deep learning algorithm. First, the multi-dimensional US images acquired from 80 patients with RCT are processed to optimize the scanning-guide algorithm. The optimized algorithm then classifies the existence of RCT successfully. Furthermore, if RCT is not in the current frame, the algorithm provides the proper direction toward RCT. The experimental results demonstrate that the fully optimized scanning-guide algorithm offers the proper directions to localize a probe to target regions with high accuracy and assists the acquisition of informative US images.\n"}, {"title": "Using Guided Self-Attention with Local Information for Polyp Segmentation", "abstract": "Automatic and precise polyp segmentation is crucial for the early diagnosis of colorectal cancer. Existing polyp segmentation methods are mostly based on convolutional neural networks (CNNs), which usually utilize the global features to enhance local features through well-designed modules, thereby dealing with the diversity of polyps. Although CNN-based methods achieve impressive results, they are powerless to model explicit long-range relations, which limits their performance. Different from CNN, Transformer has a strong capability of modeling long-range relations owing to self-attention. However, self-attention always spreads attention to unexpected regions and the Transformer\u00e2\u0080\u0099s ability of local feature extraction is insufficient, resulting in inaccurate localization and fuzzy boundary. To address these issues, we propose PPFormer for accurate polyp segmentation. Specifically, we first adopt a shallow CNN encoder and a deep Transformer encoder to extract rich features. In the decoder, we present the PP-guided self-attention that uses prediction maps to guide self-attention to focus on the hard regions so as to enhance the model\u00e2\u0080\u0099s perception of polyp boundary. Meanwhile, the Local-to-Global mechanism is designed to encourage the Transformer to capture more information in the local-window for better polyp localization. Extensive experiments on five challenging datasets show that PPFormer outperforms other advanced methods and achieves state-of-the-art results with six metrics, i.e. mean Dice and mean IoU.\n"}, {"title": "USPoint: Self-Supervised Interest Point Detection and Description for Ultrasound-Probe Motion Estimation during Fine-Adjustment Standard Fetal Plane Finding", "abstract": "Ultrasound~(US)-probe motion estimation is a fundamental problem in automated standard plane locating during obstetric US diagnosis. \nMost recent existing recent works employ deep neural network~(DNN) to regress the probe motion.\nHowever, these deep regression-based methods leverage the DNN to overfit on the specific training data, which is naturally lack of generalization ability for the clinical application. \nIn this paper, we are back to generalized US feature learning rather than deep parameter regression.\nWe propose a self-supervised learned local detector and descriptor, named USPoint, for US-probe motion estimation during the fine-adjustment phase of fetal plane acquisition.\nSpecifically, a hybrid neural architecture is designed to simultaneously extract a local feature, and further estimate the probe motion. \nBy embedding a differentiable USPoint-based motion estimation inside the proposed network architecture, the USPoint learns the keypoint detector, scores and descriptors from motion error alone, which doesn\u00e2\u0080\u0099t require expensive human-annotation of local features."}, {"title": "Vector Quantisation for Robust Segmentation", "abstract": "The reliability of segmentation models in the medical domain depends on the model\u00e2\u0080\u0099s robustness to perturbations in the input space. Robustness is a particular challenge in medical imaging exhibiting various sources of image noise, corruptions, and domain shifts. Obtaining robustness is often attempted via simulating heterogeneous environments, either heuristically in the form of data augmentation or by learning to generate specific perturbations in an adversarial manner.  We propose and justify that learning a discrete representation in a low dimensional embedding space improves robustness of a segmentation model. This is achieved with a dictionary learning method called vector quantisation. We use a set of experiments designed to analyse robustness in both the latent and output space under domain shift and noise perturbations in the input space.  We adapt the popular UNet architecture, inserting a quantisation block in the bottleneck. We demonstrate improved segmentation accuracy and better robustness on three segmentation tasks.\n"}, {"title": "Video-based Surgical Skills Assessment using Long term Tool Tracking", "abstract": "Mastering the technical skills required to perform surgery is an extremely challenging task. Video-based assessment allows surgeons to receive feedback on their technical skills to facilitate learning and development. Currently, this feedback comes primarily from manual video review, which is time-intensive and limits the feasibility of tracking a surgeon\u00e2\u0080\u0099s progress over many cases. In this work, we introduce a motion-based approach to automatically assess surgical skills from surgical case video feed. The proposed pipeline first tracks surgical tools reliably to create motion trajectories and then uses those trajectories to predict surgeon technical skill levels. The tracking algorithm employs a simple yet effective re-identification module that improves ID-switch compared to other state-of-the-art methods. This is critical for creating reliable tool trajectories when instruments regularly move on- and off-screen or are periodically obscured. The motion-based classification model employs a state-of-the-art self-attention transformer network to capture short- and long-term motion patterns that are essential for skill evaluation. The proposed method is evaluated on an in-vivo (Cholec80) dataset where an expert-rated GOALS skill assessment of the Calot Triangle Dissection is used as a quantitative skill measure. We compare transformer-based skill assessment with traditional machine learning approaches using the proposed and state-of-the-art tracking. Our result suggests that using motion trajectories from reliable tracking methods is beneficial for assessing surgeon skills based solely on video streams.\n"}, {"title": "Vision-Language Contrastive Learning Approach to Robust Automatic Placenta Analysis Using Photographic Images", "abstract": "The standard placental examination helps identify adverse pregnancy outcomes but is not scalable since it requires hospital-level equipment and expert knowledge. Although the current supervised learning approaches in automatic placenta analysis improved the scalability, those approaches fall short on robustness and generalizability due to the scarcity of labeled training images. In this paper, we propose to use the vision-language contrastive learning (VLC) approach to address the data scarcity problem by incorporating the abundant pathology reports into the training data. Moreover, we address the feature suppression problem in the current VLC approaches to improve generalizability and robustness. The improvements enable us to use a shared image encoder across tasks to boost efficiency. Overall, our approach outperforms the strong baselines for fetal/maternal inflammatory response (FIR/MIR), chorioamnionitis, and sepsis risk classification tasks using the images from a professional photography equipment at a large urban academic hospital; it also achieves the highest inference robustness to iPad images for MIR and chorioamnionitis risk classification tasks. It is the first approach to show robustness to placenta images from a mobile platform that is accessible to low-resource communities.\n"}, {"title": "Visual deep learning-based explanation for neuritic plaques segmentation in Alzheimer\u00e2\u0080\u0099s Disease using weakly annotated whole slide histopathological images", "abstract": "Quantifying the distribution and morphology of tau protein structures in brain tissues is key to diagnosing Alzheimer\u00e2\u0080\u0099s Disease (AD) and its subtypes. Recently, deep learning (DL) models such as UNet have been successfully used for automatic segmentation of histopathological whole slide images (WSI) of biological tissues. In this study, we propose a DL-based methodology for semantic segmentation of tau lesions (i.e., neuritic plaques) in WSI of postmortem patients with AD. The state of the art in semantic segmentation of neuritic plaques in human WSI is very limited. Our study proposes a baseline able to generate a significant advantage for morphological analysis of these tauopathies for further stratification of AD patients. Essential discussions concerning biomarkers (ALZ50 versus AT8 tau antibodies), the imaging modality (different slide scanner resolutions), and the challenge of weak annotations are addressed within this seminal study. The analysis of the impact of context in plaque segmentation is important to understand the role of the micro-environment for reliable tau protein segmentation. In addition, by integrating visual interpretability, we are able to explain how the network focuses on a region of interest (ROI), giving additional insights to pathologists. Finally, the release of a new expert-annotated database and the code (\\url{https://github.com/aramis-lab/miccai2022-stratifiad.git}) will be helpful for the scientific community to accelerate the development of new pipelines for human WSI processing in AD. \n"}, {"title": "Visual explanations for the detection of diabetic retinopathy from retinal fundus images", "abstract": "In medical image classification tasks like the detection of diabetic retinopathy from retinal fundus images, it is highly desirable to get visual explanations for the decisions of black-box deep neural networks (DNNs). However, gradient-based saliency methods often fail to highlight the diseased image regions reliably. On the other hand, adversarially robust models have more interpretable gradients than plain models but suffer typically from a significant drop in accuracy, which is unacceptable for clinical practice. Here, we show that one can get the best of both worlds by ensembling a plain and an adversarially robust model: maintaining high accuracy but having improved visual explanations. Also, our ensemble produces meaningful visual counterfactuals which are complementary to existing saliency-based techniques.\n"}, {"title": "vMFNet: Compositionality Meets Domain-generalised Segmentation", "abstract": "Training medical image segmentation models usually requires a large amount of labeled data. By contrast, humans can quickly learn to accurately recognise anatomy of interest from medical (e.g. MRI and CT) images with some limited guidance. Such recognition ability can easily generalise to new images from different clinical centres. This rapid and generalisable learning ability is mostly due to the compositional structure of image patterns in the human brain, which is less incorporated in medical image segmentation. In this paper, we model the compositional components (i.e. patterns) of human anatomy as learnable von-Mises-Fisher (vMF) kernels, which are robust to images collected from different domains (e.g. clinical centres). The image features can be decomposed to (or composed by) the components with the composing operations, i.e. the vMF likelihoods. The vMF likelihoods tell how likely each anatomical part is at each position of the image. Hence, the segmentation mask can be predicted based on the vMF likelihoods. Moreover, with a reconstruction module, unlabeled data can also be used to learn the vMF kernels and likelihoods by recombining them to reconstruct the input image. Extensive experiments show that the proposed vMFNet achieves improved generalisation performance on two benchmarks, especially when annotations are limited. Code is publicly available at: \\url{https://github.com/vios-s/vMFNet}.\n"}, {"title": "Vol2Flow: Segment 3D Volumes using a Sequence of Registration Flows", "abstract": "This work proposes a self-supervised algorithm to segment each arbitrary anatomical structure in a 3D medical image produced under various acquisition conditions, dealing with domain shift problems and generalizability. Furthermore, we advocate an interactive setting in the inference time, where the self-supervised model trained on unlabeled volumes should be directly applicable to segment each test volume given the user-provided single slice annotation. To this end, we learn a novel 3D registration network, namely Vol2Flow, from the perspective of image sequence registration to find 2D displacement fields between all adjacent slices within a 3D medical volume together. Specifically, we present a novel 3D CNN-based architecture that finds a series of registration flows between consecutive slices within a whole volume, resulting in a dense displacement field. A new self-supervised algorithm is proposed to learn the transformations or registration fields between the series of 2D images of a 3D volume. Consequently, we enable gradually propagating the user-provided single slice annotation to other slices of a volume in the inference time. We demonstrate that our model substantially outperforms related methods on various medical image segmentation tasks through several experiments on different medical image segmentation datasets.\n"}, {"title": "Warm Start Active Learning with Proxy Labels & Selection via Semi-Supervised Fine-Tuning", "abstract": "Which volume to annotate next is a challenging problem in building medical imaging datasets for deep learning. One of the promising methods to approach this question is active learning (AL). However, AL has been a hard nut to crack in terms of which AL algorithm and acquisition functions are most useful for which datasets. Also, the problem is exacerbated with which volumes to label first when there is zero labeled data to start with. This is known as the cold start problem in AL. We propose two novel strategies for AL specifically for 3D image segmentation. First, we tackle the cold start problem by proposing a proxy task and then utilizing uncertainty generated from the proxy task to rank the unlabeled data to be annotated. Second, we craft a two-stage learning framework for each active iteration where the unlabeled data is also used in the second stage as a semi-supervised fine-tuning strategy. We show the promise of our approach on two well-known large public datasets from medical segmentation decathlon. The results indicate that the initial selection of data and semi-supervised framework both showed significant improvement for several AL strategies.\n"}, {"title": "WavTrans: Synergizing Wavelet and Cross-Attention Transformer for Multi-Contrast MRI Super-resolution", "abstract": "Current multi-contrast MRI super-resolution (SR) methods often harness convolutional neural networks (CNNs) for feature extraction and fusion. However, existing models have some shortcomings that prohibit them from producing more satisfactory results. First, during the feature extraction, some high-frequency details in the images are lost, resulting in blurring boundaries in the reconstructed images, which may impede the following diagnosis and treatment. Second, the perceptual field of the convolution kernel is limited, making the networks difficult to capture long-range/non-local features. Third, most of these models are solely driven by training data, neglecting prior knowledge about the correlations among different contrasts, which, once well leveraged, will effectively enhance the performance with limited training data. In this paper, we propose a novel model to synergize wavelet transforms with a new cross-attention transformer to comprehensively tackle these challenges; we call it WavTrans. Specifically, we harness one-level wavelet transformation to obtain the detail and approximation coefficients in the reference contrast MR images (Ref). While the approximation coefficients are applied to compress the low-frequency global information, the detail coefficients are utilized to represent the high-frequency local structure and texture information. Then, we propose a new residual cross-attention swin transformer to extract and fuse extracted features to establish long-distance dependencies between features and maximize the restoration of high-frequency information in Tar. In addition, a multi-residual fusion module is designed to fuse the high-frequency information in the upsampled Tar and the original Ref to ensure the restoration of detailed information. Extensive experiments demonstrate that WavTrans outperforms the SOTA methods by a considerable margin with upsampling factors of 2-fold and 4-fold. \n"}, {"title": "Weakly Supervised MR-TRUS Image Synthesis for Brachytherapy of Prostate Cancer", "abstract": "Prostate magnetic resonance imaging (MRI) offers accurate details of structures and tumors for prostate cancer brachytherapy. However, it is unsuitable for routine treatment since MR images differ significantly from trans-rectal ultrasound (TRUS) images conventionally used for radioactive seed implants in brachytherapy. TRUS imaging is fast, convenient, and widely available in the operation room but is known for its low soft-tissue contrast and tumor visualization capability in the prostate area. Conventionally, practitioners usually rely on prostate segmentation to fuse the two imaging modalities with non-rigid registration. However, prostate delineation is often not available on diagnostic MR images. Besides, the high non-linear intensity relationship between two imaging modalities poses a challenge to non-rigid registration. Hence, we propose a method to generate a TRUS-styled image from a prostate MR image to replace the role of the TRUS image in radiation therapy dose pre-planning. We propose a structural constraint to handle non-linear projections of anatomical structures between MR and TRUS images. We further include an adversarial mechanism to enforce the model to preserve anatomical features in an MR image (such as prostate boundary and dominant intraprostatic lesion (DIL)) while synthesizing the TRUS-styled counterpart image. The proposed method is compared with other state-of-art methods with real TRUS images as the reference. The results demonstrate that the TRUS images synthesized by our method can be used for brachytherapy treatment planning for prostate cancer.\n"}, {"title": "Weakly Supervised Online Action Detection for Infant General Movements", "abstract": "To make the earlier medical intervention of infants\u00e2\u0080\u0099 cerebral palsy (CP), early diagnosis of brain damage is critical. Although general movements assessment(GMA) has shown promising results in early CP detection, it is laborious. Most existing works take videos as input to make fidgety movements(FMs) classification for the GMA automation. Those methods require a complete observation of videos and can not localize video frames containing normal FMs. Therefore we propose a novel approach named WO-GMA to perform FMs localization in the weakly supervised online setting. Infant body keypoints are first extracted as the inputs to WO-GMA. Then WO-GMA performs local spatio-temporal extraction followed by two network branches to generate pseudo clip labels and model online actions. With the clip-level pseudo labels, the action modeling branch learns to detect FMs in an online fashion. Experimental results on a dataset with 757 videos of different infants show that WO-GMA can get state-of-the-art video-level classification and clip-level detection results. Moreover, only the first 20% duration of the video is needed to get classification results as good as fully observed, implying a significantly shortened FMs diagnosis time.\n"}, {"title": "Weakly Supervised Segmentation by Tensor Graph Learning for Whole Slide Images", "abstract": "Semantic segmentation of whole slide images (WSIs) helps pathologists identify lesions and cancerous nests. However, training fully supervised segmentation networks usually requires plenty of pixel-level annotations, which consume lots of time and human efforts. Coming from tissues of different patients with large amounts of pixels, WSIs exhibit various patterns, resulting in intra-class heterogeneity and inter-class homogeneity. Meanwhile, most existing methods for WSIs focus on extracting a certain type of features, neglecting the relations between different features and their joint effect on segmentation. Therefore, we propose a novel weakly supervised network based on tensor graphs (WSNTG) for WSI segmentation. Using only sparse point annotations, it efficiently segments WSIs by superpixel-wise classification and credible node reweighting. To deal with the variability of WSIs, the proposed network represents multiple hand-crafted features and hierarchical features yielded by a pretrained Convolutional Neural Network (CNN). Particularly, it learns over the semi-labeled tensor graphs constructed on the hierarchical features to exploit nonlinear data structures and associations. It gains robustness via the tensor-graph Laplacian of the hand-crafted features superimposed on the segmentation loss. We evaluated WSNTG on two WSI datasets, DigestPath2019 and SICAPV2. Results show that it outperforms many fully supervised and weakly supervised methods with minimal point annotations in WSI segmentation. The codes are published at https://github.com/zqh369/WSNTG.\n"}, {"title": "Weakly Supervised Volumetric Image Segmentation with Deformed Templates", "abstract": "There are many approaches to weakly-supervised training of networks to segment 2D images. By contrast, existing approaches to segmenting volumetric images rely on full-supervision of a subset of 2D slices of the 3D volume. We propose an approach to volume segmentation that is truly weakly-supervised in the sense that we only need to provide a sparse set of 3D points on the surface of target objects instead of detailed 2D masks. We use the 3D points to deform a 3D template so that it roughly matches the target object outlines and we introduce an architecture that exploits the supervision it provides to train a network to find accurate boundaries. We evaluate our approach on Computed Tomography (CT), Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets and show that it substantially reduces the required amount of effort. \n"}, {"title": "Weakly-supervised Biomechanically-constrained CT/MRI Registration of the Spine", "abstract": "CT and MRI are two of the most informative modalities in spinal diagnostics and treatment planning. CT is useful when analysing bony structures, while MRI gives information about the soft tissue. Thus, fusing the information of both modalities can be very beneficial.\nRegistration is the first step for this fusion. While the soft tissues around the vertebra are deformable, each vertebral body is constrained to move rigidly. We propose a weakly-supervised deep learning framework that preserves the rigidity and the volume of each vertebra while maximizing the accuracy of the registration. To achieve this goal, we introduce anatomy-aware losses for training the network. We specifically design these losses to depend only on the CT label maps since automatic vertebra segmentation in CT gives more accurate results contrary to MRI. We evaluate our method on an in-house dataset of 167 patients. Our results show that adding the anatomy-aware losses increases the plausibility of the inferred transformation while keeping the accuracy untouched."}, {"title": "Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling", "abstract": "Ultrasound (US) is widely used due to its advantages of real-time imaging, radiation-free and portability. In clinical practice, analysis and diagnosis often rely on US sequences rather than a single image to obtain dynamic anatomical information. This is challenging for novices to learn because collecting adequate videos is clinically unpractical. In this paper, we propose a novel framework to synthesize high-fidelity US videos. Specifically, the synthesis videos are generated by animating source content images based on the motion of given driving videos. Our highlights are three-fold. First, leveraging the advantages of self- and fully-supervised learning, our proposed system is trained in weakly-supervised manner for keypoint detection. These points then provide vital information for handling complex high dynamic motions in US videos. Second, we decouple content and texture learning using the dual decoders to effectively reduce the model learning difficulty. Last, we adopt the adversarial training strategy with GAN losses for further improving the sharpness of the generated videos, narrowing the gap between real and synthesis videos.  We validate our methods on a large in-house pelvic dataset with high dynamic motion.  Extensive evaluation metrics and user study prove the effectiveness of our proposed method. \n"}, {"title": "Weighted Concordance Index Loss-based Multimodal Survival Modeling for Radiation Encephalopathy Assessment in Nasopharyngeal Carcinoma Radiotherapy", "abstract": "Radiation encephalopathy (REP) is the most common complication for nasopharyngeal carcinoma (NPC) radiotherapy. It is highly desirable to assist clinicians in optimizing the NPC radiotherapy regimen to reduce radiotherapy-induced temporal lobe injury (RTLI) according to the probability of REP onset. To the best of our knowledge, it is the first exploration of predicting radiotherapy-induced REP by jointly exploiting image and non-image data in NPC radiotherapy regimen. We cast REP prediction as a survival analysis task and evaluate the predictive accuracy in terms of the concordance index (CI). We design a deep multimodal survival network (MSN) with two feature extractors to learn discriminative features from multimodal data. One feature extractor imposes feature selection on non-image data, and the other learns visual features from images. Because the priorly balanced CI (BCI) loss function directly maximizing the CI is sensitive to uneven sampling per batch. Hence, we propose a novel weighted CI (WCI) loss function to leverage all REP samples effectively by assigning their different weights with a dual average operation. We further introduce a temperature hyper-parameter for our WCI to sharpen the risk difference of sample pairs to help model convergence. We extensively evaluate our WCI on a private dataset to demonstrate its favourability against its counterparts. The experimental results also show multimodal data of NPC radiotherapy can bring more gains for REP risk prediction.\n"}, {"title": "What can we learn about a generated image corrupting its latent representation?", "abstract": "Generative adversarial networks (GANs) offer an effective solution to the image-to-image translation problem, thereby allowing for new possibilities in medical imaging. They can translate images from one imaging modality to another at a low cost. For unpaired datasets, they rely mostly on cycle loss. Despite its effectiveness in learning the underlying data distribution, it can lead to a discrepancy between input and output data. The purpose of this work is to investigate the hypothesis that we can predict image quality based on its latent representation in the GANs bottleneck. We achieve this by corrupting the latent representation with noise and generating multiple outputs. The degree of differences between them is interpreted as the strength of the representation: the more robust the latent representation, the fewer changes in the output image the corruption causes. Our results demonstrate that our proposed method has the ability to i) predict uncertain parts of synthesized images, and ii) identify samples that may not be reliable for downstream tasks, e.g., liver segmentation task.\n"}, {"title": "What Makes for Automatic Reconstruction of Pulmonary Segments", "abstract": "3D reconstruction of pulmonary segments plays an important role in surgical treatment planning of lung cancer, which facilitates preservation of pulmonary function and helps ensure low recurrence rates. However, automatic reconstruction of pulmonary segments remains unexplored in the era of deep learning. In this paper, we investigate what makes for automatic reconstruction of pulmonary segments. First and foremost, we formulate, clinically and geometrically, the anatomical definitions of pulmonary segments, and propose evaluation metrics adhering to these definitions. Second, we propose ImPulSe (Implicit Pulmonary Segment), a deep implicit surface model designed for pulmonary segment reconstruction. The automatic reconstruction of pulmonary segments by ImPulSe is accurate in metrics and visually appealing. Compared with canonical segmentation methods, ImPulSe outputs continuous predictions of arbitrary resolutions with higher training efficiency and fewer parameters. Lastly, we experiment with different network inputs to analyze what matters in the task of pulmonary segment reconstruction. Our code is available at https://github.com/M3DV/ImPulSe.\n"}, {"title": "White Matter Tracts are Point Clouds: Neuropsychological Score Prediction and Critical Region Localization via Geometric Deep Learning", "abstract": "White matter tract microstructure has been shown to influence neuropsychological scores of cognitive performance. However, prediction of these scores from white matter tract data has not been attempted. In this paper, we propose a deep-learning-based framework for neuropsychological score prediction using microstructure measurements estimated from diffusion magnetic resonance imaging (dMRI) tractography, focusing on predicting performance on a receptive vocabulary assessment task based on a critical fiber tract for language, the arcuate fasciculus (AF). We directly utilize information from all points in a fiber tract, without the need to average data along the fiber as is traditionally required by diffusion MRI tractometry methods. Specifically, we represent the AF as a point cloud with microstructure measurements at each point, enabling our adoption of point-based neural networks. We improve prediction performance with the proposed Paired-Siamese Loss that utilizes information about differences between continuous neuropsychological scores. Finally, we propose a Critical Region Localization (CRL) algorithm to localize informative anatomical regions containing points with strong contributions to the prediction results. Our method is evaluated on data from 806 subjects from the Human Connectome Project dataset. Results demonstrate superior neuropsychological score prediction performance compared to baseline methods. We discover that critical regions in the AF are strikingly consistent across subjects, with the highest number of strongly contributing points located in frontal cortical regions (i.e., the rostral middle frontal, pars opercularis, and pars triangularis), which are strongly implicated as critical areas for language processes.\n"}, {"title": "Whole Slide Cervical Cancer Screening Using Graph Attention Network and Supervised Contrastive Learning", "abstract": "Cervical cancer is one of the primary factors that endanger women\u00e2\u0080\u0099s health, and Thin-prep cytologic test (TCT) has been widely applied for early screening. Automatic whole slide image (WSI) classification is highly demanded, as it can significantly reduce the workload of pathologists. Current methods are mainly based on suspicious lesion patch extraction and classification, which ignore the intrinsic relationships between suspicious patches and neglect the other patches apart from the suspicious patches, and therefore limit their robustness and generalizability. Here we propose a novel method to solve the problem, which is based on graph attention network (GAT) and supervised contrastive learning. First, for each WSI, we extract and rank a large number of representative patches based on suspicious cell detection. Then, we select the top-K and bottom-K suspicious patches to construct two graphs seperately. Next, we introduce GAT to aggregate the features from each node, and use supervised contrastive learning to obtain valuable representations of graphs. Specifically, we design a novel contrastive loss so that the latent distances between two graphs are enlarged for positive WSIs and reduced for negative WSIs. Experimental results show that the proposed GAT method outperforms conventional methods, and also demonstrate the effectiveness of supervised contrastive learning.\n"}, {"title": "Why patient data cannot be easily forgotten?", "abstract": "Rights provisioned within data protection regulations, permit patients to request that knowledge about their information be eliminated by data holders. With the advent of AI learned on data, one can imagine that such rights can extent to requests for forgetting knowledge of patient\u00e2\u0080\u0099s data within AI models. However, forgetting patients\u00e2\u0080\u0099 imaging data from AI models, is still an under-explored problem. In this paper, we study the influence of patient data on model performance and formulate two hypotheses for a patient\u00e2\u0080\u0099s data: either they are common and similar to other patients or form edge cases, i.e.\\ unique and rare cases. This shows that it is not possible to easily forget patient data. We propose a targeted forgetting approach to perform patient-wise forgetting. Extensive experiments on the benchmark ACDC dataset showcase the improved performance of the proposed targeted forgetting approach as opposed to a state-of-the-art method.\n"}, {"title": "XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention", "abstract": "An effective backbone network is important to deep learning-based Deformable Medical Image Registration (DMIR), because it extracts and matches the features between two images to discover the mutual correspondence for fine registration. However, the existing deep networks focus on single image situation and are limited in registration task which is performed on paired images. Therefore, we advance a novel backbone network, XMorpher, for the effective corresponding feature representation in DMIR. 1) It proposes  a novel full  transformer architecture including dual parallel feature extraction networks which exchange information through cross attention, thus discovering multi-level semantic correspondence while extracting respective features gradually for final effective registration. 2) It advances the Cross Attention Transformer (CAT) blocks to establish the attention mechanism between images which is able to find the correspondence automatically and prompts the features to fuse efficiently in the network. 3) It constrains the attention computation between base windows and searching windows with different sizes, and thus focuses on the local transformation of deformable registration and enhances the computing efficiency at the same time. Without any bells and whistles, our XMorpher gives Voxelmorph 2.8% improvement on DSC , demonstrating its effective representation of the features from the paired images in DMIR. We believe that our XMorpher has great application potential in more paired medical images. Our XMorpher is open on https://github.com/Solemoon/XMorpher\n"}, {"title": "Y-Net: A Spatiospectral Dual-Encoder Network for Medical Image Segmentation", "abstract": "Automated segmentation of retinal optical coherence tomography (OCT) images has become an important recent direction in machine learning for medical applications. We hypothesize that the anatomic structure of layers and their high-frequency variation in OCT images make retinal OCT a fitting choice for extracting spectral domain features and combining them with spatial domain features. In this work, we present \u00ce\u00a5-Net, an architecture that combines the frequency domain features with the image domain to improve the segmentation performance of OCT images. The results of this work demonstrate that the introduction of two branches, one for spectral and one for spatial domain features, brings very significant improvement in fluid segmentation performance and allows outperformance as compared to the well-known U-Net model. Our improvement was 13% on the fluid segmentation dice score and 1.8% on the average dice score. Finally, removing selected frequency ranges in the spectral domain demonstrates the impact of these features on the fluid segmentation outperformance.\n"}]